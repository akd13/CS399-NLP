{"images": [{"article_id": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "description": {"raw": "Shows an ontological diagram: a joint data strike/boycott consists of a pure data strike and traditional boycott. Pure data strikes have direct indirect effects and traditional boycotts also have direct and indirect effects.", "tokens": ["Shows", "an", "ontological", "diagram", ":", "a", "joint", "data", "strike/boycott", "consists", "of", "a", "pure", "data", "strike", "and", "traditional", "boycott", ".", "Pure", "data", "strikes", "have", "direct", "indirect", "effects", "and", "traditional", "boycotts", "also", "have", "direct", "and", "indirect", "effects", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies ", "tokens": ["“", "Data", "Strikes", "”", ":", "Evaluating", "the", "Effectiveness", "of", "a", "New", "Form", "of", "Collective", "Action", "Against", "Technology", "Companies"]}, "filename": "41cbffad975874060d643c36c8bdb5c72637564e_Image_002.jpg", "orig_filename": "41cbffad975874060d643c36c8bdb5c72637564e", "split": "train"}, {"article_id": "How We Type: Eye and Finger Movement Strategies in Mobile Typing", "description": {"raw": "Illustration of our data as heatmaps of finger touchpoints. All present typing of the same sentence by a different participant: one (index) finger with no typing errors, two thumbs with no errors, one finger with errors, and two thumbs with errors. Glances at the text-entry area increase with the number of errors made, and error correction is visible as touches of Backspace. In two-thumb typing, visual guidance of the fingers is less in demand, so the gaze covers smaller areas of the keyboard.", "tokens": ["Illustration", "of", "our", "data", "as", "heatmaps", "of", "finger", "touchpoints", ".", "All", "present", "typing", "of", "the", "same", "sentence", "by", "a", "different", "participant", ":", "one", "(", "index", ")", "finger", "with", "no", "typing", "errors", ",", "two", "thumbs", "with", "no", "errors", ",", "one", "finger", "with", "errors", ",", "and", "two", "thumbs", "with", "errors", ".", "Glances", "at", "the", "text-entry", "area", "increase", "with", "the", "number", "of", "errors", "made", ",", "and", "error", "correction", "is", "visible", "as", "touches", "of", "Backspace", ".", "In", "two-thumb", "typing", ",", "visual", "guidance", "of", "the", "fingers", "is", "less", "in", "demand", ",", "so", "the", "gaze", "covers", "smaller", "areas", "of", "the", "keyboard", "."]}, "caption": {"raw": "Figure 1. Illustration of our data as heatmaps of ﬁnger touchpoints (blue for right index ﬁnger or thumb, and red for left thumb) and eye movements (green). All present typing of the same sentence by a different participant: one (index) ﬁnger with no typing errors, two thumbs with no errors, one ﬁnger with errors, and two thumbs with errors. Glances at the text-entry area increase with the number of errors made, and error correction is visible as touches of Backspace. In two-thumb typing, visual guidance of the ﬁngers is less in demand, so the gaze covers smaller areas of the keyboard.", "tokens": ["Figure", "1", ".", "Illustration", "of", "our", "data", "as", "heatmaps", "of", "ﬁnger", "touchpoints", "(", "blue", "for", "right", "index", "ﬁnger", "or", "thumb", ",", "and", "red", "for", "left", "thumb", ")", "and", "eye", "movements", "(", "green", ")", ".", "All", "present", "typing", "of", "the", "same", "sentence", "by", "a", "different", "participant", ":", "one", "(", "index", ")", "ﬁnger", "with", "no", "typing", "errors", ",", "two", "thumbs", "with", "no", "errors", ",", "one", "ﬁnger", "with", "errors", ",", "and", "two", "thumbs", "with", "errors", ".", "Glances", "at", "the", "text-entry", "area", "increase", "with", "the", "number", "of", "errors", "made", ",", "and", "error", "correction", "is", "visible", "as", "touches", "of", "Backspace", ".", "In", "two-thumb", "typing", ",", "visual", "guidance", "of", "the", "ﬁngers", "is", "less", "in", "demand", ",", "so", "the", "gaze", "covers", "smaller", "areas", "of", "the", "keyboard", "."]}, "context": {"raw": "How We Type: Eye and Finger Movement Strategies in Mobile Typing Figure 1. Illustration of our data as heatmaps of ﬁnger touchpoints (blue for right index ﬁnger or thumb, and red for left thumb) and eye movements (green). All present typing of the same sentence by a different participant: one (index) ﬁnger with no typing errors, two thumbs with no errors, one ﬁnger with errors, and two thumbs with errors. Glances at the text-entry area increase with the number of errors made, and error correction is visible as touches of Backspace. In two-thumb typing, visual guidance of the ﬁngers is less in demand, so the gaze covers smaller areas of the keyboard.", "tokens": ["How", "We", "Type", ":", "Eye", "and", "Finger", "Movement", "Strategies", "in", "Mobile", "Typing", "Figure", "1", ".", "Illustration", "of", "our", "data", "as", "heatmaps", "of", "ﬁnger", "touchpoints", "(", "blue", "for", "right", "index", "ﬁnger", "or", "thumb", ",", "and", "red", "for", "left", "thumb", ")", "and", "eye", "movements", "(", "green", ")", ".", "All", "present", "typing", "of", "the", "same", "sentence", "by", "a", "different", "participant", ":", "one", "(", "index", ")", "ﬁnger", "with", "no", "typing", "errors", ",", "two", "thumbs", "with", "no", "errors", ",", "one", "ﬁnger", "with", "errors", ",", "and", "two", "thumbs", "with", "errors", ".", "Glances", "at", "the", "text-entry", "area", "increase", "with", "the", "number", "of", "errors", "made", ",", "and", "error", "correction", "is", "visible", "as", "touches", "of", "Backspace", ".", "In", "two-thumb", "typing", ",", "visual", "guidance", "of", "the", "ﬁngers", "is", "less", "in", "demand", ",", "so", "the", "gaze", "covers", "smaller", "areas", "of", "the", "keyboard", "."]}, "filename": "05300590913007eb710cd89f5d373e1ec0833bfa_Image_001.jpg", "orig_filename": "05300590913007eb710cd89f5d373e1ec0833bfa", "split": "train"}, {"article_id": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals", "description": {"raw": "This figure shows a stacked bar plot of percentages of 5-point frequency scale responses. The y-axis shows the percentage of participants. The x-axis has X categories \"Reading\" and \"Video\".  The following scale responses are shown from bottom to top: \"Daily\", \"Often\", \"Weekly\", \"Monthly\",  and \"Rarely\"  For \"Reading\",  28% responded \"Daily\", 28% responded \"Often\", 15.5% responded \"Weekly\", 15.5% responded \"Monthly, and 12.5% responded \"Rarely.\"  For \"Video\",  10% responded \"Daily\", 29% responded \"Often\", 19% responded \"Weekly\", 29% responded \"Monthly, and 13% responded \"Rarely.\"", "tokens": ["This", "figure", "shows", "a", "stacked", "bar", "plot", "of", "percentages", "of", "5-point", "frequency", "scale", "responses", ".", "The", "y-axis", "shows", "the", "percentage", "of", "participants", ".", "The", "x-axis", "has", "X", "categories", "``", "Reading", "''", "and", "``", "Video", "''", ".", "The", "following", "scale", "responses", "are", "shown", "from", "bottom", "to", "top", ":", "``", "Daily", "''", ",", "``", "Often", "''", ",", "``", "Weekly", "''", ",", "``", "Monthly", "''", ",", "and", "``", "Rarely", "''", "For", "``", "Reading", "''", ",", "28", "%", "responded", "``", "Daily", "''", ",", "28", "%", "responded", "``", "Often", "''", ",", "15.5", "%", "responded", "``", "Weekly", "''", ",", "15.5", "%", "responded", "``", "Monthly", ",", "and", "12.5", "%", "responded", "``", "Rarely", ".", "''", "For", "``", "Video", "''", ",", "10", "%", "responded", "``", "Daily", "''", ",", "29", "%", "responded", "``", "Often", "''", ",", "19", "%", "responded", "``", "Weekly", "''", ",", "29", "%", "responded", "``", "Monthly", ",", "and", "13", "%", "responded", "``", "Rarely", ".", "''"]}, "caption": {"raw": "Figure 4: Participants’ responses to the questions: \"How of- ten do you read to learn about technical topics for your work (e.g. information about new technologies, new software, pro- gramming information, etc.)?\" and \"How often do you watch videos to learn about technical topics for your work (e.g. in- formation about new technologies, new software, program- ming information, etc.)?\" The full text in the options was: \"rarely (less than once a month),\" \"monthly (one to three times a month),\" \"weekly (once a week),\" \"often (two to four times a week),\" and \"daily (fve or more times a week).\" There was a signifcant diference between the two (p < 0.05).", "tokens": ["Figure", "4", ":", "Participants", "’", "responses", "to", "the", "questions", ":", "``", "How", "of-", "ten", "do", "you", "read", "to", "learn", "about", "technical", "topics", "for", "your", "work", "(", "e.g", ".", "information", "about", "new", "technologies", ",", "new", "software", ",", "pro-", "gramming", "information", ",", "etc.", ")", "?", "''", "and", "``", "How", "often", "do", "you", "watch", "videos", "to", "learn", "about", "technical", "topics", "for", "your", "work", "(", "e.g", ".", "in-", "formation", "about", "new", "technologies", ",", "new", "software", ",", "program-", "ming", "information", ",", "etc.", ")", "?", "''", "The", "full", "text", "in", "the", "options", "was", ":", "``", "rarely", "(", "less", "than", "once", "a", "month", ")", ",", "''", "``", "monthly", "(", "one", "to", "three", "times", "a", "month", ")", ",", "''", "``", "weekly", "(", "once", "a", "week", ")", ",", "''", "``", "often", "(", "two", "to", "four", "times", "a", "week", ")", ",", "''", "and", "``", "daily", "(", "fve", "or", "more", "times", "a", "week", ")", ".", "''", "There", "was", "a", "signifcant", "diference", "between", "the", "two", "(", "p", "<", "0.05", ")", "."]}, "context": {"raw": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals Figure 4: Participants’ responses to the questions: \"How of- ten do you read to learn about technical topics for your work (e.g. information about new technologies, new software, pro- gramming information, etc.)?\" and \"How often do you watch videos to learn about technical topics for your work (e.g. in- formation about new technologies, new software, program- ming information, etc.)?\" The full text in the options was: \"rarely (less than once a month),\" \"monthly (one to three times a month),\" \"weekly (once a week),\" \"often (two to four times a week),\" and \"daily (fve or more times a week).\" There was a signifcant diference between the two (p < 0.05).", "tokens": ["Reading", "Experiences", "and", "Interest", "in", "Reading-Assistance", "Tools", "Among", "Deaf", "and", "Hard-of-Hearing", "Computing", "Professionals", "Figure", "4", ":", "Participants", "’", "responses", "to", "the", "questions", ":", "``", "How", "of-", "ten", "do", "you", "read", "to", "learn", "about", "technical", "topics", "for", "your", "work", "(", "e.g", ".", "information", "about", "new", "technologies", ",", "new", "software", ",", "pro-", "gramming", "information", ",", "etc.", ")", "?", "''", "and", "``", "How", "often", "do", "you", "watch", "videos", "to", "learn", "about", "technical", "topics", "for", "your", "work", "(", "e.g", ".", "in-", "formation", "about", "new", "technologies", ",", "new", "software", ",", "program-", "ming", "information", ",", "etc.", ")", "?", "''", "The", "full", "text", "in", "the", "options", "was", ":", "``", "rarely", "(", "less", "than", "once", "a", "month", ")", ",", "''", "``", "monthly", "(", "one", "to", "three", "times", "a", "month", ")", ",", "''", "``", "weekly", "(", "once", "a", "week", ")", ",", "''", "``", "often", "(", "two", "to", "four", "times", "a", "week", ")", ",", "''", "and", "``", "daily", "(", "fve", "or", "more", "times", "a", "week", ")", ".", "''", "There", "was", "a", "signifcant", "diference", "between", "the", "two", "(", "p", "<", "0.05", ")", "."]}, "filename": "b61e1747b1fef3eedde7aa87f26338f61d52ce93_Image_008.jpg", "orig_filename": "b61e1747b1fef3eedde7aa87f26338f61d52ce93", "split": "train"}, {"article_id": "Miniature Haptics: Experiencing Haptic Feedback through Hand-based and Embodied Avatars", "description": {"raw": "A pie chart showing distribution of the most preferred interaction method for the football experience. It shows that Miniature Haptics was the most preferred interaction method at 58%. The controller and finger walking are 25% and 17% respectively.", "tokens": ["A", "pie", "chart", "showing", "distribution", "of", "the", "most", "preferred", "interaction", "method", "for", "the", "football", "experience", ".", "It", "shows", "that", "Miniature", "Haptics", "was", "the", "most", "preferred", "interaction", "method", "at", "58", "%", ".", "The", "controller", "and", "finger", "walking", "are", "25", "%", "and", "17", "%", "respectively", "."]}, "caption": {"raw": "Figure 9. Distribution of the most preferred interaction method for the football experience, showing Miniature Haptics was the most preferred interaction method.", "tokens": ["Figure", "9", ".", "Distribution", "of", "the", "most", "preferred", "interaction", "method", "for", "the", "football", "experience", ",", "showing", "Miniature", "Haptics", "was", "the", "most", "preferred", "interaction", "method", "."]}, "context": {"raw": "Miniature Haptics: Experiencing Haptic Feedback through Hand-based and Embodied Avatars Figure 9. Distribution of the most preferred interaction method for the football experience, showing Miniature Haptics was the most preferred interaction method.", "tokens": ["Miniature", "Haptics", ":", "Experiencing", "Haptic", "Feedback", "through", "Hand-based", "and", "Embodied", "Avatars", "Figure", "9", ".", "Distribution", "of", "the", "most", "preferred", "interaction", "method", "for", "the", "football", "experience", ",", "showing", "Miniature", "Haptics", "was", "the", "most", "preferred", "interaction", "method", "."]}, "filename": "ac5ed73b8241b1de45a45ca6aad1e34726dc236b_Image_009.jpg", "orig_filename": "ac5ed73b8241b1de45a45ca6aad1e34726dc236b", "split": "train"}, {"article_id": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives", "description": {"raw": "A plot of the stiffness of traps that approach each other horizontally versus vertically. The plots (top) show that the stiffness varies considerably in the vertical case, whereas horizontally merged traps have relatively stable stiffness along the way. The bottom part shows the pressure fields during the motion until they are merged (rightmost).", "tokens": ["A", "plot", "of", "the", "stiffness", "of", "traps", "that", "approach", "each", "other", "horizontally", "versus", "vertically", ".", "The", "plots", "(", "top", ")", "show", "that", "the", "stiffness", "varies", "considerably", "in", "the", "vertical", "case", ",", "whereas", "horizontally", "merged", "traps", "have", "relatively", "stable", "stiffness", "along", "the", "way", ".", "The", "bottom", "part", "shows", "the", "pressure", "fields", "during", "the", "motion", "until", "they", "are", "merged", "(", "rightmost", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives ", "tokens": ["ArticuLev", ":", "An", "Integrated", "Self-Assembly", "Pipeline", "for", "Articulated", "Multi-Bead", "Levitation", "Primitives"]}, "filename": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02_Image_024.jpg", "orig_filename": "ea1eca1816100eddedc30c9e55a4ae0ee83fce02", "split": "train"}, {"article_id": "A haptic ATM interface to assist visually impaired users", "description": {"raw": "bar chart showing average completion times for finding active devices:  card = 13.5 seconds  cash = 8 seconds  receipt 8.9 seconds", "tokens": ["bar", "chart", "showing", "average", "completion", "times", "for", "finding", "active", "devices", ":", "card", "=", "13.5", "seconds", "cash", "=", "8", "seconds", "receipt", "8.9", "seconds"]}, "caption": {"raw": "Figure 8: Average Device Location Times", "tokens": ["Figure", "8", ":", "Average", "Device", "Location", "Times"]}, "context": {"raw": "A haptic ATM interface to assist visually impaired users Figure 8: Average Device Location Times", "tokens": ["A", "haptic", "ATM", "interface", "to", "assist", "visually", "impaired", "users", "Figure", "8", ":", "Average", "Device", "Location", "Times"]}, "filename": "710f0efc81ddd7ccbfc427995a7e2ed53da72046_Image_008.gif", "orig_filename": "710f0efc81ddd7ccbfc427995a7e2ed53da72046", "split": "train"}, {"article_id": "Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users", "description": {"raw": "Two graphs: Comprehension and Subjective. \nComprehension\ny-axis: % of squared R and x-axis: ten bar charts with confidence intervals. The values for each of the bars are given as (percentage, lower, upper):\nSchoolType: 0.4756, 0.1884, 0.6030\nGameGroup: 0.1011, 0.0252, 0.2820\nGender: 0.0312, 0.0071, 0.1350\nDescribe: 0.0608, 0.0083, 0.2200\nWhenBecome: 0.0394, 0.0043, 0.1790\nHomeASL: 0.0520, 0.0054, 0.2090\nInternetSearch: 0.1240, 0.0196, 0.2930\nPositiveAttitudes: 0.0536, 0.0064, 0.1530\nASLChat: 0.0502, 0.0083, 0.1940\nSeenBefore: 0.0121, 0.0047, 0.1100\nSubjective\ny-axis: % of squared R and x-axis: six bar charts with confidence intervals. The values for each of the bars are given as (percentage, lower, upper):\nSchoolType: 0.1149, 0.0245, 0.3400\nWhenLearn: 0.0260, 0.0073, 0.1680\nHomeASL: 0.2561, 0.0430, 0.5210\nComputerComplex: 0.0478, 0.0031, 0.2330\nMediaSharingSubscale: 0.3158, 0.0595, 0.5490\nAnimationAttitude: 0.2394, 0.0270, 0.4690", "tokens": ["Two", "graphs", ":", "Comprehension", "and", "Subjective", ".", "Comprehension", "y-axis", ":", "%", "of", "squared", "R", "and", "x-axis", ":", "ten", "bar", "charts", "with", "confidence", "intervals", ".", "The", "values", "for", "each", "of", "the", "bars", "are", "given", "as", "(", "percentage", ",", "lower", ",", "upper", ")", ":", "SchoolType", ":", "0.4756", ",", "0.1884", ",", "0.6030", "GameGroup", ":", "0.1011", ",", "0.0252", ",", "0.2820", "Gender", ":", "0.0312", ",", "0.0071", ",", "0.1350", "Describe", ":", "0.0608", ",", "0.0083", ",", "0.2200", "WhenBecome", ":", "0.0394", ",", "0.0043", ",", "0.1790", "HomeASL", ":", "0.0520", ",", "0.0054", ",", "0.2090", "InternetSearch", ":", "0.1240", ",", "0.0196", ",", "0.2930", "PositiveAttitudes", ":", "0.0536", ",", "0.0064", ",", "0.1530", "ASLChat", ":", "0.0502", ",", "0.0083", ",", "0.1940", "SeenBefore", ":", "0.0121", ",", "0.0047", ",", "0.1100", "Subjective", "y-axis", ":", "%", "of", "squared", "R", "and", "x-axis", ":", "six", "bar", "charts", "with", "confidence", "intervals", ".", "The", "values", "for", "each", "of", "the", "bars", "are", "given", "as", "(", "percentage", ",", "lower", ",", "upper", ")", ":", "SchoolType", ":", "0.1149", ",", "0.0245", ",", "0.3400", "WhenLearn", ":", "0.0260", ",", "0.0073", ",", "0.1680", "HomeASL", ":", "0.2561", ",", "0.0430", ",", "0.5210", "ComputerComplex", ":", "0.0478", ",", "0.0031", ",", "0.2330", "MediaSharingSubscale", ":", "0.3158", ",", "0.0595", ",", "0.5490", "AnimationAttitude", ":", "0.2394", ",", "0.0270", ",", "0.4690"]}, "caption": {"raw": "Figure 3: Relative importance (normalized to sum to 100%) of factors in Comprehension Model 2 and in Subjective Model 2, with 95% bootstrap confidence intervals.", "tokens": ["Figure", "3", ":", "Relative", "importance", "(", "normalized", "to", "sum", "to", "100", "%", ")", "of", "factors", "in", "Comprehension", "Model", "2", "and", "in", "Subjective", "Model", "2", ",", "with", "95", "%", "bootstrap", "confidence", "intervals", "."]}, "context": {"raw": "Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users Figure 3: Relative importance (normalized to sum to 100%) of factors in Comprehension Model 2 and in Subjective Model 2, with 95% bootstrap confidence intervals.", "tokens": ["Demographic", "and", "Experiential", "Factors", "Influencing", "Acceptance", "of", "Sign", "Language", "Animation", "by", "Deaf", "Users", "Figure", "3", ":", "Relative", "importance", "(", "normalized", "to", "sum", "to", "100", "%", ")", "of", "factors", "in", "Comprehension", "Model", "2", "and", "in", "Subjective", "Model", "2", ",", "with", "95", "%", "bootstrap", "confidence", "intervals", "."]}, "filename": "e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab_Image_007.gif", "orig_filename": "e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab", "split": "train"}, {"article_id": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "description": {"raw": "Figure 4: In the top left there are two versions of the visual widget presented in Figure 5. The one of the left is identical to the one in Figure 5, and the one on the right has 2 additional smaller wedges, one on the far left and one on the far right. Beneath these visual widgets are three yellow rubber ducks with varying fill-levels; the far-left one is at a low fill-level, with opaque yellow at the bottom but transparent above; the duck in the middle is half-filled; and the duck on the right is completely filled. In the top-right of the figure is the Apple menu bar, of notice on the left side are red, blue, and orange circles with different fill-levels (low fill level means mostly transparent except for the very bottom). In the bottom-right of the figure is a check-list kind of interface, where each item has icons representing the design intent (lightbulb), process (tools), important (star), and problem (alert) items.", "tokens": ["Figure", "4", ":", "In", "the", "top", "left", "there", "are", "two", "versions", "of", "the", "visual", "widget", "presented", "in", "Figure", "5", ".", "The", "one", "of", "the", "left", "is", "identical", "to", "the", "one", "in", "Figure", "5", ",", "and", "the", "one", "on", "the", "right", "has", "2", "additional", "smaller", "wedges", ",", "one", "on", "the", "far", "left", "and", "one", "on", "the", "far", "right", ".", "Beneath", "these", "visual", "widgets", "are", "three", "yellow", "rubber", "ducks", "with", "varying", "fill-levels", ";", "the", "far-left", "one", "is", "at", "a", "low", "fill-level", ",", "with", "opaque", "yellow", "at", "the", "bottom", "but", "transparent", "above", ";", "the", "duck", "in", "the", "middle", "is", "half-filled", ";", "and", "the", "duck", "on", "the", "right", "is", "completely", "filled", ".", "In", "the", "top-right", "of", "the", "figure", "is", "the", "Apple", "menu", "bar", ",", "of", "notice", "on", "the", "left", "side", "are", "red", ",", "blue", ",", "and", "orange", "circles", "with", "different", "fill-levels", "(", "low", "fill", "level", "means", "mostly", "transparent", "except", "for", "the", "very", "bottom", ")", ".", "In", "the", "bottom-right", "of", "the", "figure", "is", "a", "check-list", "kind", "of", "interface", ",", "where", "each", "item", "has", "icons", "representing", "the", "design", "intent", "(", "lightbulb", ")", ",", "process", "(", "tools", ")", ",", "important", "(", "star", ")", ",", "and", "problem", "(", "alert", ")", "items", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture ", "tokens": ["Think-Aloud", "Computing", ":", "Supporting", "Rich", "and", "Low-Effort", "Knowledge", "Capture"]}, "filename": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_008.jpg", "orig_filename": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "split": "train"}, {"article_id": "Speed-Dial: A Surrogate Mouse for Non-Visual Web Browsing", "description": {"raw": "Screen shot of an HTML data-grid view (background), and its equivalent custom data-grid view for Dial foreground).", "tokens": ["Screen", "shot", "of", "an", "HTML", "data-grid", "view", "(", "background", ")", ",", "and", "its", "equivalent", "custom", "data-grid", "view", "for", "Dial", "foreground", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Speed-Dial: A Surrogate Mouse for Non-Visual Web Browsing ", "tokens": ["Speed-Dial", ":", "A", "Surrogate", "Mouse", "for", "Non-Visual", "Web", "Browsing"]}, "filename": "d70046e4a525a66a853d01bfd586448bd0fad4bb_Image_013.jpg", "orig_filename": "d70046e4a525a66a853d01bfd586448bd0fad4bb", "split": "train"}, {"article_id": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "description": {"raw": "Interval plot showing scheduled arrival time and predicted arrival time as lines, wtih a dark shaded region indicating the 50% quantile and light shaded region indicating the 95% quantile times.", "tokens": ["Interval", "plot", "showing", "scheduled", "arrival", "time", "and", "predicted", "arrival", "time", "as", "lines", ",", "wtih", "a", "dark", "shaded", "region", "indicating", "the", "50", "%", "quantile", "and", "light", "shaded", "region", "indicating", "the", "95", "%", "quantile", "times", "."]}, "caption": {"raw": "Interval plots are perhaps the most common uncertainty visualization. We tested interval plots to see if the familiarity of the visualization", "tokens": ["Interval", "plots", "are", "perhaps", "the", "most", "common", "uncertainty", "visualization", ".", "We", "tested", "interval", "plots", "to", "see", "if", "the", "familiarity", "of", "the", "visualization"]}, "context": {"raw": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making Interval plots are perhaps the most common uncertainty visualization. We tested interval plots to see if the familiarity of the visualization", "tokens": ["Uncertainty", "Displays", "Using", "Quantile", "Dotplots", "or", "CDFs", "Improve", "Transit", "Decision-Making", "Interval", "plots", "are", "perhaps", "the", "most", "common", "uncertainty", "visualization", ".", "We", "tested", "interval", "plots", "to", "see", "if", "the", "familiarity", "of", "the", "visualization"]}, "filename": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_006.jpg", "orig_filename": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "split": "train"}, {"article_id": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", "description": {"raw": "The bar graph for precision and recall for three different conditions. Total six bars are present. (Precision/Recall for three condition from C1, C2, to C3). The values are as follows: C1 Precision : 86.9%, C1 Recall 62.7%, C2 Precision 94.7%, C2 Recall 66.8%, C3 Precision 99.2%, C3 Recall 90.0%", "tokens": ["The", "bar", "graph", "for", "precision", "and", "recall", "for", "three", "different", "conditions", ".", "Total", "six", "bars", "are", "present", ".", "(", "Precision/Recall", "for", "three", "condition", "from", "C1", ",", "C2", ",", "to", "C3", ")", ".", "The", "values", "are", "as", "follows", ":", "C1", "Precision", ":", "86.9", "%", ",", "C1", "Recall", "62.7", "%", ",", "C2", "Precision", "94.7", "%", ",", "C2", "Recall", "66.8", "%", ",", "C3", "Precision", "99.2", "%", ",", "C3", "Recall", "90.0", "%"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces ", "tokens": ["SketchExpress", ":", "Remixing", "Animations", "for", "More", "Effective", "Crowd-Powered", "Prototyping", "of", "Interactive", "Interfaces"]}, "filename": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c_Image_007.jpg", "orig_filename": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c", "split": "train"}, {"article_id": "Enabling Designers to Foresee Which Colors Users Cannot See", "description": {"raw": "Histogram plot of mean discrimination volumes (bin size = 250) for our participants. Rises sharply from 0-250 (over 500 participants) to a peak at 500-750 (over 3000 participants). Histogram then falls off with a very long tail. Minimum volume is 21.68, 25th quartile is at volume 804.62, 50th quartile (median) is at volume 1558.38, 75th quartile is at volume 3223.60, maximum volume is 1058397.75. Histogram is cut off above volumes of 12000.", "tokens": ["Histogram", "plot", "of", "mean", "discrimination", "volumes", "(", "bin", "size", "=", "250", ")", "for", "our", "participants", ".", "Rises", "sharply", "from", "0-250", "(", "over", "500", "participants", ")", "to", "a", "peak", "at", "500-750", "(", "over", "3000", "participants", ")", ".", "Histogram", "then", "falls", "off", "with", "a", "very", "long", "tail", ".", "Minimum", "volume", "is", "21.68", ",", "25th", "quartile", "is", "at", "volume", "804.62", ",", "50th", "quartile", "(", "median", ")", "is", "at", "volume", "1558.38", ",", "75th", "quartile", "is", "at", "volume", "3223.60", ",", "maximum", "volume", "is", "1058397.75", ".", "Histogram", "is", "cut", "off", "above", "volumes", "of", "12000", "."]}, "caption": {"raw": "Figure 4: Histogram of number of participants versus mean discrimination ellipsoid volumes. Participants with ellipsoid volumes above 12,000 are not shown for space reasons.", "tokens": ["Figure", "4", ":", "Histogram", "of", "number", "of", "participants", "versus", "mean", "discrimination", "ellipsoid", "volumes", ".", "Participants", "with", "ellipsoid", "volumes", "above", "12,000", "are", "not", "shown", "for", "space", "reasons", "."]}, "context": {"raw": "Enabling Designers to Foresee Which Colors Users Cannot See Figure 4: Histogram of number of participants versus mean discrimination ellipsoid volumes. Participants with ellipsoid volumes above 12,000 are not shown for space reasons.", "tokens": ["Enabling", "Designers", "to", "Foresee", "Which", "Colors", "Users", "Can", "not", "See", "Figure", "4", ":", "Histogram", "of", "number", "of", "participants", "versus", "mean", "discrimination", "ellipsoid", "volumes", ".", "Participants", "with", "ellipsoid", "volumes", "above", "12,000", "are", "not", "shown", "for", "space", "reasons", "."]}, "filename": "1110852f906fe08d42814a051e4e8c15f0b930b5_Image_005.jpg", "orig_filename": "1110852f906fe08d42814a051e4e8c15f0b930b5", "split": "train"}, {"article_id": "Pin-and-Cross: A Unimanual Multitouch Technique Combining Static Touches with Crossing Selection", "description": {"raw": "Graphs of results for each task. Each graph has three lines: A middle line that shows time in milliseconds vs angle, and two lines that show the 95% confidence intervals.There are also dotted horizontal lines for preference and error rate.", "tokens": ["Graphs", "of", "results", "for", "each", "task", ".", "Each", "graph", "has", "three", "lines", ":", "A", "middle", "line", "that", "shows", "time", "in", "milliseconds", "vs", "angle", ",", "and", "two", "lines", "that", "show", "the", "95", "%", "confidence", "intervals.There", "are", "also", "dotted", "horizontal", "lines", "for", "preference", "and", "error", "rate", "."]}, "caption": {"raw": "Figure 3. Time, error, and preference by crossing target Angle for each Task (lower values are better, 95% CI shown for time).", "tokens": ["Figure", "3", ".", "Time", ",", "error", ",", "and", "preference", "by", "crossing", "target", "Angle", "for", "each", "Task", "(", "lower", "values", "are", "better", ",", "95", "%", "CI", "shown", "for", "time", ")", "."]}, "context": {"raw": "Pin-and-Cross: A Unimanual Multitouch Technique Combining Static Touches with Crossing Selection Figure 3. Time, error, and preference by crossing target Angle for each Task (lower values are better, 95% CI shown for time).", "tokens": ["Pin-and-Cross", ":", "A", "Unimanual", "Multitouch", "Technique", "Combining", "Static", "Touches", "with", "Crossing", "Selection", "Figure", "3", ".", "Time", ",", "error", ",", "and", "preference", "by", "crossing", "target", "Angle", "for", "each", "Task", "(", "lower", "values", "are", "better", ",", "95", "%", "CI", "shown", "for", "time", ")", "."]}, "filename": "7940222e83f0bea7bea567509fbf9dd90c735230_Image_003.png", "orig_filename": "7940222e83f0bea7bea567509fbf9dd90c735230", "split": "train"}, {"article_id": "Measuring text simplification with the crowd", "description": {"raw": "Figure 2: Average worker ratings of all 10 trial sentences as dif- ferent canonical rules are applied. Different rules have differ- ent effects although generally the trend is to increase simplicity as expected.", "tokens": ["Figure", "2", ":", "Average", "worker", "ratings", "of", "all", "10", "trial", "sentences", "as", "dif-", "ferent", "canonical", "rules", "are", "applied", ".", "Different", "rules", "have", "differ-", "ent", "effects", "although", "generally", "the", "trend", "is", "to", "increase", "simplicity", "as", "expected", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Measuring text simplification with the crowd ", "tokens": ["Measuring", "text", "simplification", "with", "the", "crowd"]}, "filename": "cbd7e92b327dc9e54024aaf0a1ff39a45dc20623_Image_003.png", "orig_filename": "cbd7e92b327dc9e54024aaf0a1ff39a45dc20623", "split": "train"}, {"article_id": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning", "description": {"raw": "The radar chart from T1 shows plots from 6 previous sessions as well as the yet unmodified plot for the 7th session. It shows how the teachers' focus may change from session to session.", "tokens": ["The", "radar", "chart", "from", "T1", "shows", "plots", "from", "6", "previous", "sessions", "as", "well", "as", "the", "yet", "unmodified", "plot", "for", "the", "7th", "session", ".", "It", "shows", "how", "the", "teachers", "'", "focus", "may", "change", "from", "session", "to", "session", "."]}, "caption": {"raw": "Figure 6. Graphs of 6 sessions by T1 with the yet unmodified graph of session 7.", "tokens": ["Figure", "6", ".", "Graphs", "of", "6", "sessions", "by", "T1", "with", "the", "yet", "unmodified", "graph", "of", "session", "7", "."]}, "context": {"raw": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning Figure 6. Graphs of 6 sessions by T1 with the yet unmodified graph of session 7.", "tokens": ["Group", "Spinner", ":", "Recognizing", "&", "Visualizing", "Learning", "in", "the", "Classroom", "for", "Reflection", ",", "Communication", "&", "Planning", "Figure", "6", ".", "Graphs", "of", "6", "sessions", "by", "T1", "with", "the", "yet", "unmodified", "graph", "of", "session", "7", "."]}, "filename": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a_Image_011.jpg", "orig_filename": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a", "split": "train"}, {"article_id": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "description": {"raw": "This figure shows a stacked bar plot showing percentages of 5-point Likert-scale responses. The title is “I would be likely to use:”, and the y-axis has 3 categories; \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. There is a bracket indicating p<.01 significance between the “Automatic” and “Pop-up” plots. There is also a bracket indicating p<.001 significance between the “Automatic” and “Decoration” plots. The Likert-scale responses for “Neutral” is centered, and “Disagree” and “Strongly Disagree” are on the left side, and “Agree” and “Strongly Agree” are on the right side. It can be seen that a majority of the responses for “Pop-up” and “Decoration” are either “Agree” or “Strongly Agree”.\n\nFor \"Automatic\" 28% responded “Strongly Disagree”, 28% for “Disagree”, 24% for “Neutral”, 12% for “Agree”, and 8% for “Strongly Agree”.\nFor \"Pop-up\" 4% responded “Strongly Disagree”, 8% for “Disagree”, 8% for “Neutral”, 28% for “Agree”, and 52% for “Strongly Agree”. \nFor \"Decoration\" 4% responded “Strongly Disagree”, 8% for “Disagree”, 8% for “Neutral”, 36% for “Agree”, and 44% for “Strongly Agree”.", "tokens": ["This", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "5-point", "Likert-scale", "responses", ".", "The", "title", "is", "“", "I", "would", "be", "likely", "to", "use", ":", "”", ",", "and", "the", "y-axis", "has", "3", "categories", ";", "``", "Automatic", "''", ",", "``", "Pop-up", "''", ",", "and", "``", "Decoration", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "There", "is", "a", "bracket", "indicating", "p", "<", ".01", "significance", "between", "the", "“", "Automatic", "”", "and", "“", "Pop-up", "”", "plots", ".", "There", "is", "also", "a", "bracket", "indicating", "p", "<", ".001", "significance", "between", "the", "“", "Automatic", "”", "and", "“", "Decoration", "”", "plots", ".", "The", "Likert-scale", "responses", "for", "“", "Neutral", "”", "is", "centered", ",", "and", "“", "Disagree", "”", "and", "“", "Strongly", "Disagree", "”", "are", "on", "the", "left", "side", ",", "and", "“", "Agree", "”", "and", "“", "Strongly", "Agree", "”", "are", "on", "the", "right", "side", ".", "It", "can", "be", "seen", "that", "a", "majority", "of", "the", "responses", "for", "“", "Pop-up", "”", "and", "“", "Decoration", "”", "are", "either", "“", "Agree", "”", "or", "“", "Strongly", "Agree", "”", ".", "For", "``", "Automatic", "''", "28", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "28", "%", "for", "“", "Disagree", "”", ",", "24", "%", "for", "“", "Neutral", "”", ",", "12", "%", "for", "“", "Agree", "”", ",", "and", "8", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Pop-up", "''", "4", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "8", "%", "for", "“", "Disagree", "”", ",", "8", "%", "for", "“", "Neutral", "”", ",", "28", "%", "for", "“", "Agree", "”", ",", "and", "52", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Decoration", "''", "4", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "8", "%", "for", "“", "Disagree", "”", ",", "8", "%", "for", "“", "Neutral", "”", ",", "36", "%", "for", "“", "Agree", "”", ",", "and", "44", "%", "for", "“", "Strongly", "Agree", "”", "."]}, "caption": {"raw": "Figure 5. Participants’ agreement to a Likert-scale question, presented at the end of the study, as to whether they would be likely to use each of the three simplification conditions, with asterisks marking significant pairwise differences (** p<0.01,", "tokens": ["Figure", "5", ".", "Participants", "’", "agreement", "to", "a", "Likert-scale", "question", ",", "presented", "at", "the", "end", "of", "the", "study", ",", "as", "to", "whether", "they", "would", "be", "likely", "to", "use", "each", "of", "the", "three", "simplification", "conditions", ",", "with", "asterisks", "marking", "significant", "pairwise", "differences", "(", "*", "*", "p", "<", "0.01", ","]}, "context": {"raw": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy Figure 5. Participants’ agreement to a Likert-scale question, presented at the end of the study, as to whether they would be likely to use each of the three simplification conditions, with asterisks marking significant pairwise differences (** p<0.01,", "tokens": ["Automatic", "Text", "Simplification", "Tools", "for", "Deaf", "and", "Hard", "of", "Hearing", "Adults", ":", "Benefits", "of", "Lexical", "Simplification", "and", "Providing", "Users", "with", "Autonomy", "Figure", "5", ".", "Participants", "’", "agreement", "to", "a", "Likert-scale", "question", ",", "presented", "at", "the", "end", "of", "the", "study", ",", "as", "to", "whether", "they", "would", "be", "likely", "to", "use", "each", "of", "the", "three", "simplification", "conditions", ",", "with", "asterisks", "marking", "significant", "pairwise", "differences", "(", "*", "*", "p", "<", "0.01", ","]}, "filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_008.jpg", "orig_filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "split": "train"}, {"article_id": "Improving Dwell-Based Gaze Typing with Dynamic, Cascading Dwell Times", "description": {"raw": "Two graphs. The graph on the left shows the text entry speeds for each technique over all eight sessions with learning curves fit to the speed data. The graph on the right shows corrected error rates for each technique over all sessions.", "tokens": ["Two", "graphs", ".", "The", "graph", "on", "the", "left", "shows", "the", "text", "entry", "speeds", "for", "each", "technique", "over", "all", "eight", "sessions", "with", "learning", "curves", "fit", "to", "the", "speed", "data", ".", "The", "graph", "on", "the", "right", "shows", "corrected", "error", "rates", "for", "each", "technique", "over", "all", "sessions", "."]}, "caption": {"raw": "Figure 5. (Left) Text entry speeds (in WPM) for each technique over all eight sessions and learning curves fit to the speed data modeled by the function y=axb. (Right) Corrected error rates for each technique over all sessions.", "tokens": ["Figure", "5", ".", "(", "Left", ")", "Text", "entry", "speeds", "(", "in", "WPM", ")", "for", "each", "technique", "over", "all", "eight", "sessions", "and", "learning", "curves", "fit", "to", "the", "speed", "data", "modeled", "by", "the", "function", "y=axb", ".", "(", "Right", ")", "Corrected", "error", "rates", "for", "each", "technique", "over", "all", "sessions", "."]}, "context": {"raw": "Improving Dwell-Based Gaze Typing with Dynamic, Cascading Dwell Times Figure 5. (Left) Text entry speeds (in WPM) for each technique over all eight sessions and learning curves fit to the speed data modeled by the function y=axb. (Right) Corrected error rates for each technique over all sessions.", "tokens": ["Improving", "Dwell-Based", "Gaze", "Typing", "with", "Dynamic", ",", "Cascading", "Dwell", "Times", "Figure", "5", ".", "(", "Left", ")", "Text", "entry", "speeds", "(", "in", "WPM", ")", "for", "each", "technique", "over", "all", "eight", "sessions", "and", "learning", "curves", "fit", "to", "the", "speed", "data", "modeled", "by", "the", "function", "y=axb", ".", "(", "Right", ")", "Corrected", "error", "rates", "for", "each", "technique", "over", "all", "sessions", "."]}, "filename": "415c0ef55b1c603af7ed537f4aede720d80f34bd_Image_007.jpg", "orig_filename": "415c0ef55b1c603af7ed537f4aede720d80f34bd", "split": "train"}, {"article_id": "Gesture-based Interaction for Individuals with Developmental Disabilities in India", "description": {"raw": "The bar graph consists of the math scores from 0 to 10 on the y axis with participants on the x axis. For each participant, her pre and post test math score and post test math) is displayed. The solid blue bar is the pre test score and the lined blue bar is the post test math score. For P1 the pre test score is 9.5 and post test is 8, for P2, both pre and post scrores are 5, for P3 pre test score is 4.5 and post test is 6, for P4 pre test score is 8.5 and post is 7, for P5 and P6 pre test score is 6 and post 7, for P7 both pre test and post are 7, for P8 both pre and post test are 6, for P9 pre test is 6 and post 6.5, for P10 pre test is 3 and post is 8, for P11 pre test is 4.5 and post is 7.5, for P12 pre test is 3 and post is 6 , for P13 pre is 3 and post 5, for P14 pre is 8 and post is 7.5, for P15 pre is 8 and post is 6.5, for P16 pre is 5 and post is 6.5, for P17 pre is 8 and post is 9 and for P18, pre is 7.5 and post is 8.", "tokens": ["The", "bar", "graph", "consists", "of", "the", "math", "scores", "from", "0", "to", "10", "on", "the", "y", "axis", "with", "participants", "on", "the", "x", "axis", ".", "For", "each", "participant", ",", "her", "pre", "and", "post", "test", "math", "score", "and", "post", "test", "math", ")", "is", "displayed", ".", "The", "solid", "blue", "bar", "is", "the", "pre", "test", "score", "and", "the", "lined", "blue", "bar", "is", "the", "post", "test", "math", "score", ".", "For", "P1", "the", "pre", "test", "score", "is", "9.5", "and", "post", "test", "is", "8", ",", "for", "P2", ",", "both", "pre", "and", "post", "scrores", "are", "5", ",", "for", "P3", "pre", "test", "score", "is", "4.5", "and", "post", "test", "is", "6", ",", "for", "P4", "pre", "test", "score", "is", "8.5", "and", "post", "is", "7", ",", "for", "P5", "and", "P6", "pre", "test", "score", "is", "6", "and", "post", "7", ",", "for", "P7", "both", "pre", "test", "and", "post", "are", "7", ",", "for", "P8", "both", "pre", "and", "post", "test", "are", "6", ",", "for", "P9", "pre", "test", "is", "6", "and", "post", "6.5", ",", "for", "P10", "pre", "test", "is", "3", "and", "post", "is", "8", ",", "for", "P11", "pre", "test", "is", "4.5", "and", "post", "is", "7.5", ",", "for", "P12", "pre", "test", "is", "3", "and", "post", "is", "6", ",", "for", "P13", "pre", "is", "3", "and", "post", "5", ",", "for", "P14", "pre", "is", "8", "and", "post", "is", "7.5", ",", "for", "P15", "pre", "is", "8", "and", "post", "is", "6.5", ",", "for", "P16", "pre", "is", "5", "and", "post", "is", "6.5", ",", "for", "P17", "pre", "is", "8", "and", "post", "is", "9", "and", "for", "P18", ",", "pre", "is", "7.5", "and", "post", "is", "8", "."]}, "caption": {"raw": "Figure 7: Phase I mathematical test scores with improvements seen in Phase III for participants P1 – P18", "tokens": ["Figure", "7", ":", "Phase", "I", "mathematical", "test", "scores", "with", "improvements", "seen", "in", "Phase", "III", "for", "participants", "P1", "–", "P18"]}, "context": {"raw": "Gesture-based Interaction for Individuals with Developmental Disabilities in India Figure 7: Phase I mathematical test scores with improvements seen in Phase III for participants P1 – P18", "tokens": ["Gesture-based", "Interaction", "for", "Individuals", "with", "Developmental", "Disabilities", "in", "India", "Figure", "7", ":", "Phase", "I", "mathematical", "test", "scores", "with", "improvements", "seen", "in", "Phase", "III", "for", "participants", "P1", "–", "P18"]}, "filename": "fc76954a4eca8adbdbec961d479c22e6d0d1a1cb_Image_009.png", "orig_filename": "fc76954a4eca8adbdbec961d479c22e6d0d1a1cb", "split": "train"}, {"article_id": "How Relevant is Hick's Law for HCI?", "description": {"raw": "Reanalysis of data from Roy et al. and Liu et al.: reaction time as a function of stimulus uncertainty.", "tokens": ["Reanalysis", "of", "data", "from", "Roy", "et", "al", ".", "and", "Liu", "et", "al", ".", ":", "reaction", "time", "as", "a", "function", "of", "stimulus", "uncertainty", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "How Relevant is Hick's Law for HCI? ", "tokens": ["How", "Relevant", "is", "Hick", "'s", "Law", "for", "HCI", "?"]}, "filename": "cd857ed14546749328da5ddbc173e34a31ddb499_Image_006.jpg", "orig_filename": "cd857ed14546749328da5ddbc173e34a31ddb499", "split": "train"}, {"article_id": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "description": {"raw": "This figure is a line graph with frame index number on the horizontal axis and number of edge lines on the vertical axis. It shows both the original and filtered result. In the filtered result, the peaks are prominent and easier to identify.", "tokens": ["This", "figure", "is", "a", "line", "graph", "with", "frame", "index", "number", "on", "the", "horizontal", "axis", "and", "number", "of", "edge", "lines", "on", "the", "vertical", "axis", ".", "It", "shows", "both", "the", "original", "and", "filtered", "result", ".", "In", "the", "filtered", "result", ",", "the", "peaks", "are", "prominent", "and", "easier", "to", "identify", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera ", "tokens": ["A", "Computer", "Vision-Based", "System", "for", "Stride", "Length", "Estimation", "using", "a", "Mobile", "Phone", "Camera"]}, "filename": "84e867b0e07ef33492478682a68899d99ea211d8_Image_020.jpg", "orig_filename": "84e867b0e07ef33492478682a68899d99ea211d8", "split": "train"}, {"article_id": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "description": {"raw": "Bar chart with average time on each visual sources for DHH participants: slides, ASL, and instructor. Differences between control and SlidePacer conditions are significant for all visual sources.", "tokens": ["Bar", "chart", "with", "average", "time", "on", "each", "visual", "sources", "for", "DHH", "participants", ":", "slides", ",", "ASL", ",", "and", "instructor", ".", "Differences", "between", "control", "and", "SlidePacer", "conditions", "are", "significant", "for", "all", "visual", "sources", "."]}, "caption": {"raw": "Figure 5. Average time DHH participants spent looking at each visual source.", "tokens": ["Figure", "5", ".", "Average", "time", "DHH", "participants", "spent", "looking", "at", "each", "visual", "source", "."]}, "context": {"raw": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students Figure 5. Average time DHH participants spent looking at each visual source.", "tokens": ["SlidePacer", ":", "A", "Presentation", "Delivery", "Tool", "for", "Instructors", "of", "Deaf", "and", "Hard", "of", "Hearing", "Students", "Figure", "5", ".", "Average", "time", "DHH", "participants", "spent", "looking", "at", "each", "visual", "source", "."]}, "filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_008.png", "orig_filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "split": "train"}, {"article_id": "Urban Accessibility as a Socio-Political Problem", "description": {"raw": "2-part diagram. Left half shows the four stakeholder groups laid along a diamond shape with arrows showing interactions. Each edge is also annotated by smaller arrows indicating the amount of interactions occuring between stakeholders.  Right half shows a list of interaction goals that occur on each edge of the graph.", "tokens": ["2-part", "diagram", ".", "Left", "half", "shows", "the", "four", "stakeholder", "groups", "laid", "along", "a", "diamond", "shape", "with", "arrows", "showing", "interactions", ".", "Each", "edge", "is", "also", "annotated", "by", "smaller", "arrows", "indicating", "the", "amount", "of", "interactions", "occuring", "between", "stakeholders", ".", "Right", "half", "shows", "a", "list", "of", "interaction", "goals", "that", "occur", "on", "each", "edge", "of", "the", "graph", "."]}, "caption": {"raw": "Figure 4. Civic Interaction Space. Illustrates the different civic interactions between the primary stakeholder groups and identifies six points of interactions. Groups are denoted by: CM=community includes, MI/caregivers and general public, A=advocates (and activists), D=department officials, and PM=policymakers. The perceived number of interactions between stakeholders is represented by the weight of the arrows. For example, high interactions between policymakers and department officials due to interdependent roles vs relatively less interactions between government officials and citizens.", "tokens": ["Figure", "4", ".", "Civic", "Interaction", "Space", ".", "Illustrates", "the", "different", "civic", "interactions", "between", "the", "primary", "stakeholder", "groups", "and", "identifies", "six", "points", "of", "interactions", ".", "Groups", "are", "denoted", "by", ":", "CM=community", "includes", ",", "MI/caregivers", "and", "general", "public", ",", "A=advocates", "(", "and", "activists", ")", ",", "D=department", "officials", ",", "and", "PM=policymakers", ".", "The", "perceived", "number", "of", "interactions", "between", "stakeholders", "is", "represented", "by", "the", "weight", "of", "the", "arrows", ".", "For", "example", ",", "high", "interactions", "between", "policymakers", "and", "department", "officials", "due", "to", "interdependent", "roles", "vs", "relatively", "less", "interactions", "between", "government", "officials", "and", "citizens", "."]}, "context": {"raw": "Urban Accessibility as a Socio-Political Problem Figure 4. Civic Interaction Space. Illustrates the different civic interactions between the primary stakeholder groups and identifies six points of interactions. Groups are denoted by: CM=community includes, MI/caregivers and general public, A=advocates (and activists), D=department officials, and PM=policymakers. The perceived number of interactions between stakeholders is represented by the weight of the arrows. For example, high interactions between policymakers and department officials due to interdependent roles vs relatively less interactions between government officials and citizens.", "tokens": ["Urban", "Accessibility", "as", "a", "Socio-Political", "Problem", "Figure", "4", ".", "Civic", "Interaction", "Space", ".", "Illustrates", "the", "different", "civic", "interactions", "between", "the", "primary", "stakeholder", "groups", "and", "identifies", "six", "points", "of", "interactions", ".", "Groups", "are", "denoted", "by", ":", "CM=community", "includes", ",", "MI/caregivers", "and", "general", "public", ",", "A=advocates", "(", "and", "activists", ")", ",", "D=department", "officials", ",", "and", "PM=policymakers", ".", "The", "perceived", "number", "of", "interactions", "between", "stakeholders", "is", "represented", "by", "the", "weight", "of", "the", "arrows", ".", "For", "example", ",", "high", "interactions", "between", "policymakers", "and", "department", "officials", "due", "to", "interdependent", "roles", "vs", "relatively", "less", "interactions", "between", "government", "officials", "and", "citizens", "."]}, "filename": "249599b991052f90d48141883b074bcb229fa8c3_Image_010.png", "orig_filename": "249599b991052f90d48141883b074bcb229fa8c3", "split": "train"}, {"article_id": "How We Type: Eye and Finger Movement Strategies in Mobile Typing", "description": {"raw": "Heatmap showing finger-to-key mapping in two-thumb touch data of three participants (all sentences aggregated). These patters are representative of a tendency we found in the data for the right thumb to cover more keys than the left. The right hand was the dominant hand for most of the participants, but the same pattern was observed also for the left-handed participants.", "tokens": ["Heatmap", "showing", "finger-to-key", "mapping", "in", "two-thumb", "touch", "data", "of", "three", "participants", "(", "all", "sentences", "aggregated", ")", ".", "These", "patters", "are", "representative", "of", "a", "tendency", "we", "found", "in", "the", "data", "for", "the", "right", "thumb", "to", "cover", "more", "keys", "than", "the", "left", ".", "The", "right", "hand", "was", "the", "dominant", "hand", "for", "most", "of", "the", "participants", ",", "but", "the", "same", "pattern", "was", "observed", "also", "for", "the", "left-handed", "participants", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "How We Type: Eye and Finger Movement Strategies in Mobile Typing ", "tokens": ["How", "We", "Type", ":", "Eye", "and", "Finger", "Movement", "Strategies", "in", "Mobile", "Typing"]}, "filename": "05300590913007eb710cd89f5d373e1ec0833bfa_Image_006.jpg", "orig_filename": "05300590913007eb710cd89f5d373e1ec0833bfa", "split": "train"}, {"article_id": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "description": {"raw": "Plot of Effect of Surfaced Hits on ML-1M. Shows boycotts and data strikes. As size of boycotts/data strikes increases, Surfaced Hits are reduced.", "tokens": ["Plot", "of", "Effect", "of", "Surfaced", "Hits", "on", "ML-1M", ".", "Shows", "boycotts", "and", "data", "strikes", ".", "As", "size", "of", "boycotts/data", "strikes", "increases", ",", "Surfaced", "Hits", "are", "reduced", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies ", "tokens": ["“", "Data", "Strikes", "”", ":", "Evaluating", "the", "Effectiveness", "of", "a", "New", "Form", "of", "Collective", "Action", "Against", "Technology", "Companies"]}, "filename": "41cbffad975874060d643c36c8bdb5c72637564e_Image_004.jpg", "orig_filename": "41cbffad975874060d643c36c8bdb5c72637564e", "split": "train"}, {"article_id": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "description": {"raw": "On the left is a bar graph of the average selection times for each participant. The maximum value is about 1150~ms (P2) while the minimum value is about 700~ms (P9). On the right is a histogram of the selection times of all participants. Most selections occurred between 500~ms and 800~ms. However, there were about 30 selections that took over 1500~ms.", "tokens": ["On", "the", "left", "is", "a", "bar", "graph", "of", "the", "average", "selection", "times", "for", "each", "participant", ".", "The", "maximum", "value", "is", "about", "1150~ms", "(", "P2", ")", "while", "the", "minimum", "value", "is", "about", "700~ms", "(", "P9", ")", ".", "On", "the", "right", "is", "a", "histogram", "of", "the", "selection", "times", "of", "all", "participants", ".", "Most", "selections", "occurred", "between", "500~ms", "and", "800~ms", ".", "However", ",", "there", "were", "about", "30", "selections", "that", "took", "over", "1500~ms", "."]}, "caption": {"raw": "Figure 7: Results of shape recognition task: (a) Average ac- curacy, (b) stimulus-response confusion matrix. (Error bar represents standard error)", "tokens": ["Figure", "7", ":", "Results", "of", "shape", "recognition", "task", ":", "(", "a", ")", "Average", "ac-", "curacy", ",", "(", "b", ")", "stimulus-response", "confusion", "matrix", ".", "(", "Error", "bar", "represents", "standard", "error", ")"]}, "context": {"raw": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects Figure 7: Results of shape recognition task: (a) Average ac- curacy, (b) stimulus-response confusion matrix. (Error bar represents standard error)", "tokens": ["ThroughHand", ":", "2D", "Tactile", "Interaction", "to", "Simultaneously", "Recognize", "and", "Touch", "Multiple", "Objects", "Figure", "7", ":", "Results", "of", "shape", "recognition", "task", ":", "(", "a", ")", "Average", "ac-", "curacy", ",", "(", "b", ")", "stimulus-response", "confusion", "matrix", ".", "(", "Error", "bar", "represents", "standard", "error", ")"]}, "filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_014.png", "orig_filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "split": "train"}, {"article_id": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion", "description": {"raw": "Figure 8 shows the result of the second experiment. The median response time for Shift is approximately 4000, for FBG 2600 and for PBG 2200 milliseconds.  The other bar chart shows the response time divided by interaction techniques and mobility conditions.  Shift-walking and shift standing are very close and they are the worst. PBG-walking and PBG standing are very close and they have the best performance", "tokens": ["Figure", "8", "shows", "the", "result", "of", "the", "second", "experiment", ".", "The", "median", "response", "time", "for", "Shift", "is", "approximately", "4000", ",", "for", "FBG", "2600", "and", "for", "PBG", "2200", "milliseconds", ".", "The", "other", "bar", "chart", "shows", "the", "response", "time", "divided", "by", "interaction", "techniques", "and", "mobility", "conditions", ".", "Shift-walking", "and", "shift", "standing", "are", "very", "close", "and", "they", "are", "the", "worst", ".", "PBG-walking", "and", "PBG", "standing", "are", "very", "close", "and", "they", "have", "the", "best", "performance"]}, "caption": {"raw": "Figure 8: Left: Overall average response times (ms) for Shift, FBG, and PBG. Right: average response times for each inter- action technique, separated out by mobility condition.", "tokens": ["Figure", "8", ":", "Left", ":", "Overall", "average", "response", "times", "(", "ms", ")", "for", "Shift", ",", "FBG", ",", "and", "PBG", ".", "Right", ":", "average", "response", "times", "for", "each", "inter-", "action", "technique", ",", "separated", "out", "by", "mobility", "condition", "."]}, "context": {"raw": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion Figure 8: Left: Overall average response times (ms) for Shift, FBG, and PBG. Right: average response times for each inter- action technique, separated out by mobility condition.", "tokens": ["BezelGlide", ":", "Interacting", "with", "Graphs", "on", "Smartwatches", "with", "Minimal", "Screen", "Occlusion", "Figure", "8", ":", "Left", ":", "Overall", "average", "response", "times", "(", "ms", ")", "for", "Shift", ",", "FBG", ",", "and", "PBG", ".", "Right", ":", "average", "response", "times", "for", "each", "inter-", "action", "technique", ",", "separated", "out", "by", "mobility", "condition", "."]}, "filename": "166d91ef569340148a9d77fe64330e4599239c2f_Image_013.png", "orig_filename": "166d91ef569340148a9d77fe64330e4599239c2f", "split": "train"}, {"article_id": "Motor-impaired touchscreen interactions in the wild", "description": {"raw": "bar graph showing the effect of the subject used to train the models, user specific group 59%, individual, 82.6%; session specific group 97%, individual 93.6% recognition accuracy", "tokens": ["bar", "graph", "showing", "the", "effect", "of", "the", "subject", "used", "to", "train", "the", "models", ",", "user", "specific", "group", "59", "%", ",", "individual", ",", "82.6", "%", ";", "session", "specific", "group", "97", "%", ",", "individual", "93.6", "%", "recognition", "accuracy"]}, "caption": {"raw": "Figure 5 Classification accuracy of tap gesture recognizers for touch models by subject condition", "tokens": ["Figure", "5", "Classification", "accuracy", "of", "tap", "gesture", "recognizers", "for", "touch", "models", "by", "subject", "condition"]}, "context": {"raw": "Motor-impaired touchscreen interactions in the wild Figure 5 Classification accuracy of tap gesture recognizers for touch models by subject condition", "tokens": ["Motor-impaired", "touchscreen", "interactions", "in", "the", "wild", "Figure", "5", "Classification", "accuracy", "of", "tap", "gesture", "recognizers", "for", "touch", "models", "by", "subject", "condition"]}, "filename": "c7cbe32734918f335533efd49f9c46e69e2e14b4_Image_006.jpg", "orig_filename": "c7cbe32734918f335533efd49f9c46e69e2e14b4", "split": "train"}, {"article_id": "What Makes Smartphone Use Meaningful or Meaningless?", "description": {"raw": "This line graph shows the percentage share of instrumental motivation for 5 types of use as timing goes from start, during, to end. The absolute change in instrumental motivation from the start to the end timing was: 0.0% for productivity, -4.0% for information, -6.0% for communication, -7.9% for entertainment, and -9.8% for social media.", "tokens": ["This", "line", "graph", "shows", "the", "percentage", "share", "of", "instrumental", "motivation", "for", "5", "types", "of", "use", "as", "timing", "goes", "from", "start", ",", "during", ",", "to", "end", ".", "The", "absolute", "change", "in", "instrumental", "motivation", "from", "the", "start", "to", "the", "end", "timing", "was", ":", "0.0", "%", "for", "productivity", ",", "-4.0", "%", "for", "information", ",", "-6.0", "%", "for", "communication", ",", "-7.9", "%", "for", "entertainment", ",", "and", "-9.8", "%", "for", "social", "media", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "What Makes Smartphone Use Meaningful or Meaningless? ", "tokens": ["What", "Makes", "Smartphone", "Use", "Meaningful", "or", "Meaningless", "?"]}, "filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_009.jpg", "orig_filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "split": "train"}, {"article_id": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "description": {"raw": "Figure 2 is a chart illustrating participants' ratings of their confidence in capturing all the important information while speaking while working. The chart has 6 mini bar charts. On the left are 3 mini bar charts for the concurrent think-aloud condition, and on the right are 3 mini bar charts for the retrospective think-aloud condition. For each condition, there is a mini bar chart per domain (i.e., coding, models, and slides). First we describe the concurrent think-aloud ratings: For concurrent think-aloud for the coding domain, there are 3 bars, from left: an orange bar of height 1 (i.e., 1 participant out of 4) for \"not confident\", a bar of height 0 for \"neutral\", and a green bar of height 3 for \"confident\". For concurrent think-aloud for the models domain, there are 3 bars, from left: an orange bar of height 2 for \"not confident\", a yellow bar of height 2 for \"neutral\", and a bar of height 0 for \"confident\". For concurrent think-aloud for the slides domain, there are 3 bars, from left: an orange bar of height 1 for \"not confident\", a bar of height 0 for \"neutral\", and a green bar of height 3 for \"confident\". For retrospective think-aloud for the coding domain, there are 3 bars, from left: an orange bar of height 2 for \"not confident\", a bar of height 0 for \"neutral\", and a green bar of height 2 for \"confident\". For retrospective think-aloud for the models domain, there are 3 bars, from left: a bar of height 0 for \"not confident\", a yellow bar of height 1 for \"neutral\", and a green bar of height 3 for \"confident\". For retrospective think-aloud for the slides domain, there are 3 bars, from left: a bar of height 0 for \"not confident\", a yellow bar of height 1 for \"neutral\", and a green bar of height 3 for \"confident\". Note that these categories are aggregations of the participant ratings from the 7-point Likert scale results (between 1-Very Not Confident, and 7-Very Confident). Ratings (1, 2, 3) have been aggregated into \"Not Confident\", rating (4) is \"Neutral\", and ratings (5, 6, 7) have been aggregated into \"Confident\".", "tokens": ["Figure", "2", "is", "a", "chart", "illustrating", "participants", "'", "ratings", "of", "their", "confidence", "in", "capturing", "all", "the", "important", "information", "while", "speaking", "while", "working", ".", "The", "chart", "has", "6", "mini", "bar", "charts", ".", "On", "the", "left", "are", "3", "mini", "bar", "charts", "for", "the", "concurrent", "think-aloud", "condition", ",", "and", "on", "the", "right", "are", "3", "mini", "bar", "charts", "for", "the", "retrospective", "think-aloud", "condition", ".", "For", "each", "condition", ",", "there", "is", "a", "mini", "bar", "chart", "per", "domain", "(", "i.e.", ",", "coding", ",", "models", ",", "and", "slides", ")", ".", "First", "we", "describe", "the", "concurrent", "think-aloud", "ratings", ":", "For", "concurrent", "think-aloud", "for", "the", "coding", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "an", "orange", "bar", "of", "height", "1", "(", "i.e.", ",", "1", "participant", "out", "of", "4", ")", "for", "``", "not", "confident", "''", ",", "a", "bar", "of", "height", "0", "for", "``", "neutral", "''", ",", "and", "a", "green", "bar", "of", "height", "3", "for", "``", "confident", "''", ".", "For", "concurrent", "think-aloud", "for", "the", "models", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "an", "orange", "bar", "of", "height", "2", "for", "``", "not", "confident", "''", ",", "a", "yellow", "bar", "of", "height", "2", "for", "``", "neutral", "''", ",", "and", "a", "bar", "of", "height", "0", "for", "``", "confident", "''", ".", "For", "concurrent", "think-aloud", "for", "the", "slides", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "an", "orange", "bar", "of", "height", "1", "for", "``", "not", "confident", "''", ",", "a", "bar", "of", "height", "0", "for", "``", "neutral", "''", ",", "and", "a", "green", "bar", "of", "height", "3", "for", "``", "confident", "''", ".", "For", "retrospective", "think-aloud", "for", "the", "coding", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "an", "orange", "bar", "of", "height", "2", "for", "``", "not", "confident", "''", ",", "a", "bar", "of", "height", "0", "for", "``", "neutral", "''", ",", "and", "a", "green", "bar", "of", "height", "2", "for", "``", "confident", "''", ".", "For", "retrospective", "think-aloud", "for", "the", "models", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "a", "bar", "of", "height", "0", "for", "``", "not", "confident", "''", ",", "a", "yellow", "bar", "of", "height", "1", "for", "``", "neutral", "''", ",", "and", "a", "green", "bar", "of", "height", "3", "for", "``", "confident", "''", ".", "For", "retrospective", "think-aloud", "for", "the", "slides", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "a", "bar", "of", "height", "0", "for", "``", "not", "confident", "''", ",", "a", "yellow", "bar", "of", "height", "1", "for", "``", "neutral", "''", ",", "and", "a", "green", "bar", "of", "height", "3", "for", "``", "confident", "''", ".", "Note", "that", "these", "categories", "are", "aggregations", "of", "the", "participant", "ratings", "from", "the", "7-point", "Likert", "scale", "results", "(", "between", "1-Very", "Not", "Confident", ",", "and", "7-Very", "Confident", ")", ".", "Ratings", "(", "1", ",", "2", ",", "3", ")", "have", "been", "aggregated", "into", "``", "Not", "Confident", "''", ",", "rating", "(", "4", ")", "is", "``", "Neutral", "''", ",", "and", "ratings", "(", "5", ",", "6", ",", "7", ")", "have", "been", "aggregated", "into", "``", "Confident", "''", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture ", "tokens": ["Think-Aloud", "Computing", ":", "Supporting", "Rich", "and", "Low-Effort", "Knowledge", "Capture"]}, "filename": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_002.jpg", "orig_filename": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "split": "train"}, {"article_id": "Communicating Uncertainty in Fertility Prognosis", "description": {"raw": "This figure shows three screenshots that display three different views of our prototype each representing the fertility prognosis of a specific timeframe in the past. The first presents one day, the second six days and the thrid a whole month in the past. The first shows two bars on top of each other: an orange bar indicates that the system predicted 30% conception probability for the given day and a smaller blue bar indicates that the system retrospectively calculated a conception probability of 13% for that day. In the same way the second screenshows shows orange and blue bars indicating predicted and detected fertile phases for six consecutive days and the third visualization shows orange and blue bars within the tiles of a calendar. A problem with this visualization that we explain in the limitation section is that the orange bars are occuded by the blue ones so that only their dotted outline is visible if the orange bar is smaller. We did not optimze our prototype for this case, because the initial plan for our study and the data we received for prototyping purposes focused only on data in the future. However, this visualization of past data was very effective in helping users understand uncertainties in the predictions.", "tokens": ["This", "figure", "shows", "three", "screenshots", "that", "display", "three", "different", "views", "of", "our", "prototype", "each", "representing", "the", "fertility", "prognosis", "of", "a", "specific", "timeframe", "in", "the", "past", ".", "The", "first", "presents", "one", "day", ",", "the", "second", "six", "days", "and", "the", "thrid", "a", "whole", "month", "in", "the", "past", ".", "The", "first", "shows", "two", "bars", "on", "top", "of", "each", "other", ":", "an", "orange", "bar", "indicates", "that", "the", "system", "predicted", "30", "%", "conception", "probability", "for", "the", "given", "day", "and", "a", "smaller", "blue", "bar", "indicates", "that", "the", "system", "retrospectively", "calculated", "a", "conception", "probability", "of", "13", "%", "for", "that", "day", ".", "In", "the", "same", "way", "the", "second", "screenshows", "shows", "orange", "and", "blue", "bars", "indicating", "predicted", "and", "detected", "fertile", "phases", "for", "six", "consecutive", "days", "and", "the", "third", "visualization", "shows", "orange", "and", "blue", "bars", "within", "the", "tiles", "of", "a", "calendar", ".", "A", "problem", "with", "this", "visualization", "that", "we", "explain", "in", "the", "limitation", "section", "is", "that", "the", "orange", "bars", "are", "occuded", "by", "the", "blue", "ones", "so", "that", "only", "their", "dotted", "outline", "is", "visible", "if", "the", "orange", "bar", "is", "smaller", ".", "We", "did", "not", "optimze", "our", "prototype", "for", "this", "case", ",", "because", "the", "initial", "plan", "for", "our", "study", "and", "the", "data", "we", "received", "for", "prototyping", "purposes", "focused", "only", "on", "data", "in", "the", "future", ".", "However", ",", "this", "visualization", "of", "past", "data", "was", "very", "effective", "in", "helping", "users", "understand", "uncertainties", "in", "the", "predictions", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Communicating Uncertainty in Fertility Prognosis ", "tokens": ["Communicating", "Uncertainty", "in", "Fertility", "Prognosis"]}, "filename": "d1177fed35f70e0442faed7f79433ce426de7a06_Image_006.jpg", "orig_filename": "d1177fed35f70e0442faed7f79433ce426de7a06", "split": "train"}, {"article_id": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "description": {"raw": "The chart shows that noise depends on force and the use of nozzles, not tubing size and tubing length.", "tokens": ["The", "chart", "shows", "that", "noise", "depends", "on", "force", "and", "the", "use", "of", "nozzles", ",", "not", "tubing", "size", "and", "tubing", "length", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets ", "tokens": ["JetController", ":", "High-speed", "Ungrounded", "3-DoF", "Force", "Feedback", "Controllers", "using", "Air", "Propulsion", "Jets"]}, "filename": "abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_009.jpg", "orig_filename": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "split": "train"}, {"article_id": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "description": {"raw": "There are 5 subfigures in total.       In subfigure a, there is a conical frustum simulating the virtual grid centered to the user's forearm.       Subfigure b shows the longitude dimension, where the longitude levels are surrounding the arm in cardinal and ordinal directions, i.e., N, NW, W, SW, S, SE, E, and NE for 0-7, respectively.      Subfigure c shows the latitude levels, wrist (0), forearm (1), and elbow (2), with 8cm gaps between levels.       Subfigure d shows the height levels, close (0), medium (1), and far (2), with 4cm, 12cm, and 20cm distance from the skin.      Subfigure e shows how to locate the 8 longitude levels given the tracking data of the 3D printed pads mounted to the hand. The two pads are mounted to the dorsal side and ventral side. If a vector is drawn perpendicular to these pads, then the longitude level 0 has an angle of 22.5 degrees with this aforementioned vector.", "tokens": ["There", "are", "5", "subfigures", "in", "total", ".", "In", "subfigure", "a", ",", "there", "is", "a", "conical", "frustum", "simulating", "the", "virtual", "grid", "centered", "to", "the", "user", "'s", "forearm", ".", "Subfigure", "b", "shows", "the", "longitude", "dimension", ",", "where", "the", "longitude", "levels", "are", "surrounding", "the", "arm", "in", "cardinal", "and", "ordinal", "directions", ",", "i.e.", ",", "N", ",", "NW", ",", "W", ",", "SW", ",", "S", ",", "SE", ",", "E", ",", "and", "NE", "for", "0-7", ",", "respectively", ".", "Subfigure", "c", "shows", "the", "latitude", "levels", ",", "wrist", "(", "0", ")", ",", "forearm", "(", "1", ")", ",", "and", "elbow", "(", "2", ")", ",", "with", "8cm", "gaps", "between", "levels", ".", "Subfigure", "d", "shows", "the", "height", "levels", ",", "close", "(", "0", ")", ",", "medium", "(", "1", ")", ",", "and", "far", "(", "2", ")", ",", "with", "4cm", ",", "12cm", ",", "and", "20cm", "distance", "from", "the", "skin", ".", "Subfigure", "e", "shows", "how", "to", "locate", "the", "8", "longitude", "levels", "given", "the", "tracking", "data", "of", "the", "3D", "printed", "pads", "mounted", "to", "the", "hand", ".", "The", "two", "pads", "are", "mounted", "to", "the", "dorsal", "side", "and", "ventral", "side", ".", "If", "a", "vector", "is", "drawn", "perpendicular", "to", "these", "pads", ",", "then", "the", "longitude", "level", "0", "has", "an", "angle", "of", "22.5", "degrees", "with", "this", "aforementioned", "vector", "."]}, "caption": {"raw": "Figure 3: (a) An illustration of the ‘conical frustum’ grids and depictions of how the (b) longitude, (c) latitude, and (d) height target locations were situated around and along the arm. (e) An illustration of the mapping of the eight longitude levels to the tracking results of the left arm, when the arm was held forward with the 3D printed pads on the (i) dorsal side and (ii) ventral side parallel to each other. The longitude level 0 (N) has an angle of 22.5 degrees perpendicular to the two pads.", "tokens": ["Figure", "3", ":", "(", "a", ")", "An", "illustration", "of", "the", "‘", "conical", "frustum", "’", "grids", "and", "depictions", "of", "how", "the", "(", "b", ")", "longitude", ",", "(", "c", ")", "latitude", ",", "and", "(", "d", ")", "height", "target", "locations", "were", "situated", "around", "and", "along", "the", "arm", ".", "(", "e", ")", "An", "illustration", "of", "the", "mapping", "of", "the", "eight", "longitude", "levels", "to", "the", "tracking", "results", "of", "the", "left", "arm", ",", "when", "the", "arm", "was", "held", "forward", "with", "the", "3D", "printed", "pads", "on", "the", "(", "i", ")", "dorsal", "side", "and", "(", "ii", ")", "ventral", "side", "parallel", "to", "each", "other", ".", "The", "longitude", "level", "0", "(", "N", ")", "has", "an", "angle", "of", "22.5", "degrees", "perpendicular", "to", "the", "two", "pads", "."]}, "context": {"raw": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality Figure 3: (a) An illustration of the ‘conical frustum’ grids and depictions of how the (b) longitude, (c) latitude, and (d) height target locations were situated around and along the arm. (e) An illustration of the mapping of the eight longitude levels to the tracking results of the left arm, when the arm was held forward with the 3D printed pads on the (i) dorsal side and (ii) ventral side parallel to each other. The longitude level 0 (N) has an angle of 22.5 degrees perpendicular to the two pads.", "tokens": ["Armstrong", ":", "An", "Empirical", "Examination", "of", "Pointing", "at", "Non-Dominant", "Arm-Anchored", "UIs", "in", "Virtual", "Reality", "Figure", "3", ":", "(", "a", ")", "An", "illustration", "of", "the", "‘", "conical", "frustum", "’", "grids", "and", "depictions", "of", "how", "the", "(", "b", ")", "longitude", ",", "(", "c", ")", "latitude", ",", "and", "(", "d", ")", "height", "target", "locations", "were", "situated", "around", "and", "along", "the", "arm", ".", "(", "e", ")", "An", "illustration", "of", "the", "mapping", "of", "the", "eight", "longitude", "levels", "to", "the", "tracking", "results", "of", "the", "left", "arm", ",", "when", "the", "arm", "was", "held", "forward", "with", "the", "3D", "printed", "pads", "on", "the", "(", "i", ")", "dorsal", "side", "and", "(", "ii", ")", "ventral", "side", "parallel", "to", "each", "other", ".", "The", "longitude", "level", "0", "(", "N", ")", "has", "an", "angle", "of", "22.5", "degrees", "perpendicular", "to", "the", "two", "pads", "."]}, "filename": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_004.png", "orig_filename": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "split": "train"}, {"article_id": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People", "description": {"raw": "The bar chart shows the average selection error (y-axis, in percent) for all feedback conditions, and contains 10 vertical bars, one for each condition. The x-axis groups bars by the feedback designs, from left: Geiger, Pitch, Constant and Control. There are three coloured bars in the Geiger, Pitch and Constant, one for each of the Wrist, Object and Both locations. The Control Design only has a single bar (for Object). The values, from left to right, are as follows: Geiger-Wrist = 17.6%, Geiger-Object = 12%, Geiger-Both = 6.5%; Pitch-Wrist = 13%, Pitch-Object = 9.2%, Pitch-Both = 15.7%; Constant-Wrist = 25%, Constant-Object = 25.9%, Constant-Both = 21.3%; Control-Object = 36.1%.", "tokens": ["The", "bar", "chart", "shows", "the", "average", "selection", "error", "(", "y-axis", ",", "in", "percent", ")", "for", "all", "feedback", "conditions", ",", "and", "contains", "10", "vertical", "bars", ",", "one", "for", "each", "condition", ".", "The", "x-axis", "groups", "bars", "by", "the", "feedback", "designs", ",", "from", "left", ":", "Geiger", ",", "Pitch", ",", "Constant", "and", "Control", ".", "There", "are", "three", "coloured", "bars", "in", "the", "Geiger", ",", "Pitch", "and", "Constant", ",", "one", "for", "each", "of", "the", "Wrist", ",", "Object", "and", "Both", "locations", ".", "The", "Control", "Design", "only", "has", "a", "single", "bar", "(", "for", "Object", ")", ".", "The", "values", ",", "from", "left", "to", "right", ",", "are", "as", "follows", ":", "Geiger-Wrist", "=", "17.6", "%", ",", "Geiger-Object", "=", "12", "%", ",", "Geiger-Both", "=", "6.5", "%", ";", "Pitch-Wrist", "=", "13", "%", ",", "Pitch-Object", "=", "9.2", "%", ",", "Pitch-Both", "=", "15.7", "%", ";", "Constant-Wrist", "=", "25", "%", ",", "Constant-Object", "=", "25.9", "%", ",", "Constant-Both", "=", "21.3", "%", ";", "Control-Object", "=", "36.1", "%", "."]}, "caption": {"raw": "Figure 5: Mean Study 1 target selection error for all Feedback Designs and Speaker Locations. Error bars = 95% CI.", "tokens": ["Figure", "5", ":", "Mean", "Study", "1", "target", "selection", "error", "for", "all", "Feedback", "Designs", "and", "Speaker", "Locations", ".", "Error", "bars", "=", "95", "%", "CI", "."]}, "context": {"raw": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People Figure 5: Mean Study 1 target selection error for all Feedback Designs and Speaker Locations. Error bars = 95% CI.", "tokens": ["Using", "Dynamic", "Audio", "Feedback", "to", "Support", "Peripersonal", "Reaching", "in", "Young", "Visually", "Impaired", "People", "Figure", "5", ":", "Mean", "Study", "1", "target", "selection", "error", "for", "all", "Feedback", "Designs", "and", "Speaker", "Locations", ".", "Error", "bars", "=", "95", "%", "CI", "."]}, "filename": "09f444597b50aebe550b5efd1368a55953d8ddd1_Image_005.jpg", "orig_filename": "09f444597b50aebe550b5efd1368a55953d8ddd1", "split": "train"}, {"article_id": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "description": {"raw": "A box plot displaying the F1 Mean across each condition, with units in Hertz. First results are presented for the 9 participants who spoke in all three conditions, with median value of 948.497 for Markup, 1152.505 for No ASR, and 905.744 for ASR.  There were significant differences between Markup and No ASR, as well as ASR and No ASR.  Next, results are shown for all 12 participants, with median value of 959.743 for Markup and 909.571 for ASR.  There was no significant difference between these two conditions.", "tokens": ["A", "box", "plot", "displaying", "the", "F1", "Mean", "across", "each", "condition", ",", "with", "units", "in", "Hertz", ".", "First", "results", "are", "presented", "for", "the", "9", "participants", "who", "spoke", "in", "all", "three", "conditions", ",", "with", "median", "value", "of", "948.497", "for", "Markup", ",", "1152.505", "for", "No", "ASR", ",", "and", "905.744", "for", "ASR", ".", "There", "were", "significant", "differences", "between", "Markup", "and", "No", "ASR", ",", "as", "well", "as", "ASR", "and", "No", "ASR", ".", "Next", ",", "results", "are", "shown", "for", "all", "12", "participants", ",", "with", "median", "value", "of", "959.743", "for", "Markup", "and", "909.571", "for", "ASR", ".", "There", "was", "no", "significant", "difference", "between", "these", "two", "conditions", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers ", "tokens": ["Behavioral", "Changes", "in", "Speakers", "who", "are", "Automatically", "Captioned", "in", "Meetings", "with", "Deaf", "or", "Hard-of-Hearing", "Peers"]}, "filename": "4cecd70a9e46a761774a54ed11d613b33721b95d_Image_009.gif", "orig_filename": "4cecd70a9e46a761774a54ed11d613b33721b95d", "split": "train"}, {"article_id": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "description": {"raw": "Examples of arranging 5 UI controls around the arm using the throughput heatmap data.       In subfigure a, based on the unknown dataset, the 5 UI controls are placed at: SE x wrist x close; S x wrist x close; SE x wrist x medium; SW x wrist x close; NE x wrist x close.       In subfigure b, based on the known dataset, the same 5 controls are placed at: N x forearm x medium; E x wrist x medium; NE x forearm x close; SW x wrist x close; NE x wrist x close.       In Subfigure c, based on the known dataset with preference of forearm, the 5 UI controls are placed at: NE x forearm x close; NE x forearm x medium; NE x forearm x far; N x forearm x close; N x forearm x medium.", "tokens": ["Examples", "of", "arranging", "5", "UI", "controls", "around", "the", "arm", "using", "the", "throughput", "heatmap", "data", ".", "In", "subfigure", "a", ",", "based", "on", "the", "unknown", "dataset", ",", "the", "5", "UI", "controls", "are", "placed", "at", ":", "SE", "x", "wrist", "x", "close", ";", "S", "x", "wrist", "x", "close", ";", "SE", "x", "wrist", "x", "medium", ";", "SW", "x", "wrist", "x", "close", ";", "NE", "x", "wrist", "x", "close", ".", "In", "subfigure", "b", ",", "based", "on", "the", "known", "dataset", ",", "the", "same", "5", "controls", "are", "placed", "at", ":", "N", "x", "forearm", "x", "medium", ";", "E", "x", "wrist", "x", "medium", ";", "NE", "x", "forearm", "x", "close", ";", "SW", "x", "wrist", "x", "close", ";", "NE", "x", "wrist", "x", "close", ".", "In", "Subfigure", "c", ",", "based", "on", "the", "known", "dataset", "with", "preference", "of", "forearm", ",", "the", "5", "UI", "controls", "are", "placed", "at", ":", "NE", "x", "forearm", "x", "close", ";", "NE", "x", "forearm", "x", "medium", ";", "NE", "x", "forearm", "x", "far", ";", "N", "x", "forearm", "x", "close", ";", "N", "x", "forearm", "x", "medium", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality ", "tokens": ["Armstrong", ":", "An", "Empirical", "Examination", "of", "Pointing", "at", "Non-Dominant", "Arm-Anchored", "UIs", "in", "Virtual", "Reality"]}, "filename": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_092.png", "orig_filename": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "split": "train"}, {"article_id": "Diagramming Working Field Theories for Design in the HCI Classroom", "description": {"raw": "Figure 3: A diagram of a Field Theory of Managing Diabetes. It is represented as a two-dimension graph that shows glucose levels in the y-axis, and temporal milestones (hospital, diagnosis, treatment, tools, education, practice) in the x-axis. The glucose curve ascends and descends regularly, also representing the ups and downs of the journey of self-managing diabetes, leading to a more stable curve after the use of tools, education and practice. The curve leads to identifying opportunities for design using Internet of Things.", "tokens": ["Figure", "3", ":", "A", "diagram", "of", "a", "Field", "Theory", "of", "Managing", "Diabetes", ".", "It", "is", "represented", "as", "a", "two-dimension", "graph", "that", "shows", "glucose", "levels", "in", "the", "y-axis", ",", "and", "temporal", "milestones", "(", "hospital", ",", "diagnosis", ",", "treatment", ",", "tools", ",", "education", ",", "practice", ")", "in", "the", "x-axis", ".", "The", "glucose", "curve", "ascends", "and", "descends", "regularly", ",", "also", "representing", "the", "ups", "and", "downs", "of", "the", "journey", "of", "self-managing", "diabetes", ",", "leading", "to", "a", "more", "stable", "curve", "after", "the", "use", "of", "tools", ",", "education", "and", "practice", ".", "The", "curve", "leads", "to", "identifying", "opportunities", "for", "design", "using", "Internet", "of", "Things", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Diagramming Working Field Theories for Design in the HCI Classroom ", "tokens": ["Diagramming", "Working", "Field", "Theories", "for", "Design", "in", "the", "HCI", "Classroom"]}, "filename": "680984459a2ab9dcaed587a6e5775a25ce140804_Image_004.jpg", "orig_filename": "680984459a2ab9dcaed587a6e5775a25ce140804", "split": "train"}, {"article_id": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces", "description": {"raw": "Figure 3. Each panel shows the random walk through the interface space via the Metropolis algorithm with a different value for scalar k with T (arbitrarily) at 10. Each data point represents the WPM for an interface arrangement with a higher efficiency than the “current” arrangement.", "tokens": ["Figure", "3", ".", "Each", "panel", "shows", "the", "random", "walk", "through", "the", "interface", "space", "via", "the", "Metropolis", "algorithm", "with", "a", "different", "value", "for", "scalar", "k", "with", "T", "(", "arbitrarily", ")", "at", "10", ".", "Each", "data", "point", "represents", "the", "WPM", "for", "an", "interface", "arrangement", "with", "a", "higher", "efficiency", "than", "the", "“", "current", "”", "arrangement", "."]}, "caption": {"raw": "Figure 3. Each panel shows the random walk through the interface space via the Metropolis algorithm with a different value for scalar k with T (arbitrarily) at 10. Each data point represents the WPM for an interface arrangement with a", "tokens": ["Figure", "3", ".", "Each", "panel", "shows", "the", "random", "walk", "through", "the", "interface", "space", "via", "the", "Metropolis", "algorithm", "with", "a", "different", "value", "for", "scalar", "k", "with", "T", "(", "arbitrarily", ")", "at", "10", ".", "Each", "data", "point", "represents", "the", "WPM", "for", "an", "interface", "arrangement", "with", "a"]}, "context": {"raw": "Development and Theoretical Evaluation of Optimized Phonemic Interfaces Figure 3. Each panel shows the random walk through the interface space via the Metropolis algorithm with a different value for scalar k with T (arbitrarily) at 10. Each data point represents the WPM for an interface arrangement with a", "tokens": ["Development", "and", "Theoretical", "Evaluation", "of", "Optimized", "Phonemic", "Interfaces", "Figure", "3", ".", "Each", "panel", "shows", "the", "random", "walk", "through", "the", "interface", "space", "via", "the", "Metropolis", "algorithm", "with", "a", "different", "value", "for", "scalar", "k", "with", "T", "(", "arbitrarily", ")", "at", "10", ".", "Each", "data", "point", "represents", "the", "WPM", "for", "an", "interface", "arrangement", "with", "a"]}, "filename": "a92be5bddb46da574738c6452582c9a5939958c3_Image_003.jpg", "orig_filename": "a92be5bddb46da574738c6452582c9a5939958c3", "split": "train"}, {"article_id": "Head Mounted Projection Display & Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing", "description": {"raw": "On the top-left, there is the mean response times chart for MOT task. Mean response time for control and standing conditinions are  5.59 s, and 5.41 s, respectively. On the  top-right, there is the mean response times chart for VS task. Meadn response time for control, standing, and walking conditions are 1.52 s, 1.50 s, 1.54 s, respectively.  On the bottom-left, there is the accuracy chart for MOT task. Mean accuracies for control, standing and walking conditions are %89.4, %87.9, and %78.3, respectively. The relationship between control vs. walking and standing vs. walking are marked with *.   On the bottom-right there is the mean chart of accuracy for VS task. Mean accuracies for control, standing, and walking conditions are %86.4, %87.6, and %86.7, respectively", "tokens": ["On", "the", "top-left", ",", "there", "is", "the", "mean", "response", "times", "chart", "for", "MOT", "task", ".", "Mean", "response", "time", "for", "control", "and", "standing", "conditinions", "are", "5.59", "s", ",", "and", "5.41", "s", ",", "respectively", ".", "On", "the", "top-right", ",", "there", "is", "the", "mean", "response", "times", "chart", "for", "VS", "task", ".", "Meadn", "response", "time", "for", "control", ",", "standing", ",", "and", "walking", "conditions", "are", "1.52", "s", ",", "1.50", "s", ",", "1.54", "s", ",", "respectively", ".", "On", "the", "bottom-left", ",", "there", "is", "the", "accuracy", "chart", "for", "MOT", "task", ".", "Mean", "accuracies", "for", "control", ",", "standing", "and", "walking", "conditions", "are", "%", "89.4", ",", "%", "87.9", ",", "and", "%", "78.3", ",", "respectively", ".", "The", "relationship", "between", "control", "vs.", "walking", "and", "standing", "vs.", "walking", "are", "marked", "with", "*", ".", "On", "the", "bottom-right", "there", "is", "the", "mean", "chart", "of", "accuracy", "for", "VS", "task", ".", "Mean", "accuracies", "for", "control", ",", "standing", ",", "and", "walking", "conditions", "are", "%", "86.4", ",", "%", "87.6", ",", "and", "%", "86.7", ",", "respectively"]}, "caption": {"raw": "Figure 6. Mean (±SD) response times (top two panels) and accuracies (bottom two panels) for all conditions of the VS and MOT tasks. * marks significant differences between the corresponding groups.", "tokens": ["Figure", "6", ".", "Mean", "(", "±SD", ")", "response", "times", "(", "top", "two", "panels", ")", "and", "accuracies", "(", "bottom", "two", "panels", ")", "for", "all", "conditions", "of", "the", "VS", "and", "MOT", "tasks", ".", "*", "marks", "significant", "differences", "between", "the", "corresponding", "groups", "."]}, "context": {"raw": "Head Mounted Projection Display & Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing Figure 6. Mean (±SD) response times (top two panels) and accuracies (bottom two panels) for all conditions of the VS and MOT tasks. * marks significant differences between the corresponding groups.", "tokens": ["Head", "Mounted", "Projection", "Display", "&", "Visual", "Attention", ":", "Visual", "Attentional", "Processing", "of", "Head", "Referenced", "Static", "and", "Dynamic", "Displays", "while", "in", "Motion", "and", "Standing", "Figure", "6", ".", "Mean", "(", "±SD", ")", "response", "times", "(", "top", "two", "panels", ")", "and", "accuracies", "(", "bottom", "two", "panels", ")", "for", "all", "conditions", "of", "the", "VS", "and", "MOT", "tasks", ".", "*", "marks", "significant", "differences", "between", "the", "corresponding", "groups", "."]}, "filename": "94586032948e2c2ba39f17b94679efb4435237c9_Image_009.jpg", "orig_filename": "94586032948e2c2ba39f17b94679efb4435237c9", "split": "train"}, {"article_id": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing", "description": {"raw": "Bar plot of Accuracy and F1 for detecting post-semester depression. Baseline accuracy was 59.4%. Bluetooth accuracy was 69.3% and F1 was 0.64. Calls accuracy was 68.5% and F1 was 0.59. Campus Map accuracy was 68.2% and F1 was 0.66. Location accuracy was 69.5% and F1 was 0.62. Phone Usage accuracy was 70.3% and F1 was 0.75. Sleep accuracy was 69.2% and F1 was 0.66. Steps accuracy was 63.6% and F1 was 0.53. All-7 accuracy was 82.3% and F1 was 0.78. Best set accuracy was 85.7% and F1 was 0.82.", "tokens": ["Bar", "plot", "of", "Accuracy", "and", "F1", "for", "detecting", "post-semester", "depression", ".", "Baseline", "accuracy", "was", "59.4", "%", ".", "Bluetooth", "accuracy", "was", "69.3", "%", "and", "F1", "was", "0.64", ".", "Calls", "accuracy", "was", "68.5", "%", "and", "F1", "was", "0.59", ".", "Campus", "Map", "accuracy", "was", "68.2", "%", "and", "F1", "was", "0.66", ".", "Location", "accuracy", "was", "69.5", "%", "and", "F1", "was", "0.62", ".", "Phone", "Usage", "accuracy", "was", "70.3", "%", "and", "F1", "was", "0.75", ".", "Sleep", "accuracy", "was", "69.2", "%", "and", "F1", "was", "0.66", ".", "Steps", "accuracy", "was", "63.6", "%", "and", "F1", "was", "0.53", ".", "All-7", "accuracy", "was", "82.3", "%", "and", "F1", "was", "0.78", ".", "Best", "set", "accuracy", "was", "85.7", "%", "and", "F1", "was", "0.82", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing ", "tokens": ["Detecting", "Depression", "and", "Predicting", "its", "Onset", "Using", "Longitudinal", "Symptoms", "Captured", "by", "Passive", "Sensing"]}, "filename": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2_Image_012.jpg", "orig_filename": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2", "split": "train"}, {"article_id": "UbiBraille: designing and evaluating a vibrotactile Braille-reading device", "description": {"raw": "Bar chart illustrating character recognition accuracy per participant: P1 - 94%, P2 - 65%, P3 - 92%, P4 - 100%, P5 - 100%, P6 - 69%, P7 - 94%, P8 - 96%, P9 - 65%, P10 - 67%, P11 -  54%", "tokens": ["Bar", "chart", "illustrating", "character", "recognition", "accuracy", "per", "participant", ":", "P1", "-", "94", "%", ",", "P2", "-", "65", "%", ",", "P3", "-", "92", "%", ",", "P4", "-", "100", "%", ",", "P5", "-", "100", "%", ",", "P6", "-", "69", "%", ",", "P7", "-", "94", "%", ",", "P8", "-", "96", "%", ",", "P9", "-", "65", "%", ",", "P10", "-", "67", "%", ",", "P11", "-", "54", "%"]}, "caption": {"raw": "Figure 4. Character recognition accuracy per participant.", "tokens": ["Figure", "4", ".", "Character", "recognition", "accuracy", "per", "participant", "."]}, "context": {"raw": "UbiBraille: designing and evaluating a vibrotactile Braille-reading device Figure 4. Character recognition accuracy per participant.", "tokens": ["UbiBraille", ":", "designing", "and", "evaluating", "a", "vibrotactile", "Braille-reading", "device", "Figure", "4", ".", "Character", "recognition", "accuracy", "per", "participant", "."]}, "filename": "39dcd4c585a6916a434dffd5cebfd70e97581dea_Image_013.gif", "orig_filename": "39dcd4c585a6916a434dffd5cebfd70e97581dea", "split": "train"}, {"article_id": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "description": {"raw": "Plot of Effect of Surfaced Hits on ML-1M. Shows boycotts and data strikes. As size of boycotts/data strikes increases, Surfaced Hits are reduced. Zoomed in on effects between 0.71 and 0.78.", "tokens": ["Plot", "of", "Effect", "of", "Surfaced", "Hits", "on", "ML-1M", ".", "Shows", "boycotts", "and", "data", "strikes", ".", "As", "size", "of", "boycotts/data", "strikes", "increases", ",", "Surfaced", "Hits", "are", "reduced", ".", "Zoomed", "in", "on", "effects", "between", "0.71", "and", "0.78", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies ", "tokens": ["“", "Data", "Strikes", "”", ":", "Evaluating", "the", "Effectiveness", "of", "a", "New", "Form", "of", "Collective", "Action", "Against", "Technology", "Companies"]}, "filename": "41cbffad975874060d643c36c8bdb5c72637564e_Image_006.jpg", "orig_filename": "41cbffad975874060d643c36c8bdb5c72637564e", "split": "train"}, {"article_id": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals", "description": {"raw": "Figure 7 shows a stacked bar plot of percentages of 5-point frequency scale responses. The y-axis shows the percentage of participants. The x-axis, labeled as \"Purpose,\" has 9 categories: \"Academic\", \"Medical\", \"Legal\",  \"Work\", \"Personal Reading\", \"News\", \"Visual media\", \"Personal com.\",  \"Recreation\". These scale responses are shown from bottom to top: \"Daily\", \"Often\", \"Weekly\", \"Monthly\",  and \"Rarely\". These scalre responses are shown from bottom to top:  \"Extremely Interested\", \"Very Interested\", \"Somewhat Interested\", \"Slightly Interested\", \"Not Interested\". For \"Academic\", 37.5% selected \"Extremely Interested\", 22% \"Very Interested\", 21%  \"Somewhat Interested\", 12.5%  \"Slightly Interested\", 6%  \"Not Interested\". For \"Medical\", 37.5% selected \"Extremely Interested\", 19%  \"Very Interested\", 25%  \"Somewhat Interested\", 6%  \"Slightly Interested\", 12.5%  \"Not Interested\". For \"Legal\", 47% selected \"Extremely Interested\", 6%  \"Very Interested\", 28%  \"Somewhat Interested\", 9%  \"Slightly Interested\", 9%  \"Not Interested\". For \"Work\", 31% selected \"Extremely Interested\", 19%  \"Very Interested\", 25%  \"Somewhat Interested\", 12.5%  \"Slightly Interested\", 12.5%  \"Not Interested\". For \"Personal reading\", 22% selected \"Extremely Interested\", 25%  \"Very Interested\", 6%  \"Somewhat Interested\", 25%  \"Slightly Interested\", 22%  \"Not Interested\". For \"News\", 9% selected \"Extremely Interested\", 3%  \"Very Interested\", 16%  \"Somewhat Interested\", 22%  \"Slightly Interondedested\", 22%  \"Not Interested\". For \"Visual media\", 9% selected \"Extremely Interested\", 25%  \"Very Interested\", 19%  \"Somewhat Interested\", 12.5%  \"Slightly Interested\", 34.5%  \"Not Interested\". For \"Personal comm.\", 19% selected \"Extremely Interested\", 12.5%  \"Very Interested\", 15.5%  \"Somewhat Interested\", 25%  \"Slightly Interested\", 28%  \"Not Interested\". For \"Recreation\", 3% selected \"Extremely Interested\", 12.5%  \"Very Interested\", 25%  \"Somewhat Interested\", 22%  \"Slightly Interested\", 37.5% \"Not interested\"", "tokens": ["Figure", "7", "shows", "a", "stacked", "bar", "plot", "of", "percentages", "of", "5-point", "frequency", "scale", "responses", ".", "The", "y-axis", "shows", "the", "percentage", "of", "participants", ".", "The", "x-axis", ",", "labeled", "as", "``", "Purpose", ",", "''", "has", "9", "categories", ":", "``", "Academic", "''", ",", "``", "Medical", "''", ",", "``", "Legal", "''", ",", "``", "Work", "''", ",", "``", "Personal", "Reading", "''", ",", "``", "News", "''", ",", "``", "Visual", "media", "''", ",", "``", "Personal", "com", ".", "``", ",", "``", "Recreation", "''", ".", "These", "scale", "responses", "are", "shown", "from", "bottom", "to", "top", ":", "``", "Daily", "''", ",", "``", "Often", "''", ",", "``", "Weekly", "''", ",", "``", "Monthly", "''", ",", "and", "``", "Rarely", "''", ".", "These", "scalre", "responses", "are", "shown", "from", "bottom", "to", "top", ":", "``", "Extremely", "Interested", "''", ",", "``", "Very", "Interested", "''", ",", "``", "Somewhat", "Interested", "''", ",", "``", "Slightly", "Interested", "''", ",", "``", "Not", "Interested", "''", ".", "For", "``", "Academic", "''", ",", "37.5", "%", "selected", "``", "Extremely", "Interested", "''", ",", "22", "%", "``", "Very", "Interested", "''", ",", "21", "%", "``", "Somewhat", "Interested", "''", ",", "12.5", "%", "``", "Slightly", "Interested", "''", ",", "6", "%", "``", "Not", "Interested", "''", ".", "For", "``", "Medical", "''", ",", "37.5", "%", "selected", "``", "Extremely", "Interested", "''", ",", "19", "%", "``", "Very", "Interested", "''", ",", "25", "%", "``", "Somewhat", "Interested", "''", ",", "6", "%", "``", "Slightly", "Interested", "''", ",", "12.5", "%", "``", "Not", "Interested", "''", ".", "For", "``", "Legal", "''", ",", "47", "%", "selected", "``", "Extremely", "Interested", "''", ",", "6", "%", "``", "Very", "Interested", "''", ",", "28", "%", "``", "Somewhat", "Interested", "''", ",", "9", "%", "``", "Slightly", "Interested", "''", ",", "9", "%", "``", "Not", "Interested", "''", ".", "For", "``", "Work", "''", ",", "31", "%", "selected", "``", "Extremely", "Interested", "''", ",", "19", "%", "``", "Very", "Interested", "''", ",", "25", "%", "``", "Somewhat", "Interested", "''", ",", "12.5", "%", "``", "Slightly", "Interested", "''", ",", "12.5", "%", "``", "Not", "Interested", "''", ".", "For", "``", "Personal", "reading", "''", ",", "22", "%", "selected", "``", "Extremely", "Interested", "''", ",", "25", "%", "``", "Very", "Interested", "''", ",", "6", "%", "``", "Somewhat", "Interested", "''", ",", "25", "%", "``", "Slightly", "Interested", "''", ",", "22", "%", "``", "Not", "Interested", "''", ".", "For", "``", "News", "''", ",", "9", "%", "selected", "``", "Extremely", "Interested", "''", ",", "3", "%", "``", "Very", "Interested", "''", ",", "16", "%", "``", "Somewhat", "Interested", "''", ",", "22", "%", "``", "Slightly", "Interondedested", "''", ",", "22", "%", "``", "Not", "Interested", "''", ".", "For", "``", "Visual", "media", "''", ",", "9", "%", "selected", "``", "Extremely", "Interested", "''", ",", "25", "%", "``", "Very", "Interested", "''", ",", "19", "%", "``", "Somewhat", "Interested", "''", ",", "12.5", "%", "``", "Slightly", "Interested", "''", ",", "34.5", "%", "``", "Not", "Interested", "''", ".", "For", "``", "Personal", "comm", ".", "``", ",", "19", "%", "selected", "``", "Extremely", "Interested", "''", ",", "12.5", "%", "``", "Very", "Interested", "''", ",", "15.5", "%", "``", "Somewhat", "Interested", "''", ",", "25", "%", "``", "Slightly", "Interested", "''", ",", "28", "%", "``", "Not", "Interested", "''", ".", "For", "``", "Recreation", "''", ",", "3", "%", "selected", "``", "Extremely", "Interested", "''", ",", "12.5", "%", "``", "Very", "Interested", "''", ",", "25", "%", "``", "Somewhat", "Interested", "''", ",", "22", "%", "``", "Slightly", "Interested", "''", ",", "37.5", "%", "``", "Not", "interested", "''"]}, "caption": {"raw": "Professionals                                        ASSETS ’20, October 26–28, 2020, Virtual Event, Greece", "tokens": ["Professionals", "ASSETS", "’", "20", ",", "October", "26–28", ",", "2020", ",", "Virtual", "Event", ",", "Greece"]}, "context": {"raw": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals Professionals                                        ASSETS ’20, October 26–28, 2020, Virtual Event, Greece", "tokens": ["Reading", "Experiences", "and", "Interest", "in", "Reading-Assistance", "Tools", "Among", "Deaf", "and", "Hard-of-Hearing", "Computing", "Professionals", "Professionals", "ASSETS", "’", "20", ",", "October", "26–28", ",", "2020", ",", "Virtual", "Event", ",", "Greece"]}, "filename": "b61e1747b1fef3eedde7aa87f26338f61d52ce93_Image_009.jpg", "orig_filename": "b61e1747b1fef3eedde7aa87f26338f61d52ce93", "split": "train"}, {"article_id": "Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations", "description": {"raw": "A grouped column graph showing results for means of satisfaction scores in the remote study for each behavior. Each behavior is listed on the x-axis, with satisfaction scores on the y-axis. The following lists each behavior groups with three means, one for high, medium, and low levels of that behavior. Speech rate had means of 3.61, 5.74, and 3.75 for high, medium, and low, respectively. Voice intensity had means of 6.91, 6.57, and 4.61. Enunciation had means of 5.96, 6.48, and 4.04. Intonation had means of 7, 6.48, and 5.74. Eye contact had means of 6.43, 5.83, and 3.65. Gesturing had means of 7, 6.74, and 5.65. Intermittent pausing had means of 5.52, 6.48, 5.39. A double asterisk is shown next to the behaviors Speech Rate, Voice Intensity, Enunciation, Intonation, and Eye Contact as they had significant omnibus Friedman test results. A double asterisk, denoting pairwise significant differences are shown between the pairs High and Medium speech rate, medium and low speech rate, high and low intensity, medium and low intensity, medium and low enunciation, high and low eye contact, and medium and low eye contact.", "tokens": ["A", "grouped", "column", "graph", "showing", "results", "for", "means", "of", "satisfaction", "scores", "in", "the", "remote", "study", "for", "each", "behavior", ".", "Each", "behavior", "is", "listed", "on", "the", "x-axis", ",", "with", "satisfaction", "scores", "on", "the", "y-axis", ".", "The", "following", "lists", "each", "behavior", "groups", "with", "three", "means", ",", "one", "for", "high", ",", "medium", ",", "and", "low", "levels", "of", "that", "behavior", ".", "Speech", "rate", "had", "means", "of", "3.61", ",", "5.74", ",", "and", "3.75", "for", "high", ",", "medium", ",", "and", "low", ",", "respectively", ".", "Voice", "intensity", "had", "means", "of", "6.91", ",", "6.57", ",", "and", "4.61", ".", "Enunciation", "had", "means", "of", "5.96", ",", "6.48", ",", "and", "4.04", ".", "Intonation", "had", "means", "of", "7", ",", "6.48", ",", "and", "5.74", ".", "Eye", "contact", "had", "means", "of", "6.43", ",", "5.83", ",", "and", "3.65", ".", "Gesturing", "had", "means", "of", "7", ",", "6.74", ",", "and", "5.65", ".", "Intermittent", "pausing", "had", "means", "of", "5.52", ",", "6.48", ",", "5.39", ".", "A", "double", "asterisk", "is", "shown", "next", "to", "the", "behaviors", "Speech", "Rate", ",", "Voice", "Intensity", ",", "Enunciation", ",", "Intonation", ",", "and", "Eye", "Contact", "as", "they", "had", "significant", "omnibus", "Friedman", "test", "results", ".", "A", "double", "asterisk", ",", "denoting", "pairwise", "significant", "differences", "are", "shown", "between", "the", "pairs", "High", "and", "Medium", "speech", "rate", ",", "medium", "and", "low", "speech", "rate", ",", "high", "and", "low", "intensity", ",", "medium", "and", "low", "intensity", ",", "medium", "and", "low", "enunciation", ",", "high", "and", "low", "eye", "contact", ",", "and", "medium", "and", "low", "eye", "contact", "."]}, "caption": {"raw": "(a)", "tokens": ["(", "a", ")"]}, "context": {"raw": "Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations (a)", "tokens": ["Deaf", "and", "hard-of-hearing", "users", "'", "preferences", "for", "hearing", "speakers", "'", "behavior", "during", "technology-mediated", "in-person", "and", "remote", "conversations", "(", "a", ")"]}, "filename": "377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0_Image_005.jpg", "orig_filename": "377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0", "split": "train"}, {"article_id": "Off-Limits: Interacting Beyond the Boundaries of Large Displays", "description": {"raw": "Figure 6. Scatter plot and regressions of our model. Grey indi- cates the display; green areas show 95% prediction intervals.", "tokens": ["Figure", "6", ".", "Scatter", "plot", "and", "regressions", "of", "our", "model", ".", "Grey", "indi-", "cates", "the", "display", ";", "green", "areas", "show", "95", "%", "prediction", "intervals", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Off-Limits: Interacting Beyond the Boundaries of Large Displays ", "tokens": ["Off-Limits", ":", "Interacting", "Beyond", "the", "Boundaries", "of", "Large", "Displays"]}, "filename": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f_Image_005.jpg", "orig_filename": "7bb3ebe039b18f33e7d0df5549c84ebe1e8f093f", "split": "train"}, {"article_id": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions", "description": {"raw": "Prior and posterior distributions for the likelihood ratio of H0 and H+, and corresponding Bayes Factors, based on performance improvement on the transfer test and learnt vocabulary after one week compared to performance before the study. Relative likelihoods are illustrated by the pie-chart. Created with JASP", "tokens": ["Prior", "and", "posterior", "distributions", "for", "the", "likelihood", "ratio", "of", "H0", "and", "H+", ",", "and", "corresponding", "Bayes", "Factors", ",", "based", "on", "performance", "improvement", "on", "the", "transfer", "test", "and", "learnt", "vocabulary", "after", "one", "week", "compared", "to", "performance", "before", "the", "study", ".", "Relative", "likelihoods", "are", "illustrated", "by", "the", "pie-chart", ".", "Created", "with", "JASP"]}, "caption": {"raw": "(a) Transfer Improvement (b) Additional Words Recalled", "tokens": ["(", "a", ")", "Transfer", "Improvement", "(", "b", ")", "Additional", "Words", "Recalled"]}, "context": {"raw": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions (a) Transfer Improvement (b) Additional Words Recalled", "tokens": ["Augmented", "Reality", "to", "Enable", "Users", "in", "Learning", "Case", "Grammar", "from", "Their", "Real-World", "Interactions", "(", "a", ")", "Transfer", "Improvement", "(", "b", ")", "Additional", "Words", "Recalled"]}, "filename": "80183519a1e9c67b6996bea274cd5e6c251e6683_Image_010.jpg", "orig_filename": "80183519a1e9c67b6996bea274cd5e6c251e6683", "split": "train"}, {"article_id": "PantoGuide: A Haptic and Audio Guidance System To Support Tactile Graphics Exploration", "description": {"raw": "Banner Image with three images labelled A, B, and C. a) A user wearing the haptic guidance device and touching a tactile graphic on top of a touchscreen. The device has labels for the tactor, pantograph mechanism, DC motor, magnetic encoder, wrist sleeve. The tactile graphic and touchscreen are also labelled.  b) A tactile graphic of a bar chart. c) A tactile graphic of a small marble or radius r sitting at rest at the top of an inclined plane with a loop de loop at the end of the incline plane.", "tokens": ["Banner", "Image", "with", "three", "images", "labelled", "A", ",", "B", ",", "and", "C.", "a", ")", "A", "user", "wearing", "the", "haptic", "guidance", "device", "and", "touching", "a", "tactile", "graphic", "on", "top", "of", "a", "touchscreen", ".", "The", "device", "has", "labels", "for", "the", "tactor", ",", "pantograph", "mechanism", ",", "DC", "motor", ",", "magnetic", "encoder", ",", "wrist", "sleeve", ".", "The", "tactile", "graphic", "and", "touchscreen", "are", "also", "labelled", ".", "b", ")", "A", "tactile", "graphic", "of", "a", "bar", "chart", ".", "c", ")", "A", "tactile", "graphic", "of", "a", "small", "marble", "or", "radius", "r", "sitting", "at", "rest", "at", "the", "top", "of", "an", "inclined", "plane", "with", "a", "loop", "de", "loop", "at", "the", "end", "of", "the", "incline", "plane", "."]}, "caption": {"raw": "Figure 1: a) PantoGuide is a system that provides haptic and audio guidance cues to a user while exploring a tactile graphic. The device has a tactor in contact with the user that moves in a 2D plane (red arrows) to provide skin-stretch feedback. We demonstrate its use in two applications: b) providing point-to-point directional guidance cues when a user explores diferent elements of a tactile bar chart, and c) providing continuous guidance on the trajectory of a moving marble", "tokens": ["Figure", "1", ":", "a", ")", "PantoGuide", "is", "a", "system", "that", "provides", "haptic", "and", "audio", "guidance", "cues", "to", "a", "user", "while", "exploring", "a", "tactile", "graphic", ".", "The", "device", "has", "a", "tactor", "in", "contact", "with", "the", "user", "that", "moves", "in", "a", "2D", "plane", "(", "red", "arrows", ")", "to", "provide", "skin-stretch", "feedback", ".", "We", "demonstrate", "its", "use", "in", "two", "applications", ":", "b", ")", "providing", "point-to-point", "directional", "guidance", "cues", "when", "a", "user", "explores", "diferent", "elements", "of", "a", "tactile", "bar", "chart", ",", "and", "c", ")", "providing", "continuous", "guidance", "on", "the", "trajectory", "of", "a", "moving", "marble"]}, "context": {"raw": "PantoGuide: A Haptic and Audio Guidance System To Support Tactile Graphics Exploration Figure 1: a) PantoGuide is a system that provides haptic and audio guidance cues to a user while exploring a tactile graphic. The device has a tactor in contact with the user that moves in a 2D plane (red arrows) to provide skin-stretch feedback. We demonstrate its use in two applications: b) providing point-to-point directional guidance cues when a user explores diferent elements of a tactile bar chart, and c) providing continuous guidance on the trajectory of a moving marble", "tokens": ["PantoGuide", ":", "A", "Haptic", "and", "Audio", "Guidance", "System", "To", "Support", "Tactile", "Graphics", "Exploration", "Figure", "1", ":", "a", ")", "PantoGuide", "is", "a", "system", "that", "provides", "haptic", "and", "audio", "guidance", "cues", "to", "a", "user", "while", "exploring", "a", "tactile", "graphic", ".", "The", "device", "has", "a", "tactor", "in", "contact", "with", "the", "user", "that", "moves", "in", "a", "2D", "plane", "(", "red", "arrows", ")", "to", "provide", "skin-stretch", "feedback", ".", "We", "demonstrate", "its", "use", "in", "two", "applications", ":", "b", ")", "providing", "point-to-point", "directional", "guidance", "cues", "when", "a", "user", "explores", "diferent", "elements", "of", "a", "tactile", "bar", "chart", ",", "and", "c", ")", "providing", "continuous", "guidance", "on", "the", "trajectory", "of", "a", "moving", "marble"]}, "filename": "ae5bcaa963cb0b92a4e99ab7b0f0674c32d56e68_Image_001.jpg", "orig_filename": "ae5bcaa963cb0b92a4e99ab7b0f0674c32d56e68", "split": "train"}, {"article_id": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "description": {"raw": "This chart shows the average completion time (ms) across the three tasks, for each of the four conditions.", "tokens": ["This", "chart", "shows", "the", "average", "completion", "time", "(", "ms", ")", "across", "the", "three", "tasks", ",", "for", "each", "of", "the", "four", "conditions", "."]}, "caption": {"raw": "Figure 3: Task completion times. Bars indicate a 95% CI.", "tokens": ["Figure", "3", ":", "Task", "completion", "times", ".", "Bars", "indicate", "a", "95", "%", "CI", "."]}, "context": {"raw": "Dive in!: enabling progressive loading for real-time navigation of data visualizations Figure 3: Task completion times. Bars indicate a 95% CI.", "tokens": ["Dive", "in", "!", ":", "enabling", "progressive", "loading", "for", "real-time", "navigation", "of", "data", "visualizations", "Figure", "3", ":", "Task", "completion", "times", ".", "Bars", "indicate", "a", "95", "%", "CI", "."]}, "filename": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_003.jpg", "orig_filename": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "split": "train"}, {"article_id": "Exergames for Physiotherapy and Rehabilitation: A Medium-term Situated Study of Motivational Aspects and Impact on Functional Reach", "description": {"raw": "The chart shows the pre and post means of the three conditions for the autonomy, presence, tension and importance dimensions of the motivational measures with error bars indicating standard error.", "tokens": ["The", "chart", "shows", "the", "pre", "and", "post", "means", "of", "the", "three", "conditions", "for", "the", "autonomy", ",", "presence", ",", "tension", "and", "importance", "dimensions", "of", "the", "motivational", "measures", "with", "error", "bars", "indicating", "standard", "error", "."]}, "caption": {"raw": "Figure 2: Means and standard errors for autonomy, presence, tension-pressure, and effort-importance collected in the first session (pre) and in the last session after five weeks (post).", "tokens": ["Figure", "2", ":", "Means", "and", "standard", "errors", "for", "autonomy", ",", "presence", ",", "tension-pressure", ",", "and", "effort-importance", "collected", "in", "the", "first", "session", "(", "pre", ")", "and", "in", "the", "last", "session", "after", "five", "weeks", "(", "post", ")", "."]}, "context": {"raw": "Exergames for Physiotherapy and Rehabilitation: A Medium-term Situated Study of Motivational Aspects and Impact on Functional Reach Figure 2: Means and standard errors for autonomy, presence, tension-pressure, and effort-importance collected in the first session (pre) and in the last session after five weeks (post).", "tokens": ["Exergames", "for", "Physiotherapy", "and", "Rehabilitation", ":", "A", "Medium-term", "Situated", "Study", "of", "Motivational", "Aspects", "and", "Impact", "on", "Functional", "Reach", "Figure", "2", ":", "Means", "and", "standard", "errors", "for", "autonomy", ",", "presence", ",", "tension-pressure", ",", "and", "effort-importance", "collected", "in", "the", "first", "session", "(", "pre", ")", "and", "in", "the", "last", "session", "after", "five", "weeks", "(", "post", ")", "."]}, "filename": "00de251fbb4aaeb4cae73a26f69b8a28e365c3a5_Image_002.jpg", "orig_filename": "00de251fbb4aaeb4cae73a26f69b8a28e365c3a5", "split": "train"}, {"article_id": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "description": {"raw": "Perceived speed bar chart of phraseflow in day 2,4,and 6 of the deployment study. Results are reported in section 7.4", "tokens": ["Perceived", "speed", "bar", "chart", "of", "phraseflow", "in", "day", "2,4", ",", "and", "6", "of", "the", "deployment", "study", ".", "Results", "are", "reported", "in", "section", "7.4"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input ", "tokens": ["PhraseFlow", ":", "Designs", "and", "Empirical", "Studies", "of", "Phrase-Level", "Input"]}, "filename": "1b2d659346931daf853e0640d874557a86b4eb2a_Image_019.png", "orig_filename": "1b2d659346931daf853e0640d874557a86b4eb2a", "split": "train"}, {"article_id": "The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations", "description": {"raw": "Two images of a tablet screen. On the left, the tablet shows a video with a small satellite crossing in front of a large star, and shows a visual chart of brightness. On the right, the same video is shown on the Haptic Video Player, with one robot on the screen representing the satellite, and another the approximate brightness level.", "tokens": ["Two", "images", "of", "a", "tablet", "screen", ".", "On", "the", "left", ",", "the", "tablet", "shows", "a", "video", "with", "a", "small", "satellite", "crossing", "in", "front", "of", "a", "large", "star", ",", "and", "shows", "a", "visual", "chart", "of", "brightness", ".", "On", "the", "right", ",", "the", "same", "video", "is", "shown", "on", "the", "Haptic", "Video", "Player", ",", "with", "one", "robot", "on", "the", "screen", "representing", "the", "satellite", ",", "and", "another", "the", "approximate", "brightness", "level", "."]}, "caption": {"raw": "Figure 1. Side-by-side view of the original video of a small satellite crossing in front of a star and a graph of a temporarily decreasing brightness level (left), and a haptic representation of the same frame with robots representing the satellite (top robot), and the relative brightness level on the graph (bottom robot).", "tokens": ["Figure", "1", ".", "Side-by-side", "view", "of", "the", "original", "video", "of", "a", "small", "satellite", "crossing", "in", "front", "of", "a", "star", "and", "a", "graph", "of", "a", "temporarily", "decreasing", "brightness", "level", "(", "left", ")", ",", "and", "a", "haptic", "representation", "of", "the", "same", "frame", "with", "robots", "representing", "the", "satellite", "(", "top", "robot", ")", ",", "and", "the", "relative", "brightness", "level", "on", "the", "graph", "(", "bottom", "robot", ")", "."]}, "context": {"raw": "The Haptic Video Player: Using Mobile Robots to Create Tangible Video Annotations Figure 1. Side-by-side view of the original video of a small satellite crossing in front of a star and a graph of a temporarily decreasing brightness level (left), and a haptic representation of the same frame with robots representing the satellite (top robot), and the relative brightness level on the graph (bottom robot).", "tokens": ["The", "Haptic", "Video", "Player", ":", "Using", "Mobile", "Robots", "to", "Create", "Tangible", "Video", "Annotations", "Figure", "1", ".", "Side-by-side", "view", "of", "the", "original", "video", "of", "a", "small", "satellite", "crossing", "in", "front", "of", "a", "star", "and", "a", "graph", "of", "a", "temporarily", "decreasing", "brightness", "level", "(", "left", ")", ",", "and", "a", "haptic", "representation", "of", "the", "same", "frame", "with", "robots", "representing", "the", "satellite", "(", "top", "robot", ")", ",", "and", "the", "relative", "brightness", "level", "on", "the", "graph", "(", "bottom", "robot", ")", "."]}, "filename": "280e9f6d4d794db1dd363bee52dd8190d3612892_Image_001.jpg", "orig_filename": "280e9f6d4d794db1dd363bee52dd8190d3612892", "split": "train"}, {"article_id": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code", "description": {"raw": "This is a bar chart of the mean response on the semantically anchored scales.    Easy: Without Tool 4.1, with tool 4.1  Frustration: Without Tool 4.3, with tool 5  Knew Location in the Code: Without Tool 4.1, with tool 5.6", "tokens": ["This", "is", "a", "bar", "chart", "of", "the", "mean", "response", "on", "the", "semantically", "anchored", "scales", ".", "Easy", ":", "Without", "Tool", "4.1", ",", "with", "tool", "4.1", "Frustration", ":", "Without", "Tool", "4.3", ",", "with", "tool", "5", "Knew", "Location", "in", "the", "Code", ":", "Without", "Tool", "4.1", ",", "with", "tool", "5.6"]}, "caption": {"raw": "Figure 7. This chart show the average score for the partic- ipants for the semantically anchored questions. A higher value is better for all three questions. The bars represent the standard error.", "tokens": ["Figure", "7", ".", "This", "chart", "show", "the", "average", "score", "for", "the", "partic-", "ipants", "for", "the", "semantically", "anchored", "questions", ".", "A", "higher", "value", "is", "better", "for", "all", "three", "questions", ".", "The", "bars", "represent", "the", "standard", "error", "."]}, "context": {"raw": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code Figure 7. This chart show the average score for the partic- ipants for the semantically anchored questions. A higher value is better for all three questions. The bars represent the standard error.", "tokens": ["StructJumper", ":", "A", "Tool", "to", "Help", "Blind", "Programmers", "Navigate", "and", "Understand", "the", "Structure", "of", "Code", "Figure", "7", ".", "This", "chart", "show", "the", "average", "score", "for", "the", "partic-", "ipants", "for", "the", "semantically", "anchored", "questions", ".", "A", "higher", "value", "is", "better", "for", "all", "three", "questions", ".", "The", "bars", "represent", "the", "standard", "error", "."]}, "filename": "13eccfcfd4ce807e14da418115a386b918870582_Image_007.jpg", "orig_filename": "13eccfcfd4ce807e14da418115a386b918870582", "split": "train"}, {"article_id": "Reading and Learning Smartfonts", "description": {"raw": "The colored (red, blue, and black) 2x3 characters used for Tricolor. They are presented as a 2D graph, where similar characters are closer together.", "tokens": ["The", "colored", "(", "red", ",", "blue", ",", "and", "black", ")", "2x3", "characters", "used", "for", "Tricolor", ".", "They", "are", "presented", "as", "a", "2D", "graph", ",", "where", "similar", "characters", "are", "closer", "together", "."]}, "caption": {"raw": "Figure 9: A D3 [11] force-directed layout of Tricolor attempts to locate similar pairs of letters near one another and also illustrates the colors chosen by our optimization algorithm.", "tokens": ["Figure", "9", ":", "A", "D3", "[", "11", "]", "force-directed", "layout", "of", "Tricolor", "attempts", "to", "locate", "similar", "pairs", "of", "letters", "near", "one", "another", "and", "also", "illustrates", "the", "colors", "chosen", "by", "our", "optimization", "algorithm", "."]}, "context": {"raw": "Reading and Learning Smartfonts Figure 9: A D3 [11] force-directed layout of Tricolor attempts to locate similar pairs of letters near one another and also illustrates the colors chosen by our optimization algorithm.", "tokens": ["Reading", "and", "Learning", "Smartfonts", "Figure", "9", ":", "A", "D3", "[", "11", "]", "force-directed", "layout", "of", "Tricolor", "attempts", "to", "locate", "similar", "pairs", "of", "letters", "near", "one", "another", "and", "also", "illustrates", "the", "colors", "chosen", "by", "our", "optimization", "algorithm", "."]}, "filename": "3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_017.jpg", "orig_filename": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "split": "train"}, {"article_id": "TouchCam: Realtime Recognition of Location-Specific On-Body Gestures to Support Users with Visual Impairments", "description": {"raw": "Study 1 data collection. Left: a participant follows on-screen instructions to complete our data collection protocol while wearing the prototype. Right: Example skin images collected during study 1 across the 15 fine-grained classes.", "tokens": ["Study", "1", "data", "collection", ".", "Left", ":", "a", "participant", "follows", "on-screen", "instructions", "to", "complete", "our", "data", "collection", "protocol", "while", "wearing", "the", "prototype", ".", "Right", ":", "Example", "skin", "images", "collected", "during", "study", "1", "across", "the", "15", "fine-grained", "classes", "."]}, "caption": {"raw": "Participant following on-screen data collection protocol                                       (b) Example skin images from Study IFig. 3. (a) Data collection setup showing our prototype, location and gesture instructions, and camera video feed. (b) Example skin-surface images recorded by  our finger-mounted camera (fingerprint images  omitted to  protect  our participants’ privacy).XX:10 • L. Stearns et al.Procedure. The procedure lasted up to 90 minutes. After a brief demographic questionnaire and setup period (i.e., selecting rings, putting on the prototype), participants completed the following tasks, in order:Location-specific touches. Participants touched and held their finger in place at 15 locations (Figure 2b) with each location prompted visually on a monitor (Figure 3a). After confirming the location and image quality, the experimenter logged the current location (e.g., timestamp, location label) and triggered the start of the next trial. Participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 15 locations (150 trials in total). In total, this dataset includes 3600 location-specific touches across all participants. Example images are shown in Figure 3b.Location-specific gestures. Participants performed the eight basic gestures: tap, swipe up, swipe down, swipe left, swipe right, circle, triangle, and square (Figure 2c) at three body locations: the palm, wrist, and thigh. These locations were selected from the 15 locations in the first task because they are easy to access, unobtrusive, and have a relatively large input area thus allowing for more complex gestures. As with the first task, participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 24 gesture and location combinations (240 trials in total). This dataset includes 5,760 location-specific gestures across all participants.", "tokens": ["Participant", "following", "on-screen", "data", "collection", "protocol", "(", "b", ")", "Example", "skin", "images", "from", "Study", "IFig", ".", "3", ".", "(", "a", ")", "Data", "collection", "setup", "showing", "our", "prototype", ",", "location", "and", "gesture", "instructions", ",", "and", "camera", "video", "feed", ".", "(", "b", ")", "Example", "skin-surface", "images", "recorded", "by", "our", "finger-mounted", "camera", "(", "fingerprint", "images", "omitted", "to", "protect", "our", "participants", "’", "privacy", ")", ".XX:10", "•", "L.", "Stearns", "et", "al.Procedure", ".", "The", "procedure", "lasted", "up", "to", "90", "minutes", ".", "After", "a", "brief", "demographic", "questionnaire", "and", "setup", "period", "(", "i.e.", ",", "selecting", "rings", ",", "putting", "on", "the", "prototype", ")", ",", "participants", "completed", "the", "following", "tasks", ",", "in", "order", ":", "Location-specific", "touches", ".", "Participants", "touched", "and", "held", "their", "finger", "in", "place", "at", "15", "locations", "(", "Figure", "2b", ")", "with", "each", "location", "prompted", "visually", "on", "a", "monitor", "(", "Figure", "3a", ")", ".", "After", "confirming", "the", "location", "and", "image", "quality", ",", "the", "experimenter", "logged", "the", "current", "location", "(", "e.g.", ",", "timestamp", ",", "location", "label", ")", "and", "triggered", "the", "start", "of", "the", "next", "trial", ".", "Participants", "completed", "10", "blocks", "of", "trials", ",", "where", "each", "block", "consisted", "of", "a", "different", "random", "permutation", "of", "the", "15", "locations", "(", "150", "trials", "in", "total", ")", ".", "In", "total", ",", "this", "dataset", "includes", "3600", "location-specific", "touches", "across", "all", "participants", ".", "Example", "images", "are", "shown", "in", "Figure", "3b.Location-specific", "gestures", ".", "Participants", "performed", "the", "eight", "basic", "gestures", ":", "tap", ",", "swipe", "up", ",", "swipe", "down", ",", "swipe", "left", ",", "swipe", "right", ",", "circle", ",", "triangle", ",", "and", "square", "(", "Figure", "2c", ")", "at", "three", "body", "locations", ":", "the", "palm", ",", "wrist", ",", "and", "thigh", ".", "These", "locations", "were", "selected", "from", "the", "15", "locations", "in", "the", "first", "task", "because", "they", "are", "easy", "to", "access", ",", "unobtrusive", ",", "and", "have", "a", "relatively", "large", "input", "area", "thus", "allowing", "for", "more", "complex", "gestures", ".", "As", "with", "the", "first", "task", ",", "participants", "completed", "10", "blocks", "of", "trials", ",", "where", "each", "block", "consisted", "of", "a", "different", "random", "permutation", "of", "the", "24", "gesture", "and", "location", "combinations", "(", "240", "trials", "in", "total", ")", ".", "This", "dataset", "includes", "5,760", "location-specific", "gestures", "across", "all", "participants", "."]}, "context": {"raw": "TouchCam: Realtime Recognition of Location-Specific On-Body Gestures to Support Users with Visual Impairments Participant following on-screen data collection protocol                                       (b) Example skin images from Study IFig. 3. (a) Data collection setup showing our prototype, location and gesture instructions, and camera video feed. (b) Example skin-surface images recorded by  our finger-mounted camera (fingerprint images  omitted to  protect  our participants’ privacy).XX:10 • L. Stearns et al.Procedure. The procedure lasted up to 90 minutes. After a brief demographic questionnaire and setup period (i.e., selecting rings, putting on the prototype), participants completed the following tasks, in order:Location-specific touches. Participants touched and held their finger in place at 15 locations (Figure 2b) with each location prompted visually on a monitor (Figure 3a). After confirming the location and image quality, the experimenter logged the current location (e.g., timestamp, location label) and triggered the start of the next trial. Participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 15 locations (150 trials in total). In total, this dataset includes 3600 location-specific touches across all participants. Example images are shown in Figure 3b.Location-specific gestures. Participants performed the eight basic gestures: tap, swipe up, swipe down, swipe left, swipe right, circle, triangle, and square (Figure 2c) at three body locations: the palm, wrist, and thigh. These locations were selected from the 15 locations in the first task because they are easy to access, unobtrusive, and have a relatively large input area thus allowing for more complex gestures. As with the first task, participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 24 gesture and location combinations (240 trials in total). This dataset includes 5,760 location-specific gestures across all participants.", "tokens": ["TouchCam", ":", "Realtime", "Recognition", "of", "Location-Specific", "On-Body", "Gestures", "to", "Support", "Users", "with", "Visual", "Impairments", "Participant", "following", "on-screen", "data", "collection", "protocol", "(", "b", ")", "Example", "skin", "images", "from", "Study", "IFig", ".", "3", ".", "(", "a", ")", "Data", "collection", "setup", "showing", "our", "prototype", ",", "location", "and", "gesture", "instructions", ",", "and", "camera", "video", "feed", ".", "(", "b", ")", "Example", "skin-surface", "images", "recorded", "by", "our", "finger-mounted", "camera", "(", "fingerprint", "images", "omitted", "to", "protect", "our", "participants", "’", "privacy", ")", ".XX:10", "•", "L.", "Stearns", "et", "al.Procedure", ".", "The", "procedure", "lasted", "up", "to", "90", "minutes", ".", "After", "a", "brief", "demographic", "questionnaire", "and", "setup", "period", "(", "i.e.", ",", "selecting", "rings", ",", "putting", "on", "the", "prototype", ")", ",", "participants", "completed", "the", "following", "tasks", ",", "in", "order", ":", "Location-specific", "touches", ".", "Participants", "touched", "and", "held", "their", "finger", "in", "place", "at", "15", "locations", "(", "Figure", "2b", ")", "with", "each", "location", "prompted", "visually", "on", "a", "monitor", "(", "Figure", "3a", ")", ".", "After", "confirming", "the", "location", "and", "image", "quality", ",", "the", "experimenter", "logged", "the", "current", "location", "(", "e.g.", ",", "timestamp", ",", "location", "label", ")", "and", "triggered", "the", "start", "of", "the", "next", "trial", ".", "Participants", "completed", "10", "blocks", "of", "trials", ",", "where", "each", "block", "consisted", "of", "a", "different", "random", "permutation", "of", "the", "15", "locations", "(", "150", "trials", "in", "total", ")", ".", "In", "total", ",", "this", "dataset", "includes", "3600", "location-specific", "touches", "across", "all", "participants", ".", "Example", "images", "are", "shown", "in", "Figure", "3b.Location-specific", "gestures", ".", "Participants", "performed", "the", "eight", "basic", "gestures", ":", "tap", ",", "swipe", "up", ",", "swipe", "down", ",", "swipe", "left", ",", "swipe", "right", ",", "circle", ",", "triangle", ",", "and", "square", "(", "Figure", "2c", ")", "at", "three", "body", "locations", ":", "the", "palm", ",", "wrist", ",", "and", "thigh", ".", "These", "locations", "were", "selected", "from", "the", "15", "locations", "in", "the", "first", "task", "because", "they", "are", "easy", "to", "access", ",", "unobtrusive", ",", "and", "have", "a", "relatively", "large", "input", "area", "thus", "allowing", "for", "more", "complex", "gestures", ".", "As", "with", "the", "first", "task", ",", "participants", "completed", "10", "blocks", "of", "trials", ",", "where", "each", "block", "consisted", "of", "a", "different", "random", "permutation", "of", "the", "24", "gesture", "and", "location", "combinations", "(", "240", "trials", "in", "total", ")", ".", "This", "dataset", "includes", "5,760", "location-specific", "gestures", "across", "all", "participants", "."]}, "filename": "a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c_Image_013.jpg", "orig_filename": "a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c", "split": "train"}, {"article_id": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing", "description": {"raw": "Bar plot of Accuracy and F1 for detecting change in depression. Baseline accuracy was 65.9%. Bluetooth accuracy was 65.8% and F1 was 0.55. Calls accuracy was 64.8% and F1 was 0.50. Campus Map accuracy was 79.1% and F1 was 0.80. Location accuracy was 74.3% and F1 was 0.73. Phone Usage accuracy was 73.9% and F1 was 0.68. Sleep accuracy was 73.8% and F1 was 0.66. Steps accuracy was 68.2% and F1 was 0.56. All-7 accuracy was 75.9% and F1 was 0.67. Best set accuracy was 85.4% and F1 was 0.80.", "tokens": ["Bar", "plot", "of", "Accuracy", "and", "F1", "for", "detecting", "change", "in", "depression", ".", "Baseline", "accuracy", "was", "65.9", "%", ".", "Bluetooth", "accuracy", "was", "65.8", "%", "and", "F1", "was", "0.55", ".", "Calls", "accuracy", "was", "64.8", "%", "and", "F1", "was", "0.50", ".", "Campus", "Map", "accuracy", "was", "79.1", "%", "and", "F1", "was", "0.80", ".", "Location", "accuracy", "was", "74.3", "%", "and", "F1", "was", "0.73", ".", "Phone", "Usage", "accuracy", "was", "73.9", "%", "and", "F1", "was", "0.68", ".", "Sleep", "accuracy", "was", "73.8", "%", "and", "F1", "was", "0.66", ".", "Steps", "accuracy", "was", "68.2", "%", "and", "F1", "was", "0.56", ".", "All-7", "accuracy", "was", "75.9", "%", "and", "F1", "was", "0.67", ".", "Best", "set", "accuracy", "was", "85.4", "%", "and", "F1", "was", "0.80", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing ", "tokens": ["Detecting", "Depression", "and", "Predicting", "its", "Onset", "Using", "Longitudinal", "Symptoms", "Captured", "by", "Passive", "Sensing"]}, "filename": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2_Image_013.jpg", "orig_filename": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2", "split": "train"}, {"article_id": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "description": {"raw": "On the left is a bar graph of the average selection times for each participant. The maximum value is about 1150~ms (P2) while the minimum value is about 700~ms (P9). On the right is a histogram of the selection times of all participants. Most selections occurred between 500~ms and 800~ms. However, there were about 30 selections that took over 1500~ms.", "tokens": ["On", "the", "left", "is", "a", "bar", "graph", "of", "the", "average", "selection", "times", "for", "each", "participant", ".", "The", "maximum", "value", "is", "about", "1150~ms", "(", "P2", ")", "while", "the", "minimum", "value", "is", "about", "700~ms", "(", "P9", ")", ".", "On", "the", "right", "is", "a", "histogram", "of", "the", "selection", "times", "of", "all", "participants", ".", "Most", "selections", "occurred", "between", "500~ms", "and", "800~ms", ".", "However", ",", "there", "were", "about", "30", "selections", "that", "took", "over", "1500~ms", "."]}, "caption": {"raw": "Figure 7: Results of shape recognition task: (a) Average ac- curacy, (b) stimulus-response confusion matrix. (Error bar represents standard error)", "tokens": ["Figure", "7", ":", "Results", "of", "shape", "recognition", "task", ":", "(", "a", ")", "Average", "ac-", "curacy", ",", "(", "b", ")", "stimulus-response", "confusion", "matrix", ".", "(", "Error", "bar", "represents", "standard", "error", ")"]}, "context": {"raw": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects Figure 7: Results of shape recognition task: (a) Average ac- curacy, (b) stimulus-response confusion matrix. (Error bar represents standard error)", "tokens": ["ThroughHand", ":", "2D", "Tactile", "Interaction", "to", "Simultaneously", "Recognize", "and", "Touch", "Multiple", "Objects", "Figure", "7", ":", "Results", "of", "shape", "recognition", "task", ":", "(", "a", ")", "Average", "ac-", "curacy", ",", "(", "b", ")", "stimulus-response", "confusion", "matrix", ".", "(", "Error", "bar", "represents", "standard", "error", ")"]}, "filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_013.png", "orig_filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "split": "train"}, {"article_id": "Diagramming Working Field Theories for Design in the HCI Classroom", "description": {"raw": "Figure 5: The figure shows two images. The left hand side shows a photo of a Recipe Globe device. A jar contains the ingredients of the recipe, and the lid shows buttons and components that allow interaction with the device, including audio playback controls, microphone and speaker. The figure on the right hand side shows a flow chart of cooking: decision making, preparing ingredients, follow recipe or cook by memory, finished meal.", "tokens": ["Figure", "5", ":", "The", "figure", "shows", "two", "images", ".", "The", "left", "hand", "side", "shows", "a", "photo", "of", "a", "Recipe", "Globe", "device", ".", "A", "jar", "contains", "the", "ingredients", "of", "the", "recipe", ",", "and", "the", "lid", "shows", "buttons", "and", "components", "that", "allow", "interaction", "with", "the", "device", ",", "including", "audio", "playback", "controls", ",", "microphone", "and", "speaker", ".", "The", "figure", "on", "the", "right", "hand", "side", "shows", "a", "flow", "chart", "of", "cooking", ":", "decision", "making", ",", "preparing", "ingredients", ",", "follow", "recipe", "or", "cook", "by", "memory", ",", "finished", "meal", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Diagramming Working Field Theories for Design in the HCI Classroom ", "tokens": ["Diagramming", "Working", "Field", "Theories", "for", "Design", "in", "the", "HCI", "Classroom"]}, "filename": "680984459a2ab9dcaed587a6e5775a25ce140804_Image_006.jpg", "orig_filename": "680984459a2ab9dcaed587a6e5775a25ce140804", "split": "train"}, {"article_id": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping", "description": {"raw": "A line chart showing mean acquisition time across touch types and trial interfaces, showing magnification is the slowest touch type, and then targets list and then conventional tapping.", "tokens": ["A", "line", "chart", "showing", "mean", "acquisition", "time", "across", "touch", "types", "and", "trial", "interfaces", ",", "showing", "magnification", "is", "the", "slowest", "touch", "type", ",", "and", "then", "targets", "list", "and", "then", "conventional", "tapping", "."]}, "caption": {"raw": "Figure 3. Mean acquisition time across touch types and trial interfaces.", "tokens": ["Figure", "3", ".", "Mean", "acquisition", "time", "across", "touch", "types", "and", "trial", "interfaces", "."]}, "context": {"raw": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping Figure 3. Mean acquisition time across touch types and trial interfaces.", "tokens": ["Enhancing", "Android", "accessibility", "for", "users", "with", "hand", "tremor", "by", "reducing", "fine", "pointing", "and", "steady", "tapping", "Figure", "3", ".", "Mean", "acquisition", "time", "across", "touch", "types", "and", "trial", "interfaces", "."]}, "filename": "dfa09494b50030571735aafbe5c114cd9ba80eba_Image_003.png", "orig_filename": "dfa09494b50030571735aafbe5c114cd9ba80eba", "split": "train"}, {"article_id": "Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments", "description": {"raw": "Figure 6  This figure shows two line graphs. The top graph shows the average binned offset along the x-axes of the smartphone screen and the location of clusters, three in total. The bottom graph shows the average binned offset along the y-axes of the smartphone screen and the location of the clusters, three in total.", "tokens": ["Figure", "6", "This", "figure", "shows", "two", "line", "graphs", ".", "The", "top", "graph", "shows", "the", "average", "binned", "offset", "along", "the", "x-axes", "of", "the", "smartphone", "screen", "and", "the", "location", "of", "clusters", ",", "three", "in", "total", ".", "The", "bottom", "graph", "shows", "the", "average", "binned", "offset", "along", "the", "y-axes", "of", "the", "smartphone", "screen", "and", "the", "location", "of", "the", "clusters", ",", "three", "in", "total", "."]}, "caption": {"raw": "𝑥", "tokens": ["𝑥"]}, "context": {"raw": "Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments 𝑥", "tokens": ["Cluster", "Touch", ":", "Improving", "Touch", "Accuracy", "on", "Smartphones", "for", "People", "with", "Motor", "and", "Situational", "Impairments", "𝑥"]}, "filename": "2efeaa97efaaca7c70ca7b4294dc12294b509aca_Image_016.jpg", "orig_filename": "2efeaa97efaaca7c70ca7b4294dc12294b509aca", "split": "train"}, {"article_id": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "description": {"raw": "Figure 2 - Histogram showing the distribution of word error rate of transcriptions generated by individual transcribers. Clips at intelligibility level 50 tended to have low word error rate, those at intelligibility level 30 tended to have high word error rate, and those at intelligibility level 40 had more evenly distributed word error rate.    Figure 3 - Bar graph showing average word error rates at the three intelligibility levels (30, 40, 50), for both automated and individual crowd worker approaches. Word error rate increased as clip intelligibility decreased. Individual transcribers outperformed the automated approach at each intelligibility level.", "tokens": ["Figure", "2", "-", "Histogram", "showing", "the", "distribution", "of", "word", "error", "rate", "of", "transcriptions", "generated", "by", "individual", "transcribers", ".", "Clips", "at", "intelligibility", "level", "50", "tended", "to", "have", "low", "word", "error", "rate", ",", "those", "at", "intelligibility", "level", "30", "tended", "to", "have", "high", "word", "error", "rate", ",", "and", "those", "at", "intelligibility", "level", "40", "had", "more", "evenly", "distributed", "word", "error", "rate", ".", "Figure", "3", "-", "Bar", "graph", "showing", "average", "word", "error", "rates", "at", "the", "three", "intelligibility", "levels", "(", "30", ",", "40", ",", "50", ")", ",", "for", "both", "automated", "and", "individual", "crowd", "worker", "approaches", ".", "Word", "error", "rate", "increased", "as", "clip", "intelligibility", "decreased", ".", "Individual", "transcribers", "outperformed", "the", "automated", "approach", "at", "each", "intelligibility", "level", "."]}, "caption": {"raw": "Figure 2. Word error rate (WER) distribution of transcriptions gen- erated by individual crowd workers, separated by three levels of clip intelligibility. A lower WER is better and indicates a more accu- rate transcription. More intelligible clips tended to have lower WER, while less intelligible clips tended to have higher WER.", "tokens": ["Figure", "2", ".", "Word", "error", "rate", "(", "WER", ")", "distribution", "of", "transcriptions", "gen-", "erated", "by", "individual", "crowd", "workers", ",", "separated", "by", "three", "levels", "of", "clip", "intelligibility", ".", "A", "lower", "WER", "is", "better", "and", "indicates", "a", "more", "accu-", "rate", "transcription", ".", "More", "intelligible", "clips", "tended", "to", "have", "lower", "WER", ",", "while", "less", "intelligible", "clips", "tended", "to", "have", "higher", "WER", "."]}, "context": {"raw": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users Figure 2. Word error rate (WER) distribution of transcriptions gen- erated by individual crowd workers, separated by three levels of clip intelligibility. A lower WER is better and indicates a more accu- rate transcription. More intelligible clips tended to have lower WER, while less intelligible clips tended to have higher WER.", "tokens": ["Towards", "More", "Robust", "Speech", "Interactions", "for", "Deaf", "and", "Hard", "of", "Hearing", "Users", "Figure", "2", ".", "Word", "error", "rate", "(", "WER", ")", "distribution", "of", "transcriptions", "gen-", "erated", "by", "individual", "crowd", "workers", ",", "separated", "by", "three", "levels", "of", "clip", "intelligibility", ".", "A", "lower", "WER", "is", "better", "and", "indicates", "a", "more", "accu-", "rate", "transcription", ".", "More", "intelligible", "clips", "tended", "to", "have", "lower", "WER", ",", "while", "less", "intelligible", "clips", "tended", "to", "have", "higher", "WER", "."]}, "filename": "8bba1845a85370618cd5c400ec8be42208554549_Image_002.jpg", "orig_filename": "8bba1845a85370618cd5c400ec8be42208554549", "split": "train"}, {"article_id": "Predicting Cognitive Load in Future Code Puzzles", "description": {"raw": "A bar graph showing the feature importance by category. Germane was the most important category followed by Intrinsic, Participant's Characteristics, and Extraneous.", "tokens": ["A", "bar", "graph", "showing", "the", "feature", "importance", "by", "category", ".", "Germane", "was", "the", "most", "important", "category", "followed", "by", "Intrinsic", ",", "Participant", "'s", "Characteristics", ",", "and", "Extraneous", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Predicting Cognitive Load in Future Code Puzzles ", "tokens": ["Predicting", "Cognitive", "Load", "in", "Future", "Code", "Puzzles"]}, "filename": "7ce152841cc96d0d4c20e8a67c43692e1b742ddb_Image_010.jpg", "orig_filename": "7ce152841cc96d0d4c20e8a67c43692e1b742ddb", "split": "train"}, {"article_id": "In the blink of an eye: investigating latency perception during stylus interaction", "description": {"raw": "Bar graph of the JND latency thresholds gathered from each participant during the small box dragging task.", "tokens": ["Bar", "graph", "of", "the", "JND", "latency", "thresholds", "gathered", "from", "each", "participant", "during", "the", "small", "box", "dragging", "task", "."]}, "caption": {"raw": "The results from the scribbling task demonstrated that participants were able to discriminate between the 7- millisecond baseline latency and a median of 40 milliseconds (Figure 11; range 10 - 70 milliseconds). Although not directly comparable to the dragging tasks, such results suggest that task demands may play a role in the perception of latency. The higher perceived latency found while scribbling compared to dragging are likely due to the different visual feedback available, the strategies used to determine latency, or the cognitive loading encountered while scribbling versus dragging the box.", "tokens": ["The", "results", "from", "the", "scribbling", "task", "demonstrated", "that", "participants", "were", "able", "to", "discriminate", "between", "the", "7-", "millisecond", "baseline", "latency", "and", "a", "median", "of", "40", "milliseconds", "(", "Figure", "11", ";", "range", "10", "-", "70", "milliseconds", ")", ".", "Although", "not", "directly", "comparable", "to", "the", "dragging", "tasks", ",", "such", "results", "suggest", "that", "task", "demands", "may", "play", "a", "role", "in", "the", "perception", "of", "latency", ".", "The", "higher", "perceived", "latency", "found", "while", "scribbling", "compared", "to", "dragging", "are", "likely", "due", "to", "the", "different", "visual", "feedback", "available", ",", "the", "strategies", "used", "to", "determine", "latency", ",", "or", "the", "cognitive", "loading", "encountered", "while", "scribbling", "versus", "dragging", "the", "box", "."]}, "context": {"raw": "In the blink of an eye: investigating latency perception during stylus interaction The results from the scribbling task demonstrated that participants were able to discriminate between the 7- millisecond baseline latency and a median of 40 milliseconds (Figure 11; range 10 - 70 milliseconds). Although not directly comparable to the dragging tasks, such results suggest that task demands may play a role in the perception of latency. The higher perceived latency found while scribbling compared to dragging are likely due to the different visual feedback available, the strategies used to determine latency, or the cognitive loading encountered while scribbling versus dragging the box.", "tokens": ["In", "the", "blink", "of", "an", "eye", ":", "investigating", "latency", "perception", "during", "stylus", "interaction", "The", "results", "from", "the", "scribbling", "task", "demonstrated", "that", "participants", "were", "able", "to", "discriminate", "between", "the", "7-", "millisecond", "baseline", "latency", "and", "a", "median", "of", "40", "milliseconds", "(", "Figure", "11", ";", "range", "10", "-", "70", "milliseconds", ")", ".", "Although", "not", "directly", "comparable", "to", "the", "dragging", "tasks", ",", "such", "results", "suggest", "that", "task", "demands", "may", "play", "a", "role", "in", "the", "perception", "of", "latency", ".", "The", "higher", "perceived", "latency", "found", "while", "scribbling", "compared", "to", "dragging", "are", "likely", "due", "to", "the", "different", "visual", "feedback", "available", ",", "the", "strategies", "used", "to", "determine", "latency", ",", "or", "the", "cognitive", "loading", "encountered", "while", "scribbling", "versus", "dragging", "the", "box", "."]}, "filename": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_010.jpg", "orig_filename": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "split": "train"}, {"article_id": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "description": {"raw": "A horizontal stacked bar graph of participants' Likert responses to the helpful question. The x axis is numeric, ranging from 0 to 100%. The y axis is the 4 markup styles: Yellow, Underline, No Change, and Italics. All markup styles had similar percentages of their scales which ranged from Strongly Disagree, Disagree, Neither agree nor disagree, Agree, and Strongly Agree. Yellow had 13.1%, 15%, 27.1%, 32.7%, and 12.1% for the scales. Underline had 15.9%, 19.6%, 19.6%, 33.6%, and 11.2%. No Change had 7.5%, 14%, 25.2%, 39.3%, and 14%. Italics had 9.3%, 18.7%, 26.2%, 31.8%, and 14%.", "tokens": ["A", "horizontal", "stacked", "bar", "graph", "of", "participants", "'", "Likert", "responses", "to", "the", "helpful", "question", ".", "The", "x", "axis", "is", "numeric", ",", "ranging", "from", "0", "to", "100", "%", ".", "The", "y", "axis", "is", "the", "4", "markup", "styles", ":", "Yellow", ",", "Underline", ",", "No", "Change", ",", "and", "Italics", ".", "All", "markup", "styles", "had", "similar", "percentages", "of", "their", "scales", "which", "ranged", "from", "Strongly", "Disagree", ",", "Disagree", ",", "Neither", "agree", "nor", "disagree", ",", "Agree", ",", "and", "Strongly", "Agree", ".", "Yellow", "had", "13.1", "%", ",", "15", "%", ",", "27.1", "%", ",", "32.7", "%", ",", "and", "12.1", "%", "for", "the", "scales", ".", "Underline", "had", "15.9", "%", ",", "19.6", "%", ",", "19.6", "%", ",", "33.6", "%", ",", "and", "11.2", "%", ".", "No", "Change", "had", "7.5", "%", ",", "14", "%", ",", "25.2", "%", ",", "39.3", "%", ",", "and", "14", "%", ".", "Italics", "had", "9.3", "%", ",", "18.7", "%", ",", "26.2", "%", ",", "31.8", "%", ",", "and", "14", "%", "."]}, "caption": {"raw": "Figure 8. Larger Study: Helpful Responses (Likert)", "tokens": ["Figure", "8", ".", "Larger", "Study", ":", "Helpful", "Responses", "(", "Likert", ")"]}, "context": {"raw": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings Figure 8. Larger Study: Helpful Responses (Likert)", "tokens": ["Deaf", "and", "Hard-of-Hearing", "Perspectives", "on", "Imperfect", "Automatic", "Speech", "Recognition", "for", "Captioning", "One-on-One", "Meetings", "Figure", "8", ".", "Larger", "Study", ":", "Helpful", "Responses", "(", "Likert", ")"]}, "filename": "471f9168db0fcb72d394222491966b97c098b1cd_Image_018.jpg", "orig_filename": "471f9168db0fcb72d394222491966b97c098b1cd", "split": "train"}, {"article_id": "Theory is in the Eye of the Beholder: Exploring Difficulties with Validating Intervention Mechanisms: Theory is in the Eye of the Beholder", "description": {"raw": "This figure contains four images. \"A\" is a screenshot of a survey question of a \"Build your Skills\" prototype. First the prototype is described as, \"This intervention suggests different ways for patients to build skills and confidence in speaking up about their concerns. Instructions: Select the construct(s) from the Integrated Behavioral Model through which you think the intervention operates. Check all construct(s) that apply.\" The prototype is headed, \"Considering bringing up some concerns about your care?\" and contains three modules: \"Reflect on your skills,\" \"Imagine how the conversation might go,\" and \"See some examples of how people can speak up about their concerns.\" Then there is a list of all constructs in the IBM for respondents to choose from. \"B\" shows a prototype headed, \"What do clinicians think patients should do when it comes to speaking up?\" and shows a pie chart showing 95% of clinicians think that patients shouldspeak up when they have concerns about their care. \"C\" shows a prototype headed, \"How often do patients speak up?\" and showing a bar graph of how many patients at the hospital have spoken up to clinicians about their concerns in the last few days. \"D\" is headed \"Your participation makes you safer\" and shows a bar graph of how many times patients have helped the hospital prevent medical errors by speaking up about their needs and concerns to clinicians.", "tokens": ["This", "figure", "contains", "four", "images", ".", "``", "A", "''", "is", "a", "screenshot", "of", "a", "survey", "question", "of", "a", "``", "Build", "your", "Skills", "''", "prototype", ".", "First", "the", "prototype", "is", "described", "as", ",", "``", "This", "intervention", "suggests", "different", "ways", "for", "patients", "to", "build", "skills", "and", "confidence", "in", "speaking", "up", "about", "their", "concerns", ".", "Instructions", ":", "Select", "the", "construct", "(", "s", ")", "from", "the", "Integrated", "Behavioral", "Model", "through", "which", "you", "think", "the", "intervention", "operates", ".", "Check", "all", "construct", "(", "s", ")", "that", "apply", ".", "''", "The", "prototype", "is", "headed", ",", "``", "Considering", "bringing", "up", "some", "concerns", "about", "your", "care", "?", "''", "and", "contains", "three", "modules", ":", "``", "Reflect", "on", "your", "skills", ",", "''", "``", "Imagine", "how", "the", "conversation", "might", "go", ",", "''", "and", "``", "See", "some", "examples", "of", "how", "people", "can", "speak", "up", "about", "their", "concerns", ".", "''", "Then", "there", "is", "a", "list", "of", "all", "constructs", "in", "the", "IBM", "for", "respondents", "to", "choose", "from", ".", "``", "B", "''", "shows", "a", "prototype", "headed", ",", "``", "What", "do", "clinicians", "think", "patients", "should", "do", "when", "it", "comes", "to", "speaking", "up", "?", "''", "and", "shows", "a", "pie", "chart", "showing", "95", "%", "of", "clinicians", "think", "that", "patients", "shouldspeak", "up", "when", "they", "have", "concerns", "about", "their", "care", ".", "``", "C", "''", "shows", "a", "prototype", "headed", ",", "``", "How", "often", "do", "patients", "speak", "up", "?", "''", "and", "showing", "a", "bar", "graph", "of", "how", "many", "patients", "at", "the", "hospital", "have", "spoken", "up", "to", "clinicians", "about", "their", "concerns", "in", "the", "last", "few", "days", ".", "``", "D", "''", "is", "headed", "``", "Your", "participation", "makes", "you", "safer", "''", "and", "shows", "a", "bar", "graph", "of", "how", "many", "times", "patients", "have", "helped", "the", "hospital", "prevent", "medical", "errors", "by", "speaking", "up", "about", "their", "needs", "and", "concerns", "to", "clinicians", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Theory is in the Eye of the Beholder: Exploring Difficulties with Validating Intervention Mechanisms: Theory is in the Eye of the Beholder ", "tokens": ["Theory", "is", "in", "the", "Eye", "of", "the", "Beholder", ":", "Exploring", "Difficulties", "with", "Validating", "Intervention", "Mechanisms", ":", "Theory", "is", "in", "the", "Eye", "of", "the", "Beholder"]}, "filename": "23e02c0a571aa25d8382b50782ee0d687c64c861_Image_002.png", "orig_filename": "23e02c0a571aa25d8382b50782ee0d687c64c861", "split": "train"}, {"article_id": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "description": {"raw": "This figure is divided into two parts: (a) and (b). \n\nPart (a) of the figure shows a stacked bar plot showing percentages of 6-point scale responses for different categories of User Initiative. The title is \"User Initiative\", the y-axis has 5 categories; \"Toggle Suggestions\", \"No Suggestions\", \"Suggestions\", \"Replace All\", and \"Automatic\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The scale responses for 1-3 are on the left side, and 4-6 are on the right side. It can be seen that for “Toggle Suggestions”, “No Suggestions”, and “Suggestions”, the majority of responses are on the right side, while for the “Replace All” and “Automatic” conditions, the majority of responses are on the left side.\n\nFor \"Toggle Suggestions\" 8.33% responded 1, 0% for 2, 0% for 3, 33.33% for 4, 33.33% for 5, and 25% for 6.\nFor \"No Suggestions\" 0% responded 1, 8.33% for 2, 8.33% for 3, 33.33% for 4, 16.67% for 5, and 33.33% for 6.\nFor \"Suggestions\" 0% responded 1, 8.33% for 2, 16.67% for 3, 16.67% for 4, 41.67% for 5, and 16.67% for 6.\nFor \"Replace All\" 8.33% responded 1, 33.33% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 0% for 6.\nFor \"Automatic\" 16.67% responded 1, 33.33% for 2, 33.33% for 3, 8.33% for 4, 8.33% for 5, and 0% for 6.\n\nPart (b) of the figure shows a stacked bar plot showing percentages of 6-point scale responses for different categories of Change Visibility. The title is \"Change Visibility\", the y-axis has 4 categories; \"Trace\", \"Pop-up\", \"No Trace\", and \"Sidebar\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The scale responses for 1-3 are on the left side, and 4-6 are on the right side. For “Trace” and “Pop-up”, the majority of responses are on the right side. For “No Trace” and “Sidebar” the majority of responses are on the left side.\n\nFor \"Trace\" 0% responded 1, 0% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 41.67% for 6. \nFor \"Pop-up\" 8.33% responded 1, 25% for 2, 8.33% for 3, 8.33% for 4, 16.67% for 5, and 33.33% for 6. \nFor \"No Trace\" 8.33% responded 1, 33.33% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 41.67% for 6. \nFor \"Sidebar\" 50% responded 1, 10% for 2, 10% for 3, 20% for 4, 10% for 5, and 0% for 6.", "tokens": ["This", "figure", "is", "divided", "into", "two", "parts", ":", "(", "a", ")", "and", "(", "b", ")", ".", "Part", "(", "a", ")", "of", "the", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "6-point", "scale", "responses", "for", "different", "categories", "of", "User", "Initiative", ".", "The", "title", "is", "``", "User", "Initiative", "''", ",", "the", "y-axis", "has", "5", "categories", ";", "``", "Toggle", "Suggestions", "''", ",", "``", "No", "Suggestions", "''", ",", "``", "Suggestions", "''", ",", "``", "Replace", "All", "''", ",", "and", "``", "Automatic", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "The", "scale", "responses", "for", "1-3", "are", "on", "the", "left", "side", ",", "and", "4-6", "are", "on", "the", "right", "side", ".", "It", "can", "be", "seen", "that", "for", "“", "Toggle", "Suggestions", "”", ",", "“", "No", "Suggestions", "”", ",", "and", "“", "Suggestions", "”", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ",", "while", "for", "the", "“", "Replace", "All", "”", "and", "“", "Automatic", "”", "conditions", ",", "the", "majority", "of", "responses", "are", "on", "the", "left", "side", ".", "For", "``", "Toggle", "Suggestions", "''", "8.33", "%", "responded", "1", ",", "0", "%", "for", "2", ",", "0", "%", "for", "3", ",", "33.33", "%", "for", "4", ",", "33.33", "%", "for", "5", ",", "and", "25", "%", "for", "6", ".", "For", "``", "No", "Suggestions", "''", "0", "%", "responded", "1", ",", "8.33", "%", "for", "2", ",", "8.33", "%", "for", "3", ",", "33.33", "%", "for", "4", ",", "16.67", "%", "for", "5", ",", "and", "33.33", "%", "for", "6", ".", "For", "``", "Suggestions", "''", "0", "%", "responded", "1", ",", "8.33", "%", "for", "2", ",", "16.67", "%", "for", "3", ",", "16.67", "%", "for", "4", ",", "41.67", "%", "for", "5", ",", "and", "16.67", "%", "for", "6", ".", "For", "``", "Replace", "All", "''", "8.33", "%", "responded", "1", ",", "33.33", "%", "for", "2", ",", "16.67", "%", "for", "3", ",", "16.67", "%", "for", "4", ",", "25", "%", "for", "5", ",", "and", "0", "%", "for", "6", ".", "For", "``", "Automatic", "''", "16.67", "%", "responded", "1", ",", "33.33", "%", "for", "2", ",", "33.33", "%", "for", "3", ",", "8.33", "%", "for", "4", ",", "8.33", "%", "for", "5", ",", "and", "0", "%", "for", "6", ".", "Part", "(", "b", ")", "of", "the", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "6-point", "scale", "responses", "for", "different", "categories", "of", "Change", "Visibility", ".", "The", "title", "is", "``", "Change", "Visibility", "''", ",", "the", "y-axis", "has", "4", "categories", ";", "``", "Trace", "''", ",", "``", "Pop-up", "''", ",", "``", "No", "Trace", "''", ",", "and", "``", "Sidebar", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "The", "scale", "responses", "for", "1-3", "are", "on", "the", "left", "side", ",", "and", "4-6", "are", "on", "the", "right", "side", ".", "For", "“", "Trace", "”", "and", "“", "Pop-up", "”", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ".", "For", "“", "No", "Trace", "”", "and", "“", "Sidebar", "”", "the", "majority", "of", "responses", "are", "on", "the", "left", "side", ".", "For", "``", "Trace", "''", "0", "%", "responded", "1", ",", "0", "%", "for", "2", ",", "16.67", "%", "for", "3", ",", "16.67", "%", "for", "4", ",", "25", "%", "for", "5", ",", "and", "41.67", "%", "for", "6", ".", "For", "``", "Pop-up", "''", "8.33", "%", "responded", "1", ",", "25", "%", "for", "2", ",", "8.33", "%", "for", "3", ",", "8.33", "%", "for", "4", ",", "16.67", "%", "for", "5", ",", "and", "33.33", "%", "for", "6", ".", "For", "``", "No", "Trace", "''", "8.33", "%", "responded", "1", ",", "33.33", "%", "for", "2", ",", "16.67", "%", "for", "3", ",", "16.67", "%", "for", "4", ",", "25", "%", "for", "5", ",", "and", "41.67", "%", "for", "6", ".", "For", "``", "Sidebar", "''", "50", "%", "responded", "1", ",", "10", "%", "for", "2", ",", "10", "%", "for", "3", ",", "20", "%", "for", "4", ",", "10", "%", "for", "5", ",", "and", "0", "%", "for", "6", "."]}, "caption": {"raw": "(b)", "tokens": ["(", "b", ")"]}, "context": {"raw": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy (b)", "tokens": ["Automatic", "Text", "Simplification", "Tools", "for", "Deaf", "and", "Hard", "of", "Hearing", "Adults", ":", "Benefits", "of", "Lexical", "Simplification", "and", "Providing", "Users", "with", "Autonomy", "(", "b", ")"]}, "filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_002.jpg", "orig_filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "split": "train"}, {"article_id": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment", "description": {"raw": "The localization error in meters per participant across three routes. All data is the following in a csv format.  participant,lower whisker,25th percentile,median,75th percentile,upper whisker,% of outliers,extremes,number of samples  P1,0.04,1.02,1.52,2.24,4.07,6.1%,48,787  P2,0.03,0.96,1.39,2.51,4.835,8.8%,66,751  P3,0.08,0.91,1.435,2.09,3.86,0.3%,2,644  P4,0.06,0.905,1.43,2.05,3.7675,2.6%,16,620  P5,0.03,0.95,1.42,2.03,3.65,2.2%,20,915  P6,0.02,0.76,1.15,1.59,2.835,6.4%,34,530  P7,0.03,0.81,1.29,1.93,3.61,1.0%,8,813  P8,0.12,1,1.53,2.305,4.2625,11.8%,115,971  P9,0.16,0.99,1.34,1.88,3.215,4.4%,36,810  P10,0.03,0.86,1.55,2.28,4.41,0.4%,3,800", "tokens": ["The", "localization", "error", "in", "meters", "per", "participant", "across", "three", "routes", ".", "All", "data", "is", "the", "following", "in", "a", "csv", "format", ".", "participant", ",", "lower", "whisker,25th", "percentile", ",", "median,75th", "percentile", ",", "upper", "whisker", ",", "%", "of", "outliers", ",", "extremes", ",", "number", "of", "samples", "P1,0.04,1.02,1.52,2.24,4.07,6.1", "%", ",48,787", "P2,0.03,0.96,1.39,2.51,4.835,8.8", "%", ",66,751", "P3,0.08,0.91,1.435,2.09,3.86,0.3", "%", ",2,644", "P4,0.06,0.905,1.43,2.05,3.7675,2.6", "%", ",16,620", "P5,0.03,0.95,1.42,2.03,3.65,2.2", "%", ",20,915", "P6,0.02,0.76,1.15,1.59,2.835,6.4", "%", ",34,530", "P7,0.03,0.81,1.29,1.93,3.61,1.0", "%", ",8,813", "P8,0.12,1,1.53,2.305,4.2625,11.8", "%", ",115,971", "P9,0.16,0.99,1.34,1.88,3.215,4.4", "%", ",36,810", "P10,0.03,0.86,1.55,2.28,4.41,0.4", "%", ",3,800"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment ", "tokens": ["NavCog3", ":", "An", "Evaluation", "of", "a", "Smartphone-Based", "Blind", "Indoor", "Navigation", "Assistant", "with", "Semantic", "Features", "in", "a", "Large-Scale", "Environment"]}, "filename": "b535ecced835fbcfffaa15a78abfa49f44709daa_Image_011.jpg", "orig_filename": "b535ecced835fbcfffaa15a78abfa49f44709daa", "split": "train"}, {"article_id": "University of Dundee Enabling designers to foresee which colors users cannot", "description": {"raw": "Histogram plot of mean discrimination volumes (bin size = 250) for our participants. Rises sharply from 0-250 (over 500 participants) to a peak at 500-750 (over 3000 participants). Histogram then falls off with a very long tail. Minimum volume is 21.68, 25th quartile is at volume 804.62, 50th quartile (median) is at volume 1558.38, 75th quartile is at volume 3223.60, maximum volume is 1058397.75. Histogram is cut off above volumes of 12000.", "tokens": ["Histogram", "plot", "of", "mean", "discrimination", "volumes", "(", "bin", "size", "=", "250", ")", "for", "our", "participants", ".", "Rises", "sharply", "from", "0-250", "(", "over", "500", "participants", ")", "to", "a", "peak", "at", "500-750", "(", "over", "3000", "participants", ")", ".", "Histogram", "then", "falls", "off", "with", "a", "very", "long", "tail", ".", "Minimum", "volume", "is", "21.68", ",", "25th", "quartile", "is", "at", "volume", "804.62", ",", "50th", "quartile", "(", "median", ")", "is", "at", "volume", "1558.38", ",", "75th", "quartile", "is", "at", "volume", "3223.60", ",", "maximum", "volume", "is", "1058397.75", ".", "Histogram", "is", "cut", "off", "above", "volumes", "of", "12000", "."]}, "caption": {"raw": "Figure 4: Histogram of number of participants versus mean discrimination ellipsoid volumes. Participants with ellipsoid volumes above 12,000 are not shown for space reasons.", "tokens": ["Figure", "4", ":", "Histogram", "of", "number", "of", "participants", "versus", "mean", "discrimination", "ellipsoid", "volumes", ".", "Participants", "with", "ellipsoid", "volumes", "above", "12,000", "are", "not", "shown", "for", "space", "reasons", "."]}, "context": {"raw": "University of Dundee Enabling designers to foresee which colors users cannot Figure 4: Histogram of number of participants versus mean discrimination ellipsoid volumes. Participants with ellipsoid volumes above 12,000 are not shown for space reasons.", "tokens": ["University", "of", "Dundee", "Enabling", "designers", "to", "foresee", "which", "colors", "users", "can", "not", "Figure", "4", ":", "Histogram", "of", "number", "of", "participants", "versus", "mean", "discrimination", "ellipsoid", "volumes", ".", "Participants", "with", "ellipsoid", "volumes", "above", "12,000", "are", "not", "shown", "for", "space", "reasons", "."]}, "filename": "0f06c57940475cfc741de29099847b6aa66c8bc0_Image_007.jpg", "orig_filename": "0f06c57940475cfc741de29099847b6aa66c8bc0", "split": "train"}, {"article_id": "Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users", "description": {"raw": "Two grouped bar charts displaying % of Accounted Variance (Adjusted squared R):\nComprehension Model 1: Demographic at 25.6%\nComprehension Model 2: Demographic and Technology at 38.2%\nSignificance code between the comprehension models: **\nSubjective Model 1: Demographic at 15.3%\nSubjective Model 2: Demographic and Technology at 33.5%\nSignificance code between the subjective models: ***", "tokens": ["Two", "grouped", "bar", "charts", "displaying", "%", "of", "Accounted", "Variance", "(", "Adjusted", "squared", "R", ")", ":", "Comprehension", "Model", "1", ":", "Demographic", "at", "25.6", "%", "Comprehension", "Model", "2", ":", "Demographic", "and", "Technology", "at", "38.2", "%", "Significance", "code", "between", "the", "comprehension", "models", ":", "*", "*", "Subjective", "Model", "1", ":", "Demographic", "at", "15.3", "%", "Subjective", "Model", "2", ":", "Demographic", "and", "Technology", "at", "33.5", "%", "Significance", "code", "between", "the", "subjective", "models", ":", "*", "*", "*"]}, "caption": {"raw": "Figure 2: Regression model comparison summary. (Significance codes: 0 ‘***’ 0.001 ‘**’ 0.01)", "tokens": ["Figure", "2", ":", "Regression", "model", "comparison", "summary", ".", "(", "Significance", "codes", ":", "0", "‘", "*", "*", "*", "’", "0.001", "‘", "*", "*", "’", "0.01", ")"]}, "context": {"raw": "Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users Figure 2: Regression model comparison summary. (Significance codes: 0 ‘***’ 0.001 ‘**’ 0.01)", "tokens": ["Demographic", "and", "Experiential", "Factors", "Influencing", "Acceptance", "of", "Sign", "Language", "Animation", "by", "Deaf", "Users", "Figure", "2", ":", "Regression", "model", "comparison", "summary", ".", "(", "Significance", "codes", ":", "0", "‘", "*", "*", "*", "’", "0.001", "‘", "*", "*", "’", "0.01", ")"]}, "filename": "e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab_Image_006.jpg", "orig_filename": "e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab", "split": "train"}, {"article_id": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics", "description": {"raw": "Figure 7a. Time on task Boxplot with task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).   Figure 7b. Excess motion: Total trial vs last 1 second while locking on target. I/O Braid had more excess motion compared to Buttons and Scroll. The boxplots show median values with quartiles and min/max extent.   Figure 7c. Weighted average subjective feedback.  We mapped the 7-point Likert scale to a score in the range [-3, 3] for Ease of Use, Perceived Accuracy and Tactile Feel. We multiplied the score by the number of times the technique received that rating and computed an average for all the scores. The chart show favorable scores for I/O Braid and Scroll, whereas Buttons was the least popular.", "tokens": ["Figure", "7a", ".", "Time", "on", "task", "Boxplot", "with", "task", "completion", "times", "for", "the", "three", "input", "devices", ".", "I/O", "Braid", "was", "faster", "than", "Buttons", "(", "statistical", "significance", ")", ".", "Figure", "7b", ".", "Excess", "motion", ":", "Total", "trial", "vs", "last", "1", "second", "while", "locking", "on", "target", ".", "I/O", "Braid", "had", "more", "excess", "motion", "compared", "to", "Buttons", "and", "Scroll", ".", "The", "boxplots", "show", "median", "values", "with", "quartiles", "and", "min/max", "extent", ".", "Figure", "7c", ".", "Weighted", "average", "subjective", "feedback", ".", "We", "mapped", "the", "7-point", "Likert", "scale", "to", "a", "score", "in", "the", "range", "[", "-3", ",", "3", "]", "for", "Ease", "of", "Use", ",", "Perceived", "Accuracy", "and", "Tactile", "Feel", ".", "We", "multiplied", "the", "score", "by", "the", "number", "of", "times", "the", "technique", "received", "that", "rating", "and", "computed", "an", "average", "for", "all", "the", "scores", ".", "The", "chart", "show", "favorable", "scores", "for", "I/O", "Braid", "and", "Scroll", ",", "whereas", "Buttons", "was", "the", "least", "popular", "."]}, "caption": {"raw": "Figure 7. a) Task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).", "tokens": ["Figure", "7.", "a", ")", "Task", "completion", "times", "for", "the", "three", "input", "devices", ".", "I/O", "Braid", "was", "faster", "than", "Buttons", "(", "statistical", "significance", ")", "."]}, "context": {"raw": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics Figure 7. a) Task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).", "tokens": ["E-Textile", "Microinteractions", ":", "Augmenting", "Twist", "with", "Flick", ",", "Slide", "and", "Grasp", "Gestures", "for", "Soft", "Electronics", "Figure", "7.", "a", ")", "Task", "completion", "times", "for", "the", "three", "input", "devices", ".", "I/O", "Braid", "was", "faster", "than", "Buttons", "(", "statistical", "significance", ")", "."]}, "filename": "f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_011.jpg", "orig_filename": "f06a381ef1ff7d02a6a1876548e83db20e71442f", "split": "train"}, {"article_id": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "description": {"raw": "This figure contains two line graphs, a and b.  The a line graph represents the vision testing and training accuracy of the network over 80 epochs.  The training data rises consistently, but the testing accuracy maxes out at around 25 epochs.  In graph b, the line graph shows the training and testing accuracy of the bag of words model.  The training accuracy rises consistently, but the testing accuracy maxes out at round 45% at epoch 30 and slightly falls afterward.", "tokens": ["This", "figure", "contains", "two", "line", "graphs", ",", "a", "and", "b", ".", "The", "a", "line", "graph", "represents", "the", "vision", "testing", "and", "training", "accuracy", "of", "the", "network", "over", "80", "epochs", ".", "The", "training", "data", "rises", "consistently", ",", "but", "the", "testing", "accuracy", "maxes", "out", "at", "around", "25", "epochs", ".", "In", "graph", "b", ",", "the", "line", "graph", "shows", "the", "training", "and", "testing", "accuracy", "of", "the", "bag", "of", "words", "model", ".", "The", "training", "accuracy", "rises", "consistently", ",", "but", "the", "testing", "accuracy", "maxes", "out", "at", "round", "45", "%", "at", "epoch", "30", "and", "slightly", "falls", "afterward", "."]}, "caption": {"raw": "Vision CNN Accuracy          (b) Bag of WordsAccuracyFigure 5. Training accuracy of the neural network using (a) just the vision part of the network, and (b) just the text part of the network. The maximum accuracy of the vision network is 69.67% obtained after 30 epochs. The maximum of the text network is 45% obtain after 32 epochs. Even though the training accuracy continues to trend upwards, the network begins to overfit to the training data as the network fails to generalize and improve on the testing data.Multimodal Accuracy", "tokens": ["Vision", "CNN", "Accuracy", "(", "b", ")", "Bag", "of", "WordsAccuracyFigure", "5", ".", "Training", "accuracy", "of", "the", "neural", "network", "using", "(", "a", ")", "just", "the", "vision", "part", "of", "the", "network", ",", "and", "(", "b", ")", "just", "the", "text", "part", "of", "the", "network", ".", "The", "maximum", "accuracy", "of", "the", "vision", "network", "is", "69.67", "%", "obtained", "after", "30", "epochs", ".", "The", "maximum", "of", "the", "text", "network", "is", "45", "%", "obtain", "after", "32", "epochs", ".", "Even", "though", "the", "training", "accuracy", "continues", "to", "trend", "upwards", ",", "the", "network", "begins", "to", "overfit", "to", "the", "training", "data", "as", "the", "network", "fails", "to", "generalize", "and", "improve", "on", "the", "testing", "data.Multimodal", "Accuracy"]}, "context": {"raw": "Multimodal Deep Learning using Images and Text for Information Graphic Classification Vision CNN Accuracy          (b) Bag of WordsAccuracyFigure 5. Training accuracy of the neural network using (a) just the vision part of the network, and (b) just the text part of the network. The maximum accuracy of the vision network is 69.67% obtained after 30 epochs. The maximum of the text network is 45% obtain after 32 epochs. Even though the training accuracy continues to trend upwards, the network begins to overfit to the training data as the network fails to generalize and improve on the testing data.Multimodal Accuracy", "tokens": ["Multimodal", "Deep", "Learning", "using", "Images", "and", "Text", "for", "Information", "Graphic", "Classification", "Vision", "CNN", "Accuracy", "(", "b", ")", "Bag", "of", "WordsAccuracyFigure", "5", ".", "Training", "accuracy", "of", "the", "neural", "network", "using", "(", "a", ")", "just", "the", "vision", "part", "of", "the", "network", ",", "and", "(", "b", ")", "just", "the", "text", "part", "of", "the", "network", ".", "The", "maximum", "accuracy", "of", "the", "vision", "network", "is", "69.67", "%", "obtained", "after", "30", "epochs", ".", "The", "maximum", "of", "the", "text", "network", "is", "45", "%", "obtain", "after", "32", "epochs", ".", "Even", "though", "the", "training", "accuracy", "continues", "to", "trend", "upwards", ",", "the", "network", "begins", "to", "overfit", "to", "the", "training", "data", "as", "the", "network", "fails", "to", "generalize", "and", "improve", "on", "the", "testing", "data.Multimodal", "Accuracy"]}, "filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_006.jpg", "orig_filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "split": "train"}, {"article_id": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "description": {"raw": "Transfer scores in a bar chart. For both user groups, differences between conditons are small and non-significant.", "tokens": ["Transfer", "scores", "in", "a", "bar", "chart", ".", "For", "both", "user", "groups", ",", "differences", "between", "conditons", "are", "small", "and", "non-significant", "."]}, "caption": {"raw": "Figure 3. Mean retention score for both user groups and experimental conditions.", "tokens": ["Figure", "3", ".", "Mean", "retention", "score", "for", "both", "user", "groups", "and", "experimental", "conditions", "."]}, "context": {"raw": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students Figure 3. Mean retention score for both user groups and experimental conditions.", "tokens": ["SlidePacer", ":", "A", "Presentation", "Delivery", "Tool", "for", "Instructors", "of", "Deaf", "and", "Hard", "of", "Hearing", "Students", "Figure", "3", ".", "Mean", "retention", "score", "for", "both", "user", "groups", "and", "experimental", "conditions", "."]}, "filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_007.png", "orig_filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "split": "train"}, {"article_id": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "description": {"raw": "A vertical bar graph of participants' preferences of the markup styles. The x axis is the 4 markup styles: No Change, Yellow, Italics, and Underline. The y axis is numeric, ranging from 0 to 0.8. The prominent feature is No Change being the most preferred with 0.65. Other styles ranged from 0.3 to 0.45 as their preference with Italics being ahead of Yellow and Underline.", "tokens": ["A", "vertical", "bar", "graph", "of", "participants", "'", "preferences", "of", "the", "markup", "styles", ".", "The", "x", "axis", "is", "the", "4", "markup", "styles", ":", "No", "Change", ",", "Yellow", ",", "Italics", ",", "and", "Underline", ".", "The", "y", "axis", "is", "numeric", ",", "ranging", "from", "0", "to", "0.8", ".", "The", "prominent", "feature", "is", "No", "Change", "being", "the", "most", "preferred", "with", "0.65", ".", "Other", "styles", "ranged", "from", "0.3", "to", "0.45", "as", "their", "preference", "with", "Italics", "being", "ahead", "of", "Yellow", "and", "Underline", "."]}, "caption": {"raw": "Figure 6. Larger Study: Preference Responses (binary)", "tokens": ["Figure", "6", ".", "Larger", "Study", ":", "Preference", "Responses", "(", "binary", ")"]}, "context": {"raw": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings Figure 6. Larger Study: Preference Responses (binary)", "tokens": ["Deaf", "and", "Hard-of-Hearing", "Perspectives", "on", "Imperfect", "Automatic", "Speech", "Recognition", "for", "Captioning", "One-on-One", "Meetings", "Figure", "6", ".", "Larger", "Study", ":", "Preference", "Responses", "(", "binary", ")"]}, "filename": "471f9168db0fcb72d394222491966b97c098b1cd_Image_016.jpg", "orig_filename": "471f9168db0fcb72d394222491966b97c098b1cd", "split": "train"}, {"article_id": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "description": {"raw": "A bar graph of participants' preferences of the markup styles. The x axis is the 12 markup styles: no_change, bold_c, bold_u, color_c, color_u, del_u, it_u, r_gray, r_size, size_u, ul_u, and ul_gray_u. The y axis is numeric, ranging from 0 to 1. The prominent feature is it_u being the most preferred with 0.5 and del_u having zero preference. Other styles ranged from 0.1 to 0.25 as their preference.", "tokens": ["A", "bar", "graph", "of", "participants", "'", "preferences", "of", "the", "markup", "styles", ".", "The", "x", "axis", "is", "the", "12", "markup", "styles", ":", "no_change", ",", "bold_c", ",", "bold_u", ",", "color_c", ",", "color_u", ",", "del_u", ",", "it_u", ",", "r_gray", ",", "r_size", ",", "size_u", ",", "ul_u", ",", "and", "ul_gray_u", ".", "The", "y", "axis", "is", "numeric", ",", "ranging", "from", "0", "to", "1", ".", "The", "prominent", "feature", "is", "it_u", "being", "the", "most", "preferred", "with", "0.5", "and", "del_u", "having", "zero", "preference", ".", "Other", "styles", "ranged", "from", "0.1", "to", "0.25", "as", "their", "preference", "."]}, "caption": {"raw": "Figure 4. Pilot Study Preference Results", "tokens": ["Figure", "4", ".", "Pilot", "Study", "Preference", "Results"]}, "context": {"raw": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings Figure 4. Pilot Study Preference Results", "tokens": ["Deaf", "and", "Hard-of-Hearing", "Perspectives", "on", "Imperfect", "Automatic", "Speech", "Recognition", "for", "Captioning", "One-on-One", "Meetings", "Figure", "4", ".", "Pilot", "Study", "Preference", "Results"]}, "filename": "471f9168db0fcb72d394222491966b97c098b1cd_Image_014.jpg", "orig_filename": "471f9168db0fcb72d394222491966b97c098b1cd", "split": "train"}, {"article_id": "Considering Wake Gestures for Smart Assistant Use", "description": {"raw": "Bar chart of RSME results for respective gestures. From highest to lowest: Clap, Double Clap, Snap, Swipe and Wave. Error Bars showing standard error.", "tokens": ["Bar", "chart", "of", "RSME", "results", "for", "respective", "gestures", ".", "From", "highest", "to", "lowest", ":", "Clap", ",", "Double", "Clap", ",", "Snap", ",", "Swipe", "and", "Wave", ".", "Error", "Bars", "showing", "standard", "error", "."]}, "caption": {"raw": "Figure 2: RSME measurements for the respective gestures. Error bars show standard error.", "tokens": ["Figure", "2", ":", "RSME", "measurements", "for", "the", "respective", "gestures", ".", "Error", "bars", "show", "standard", "error", "."]}, "context": {"raw": "Considering Wake Gestures for Smart Assistant Use Figure 2: RSME measurements for the respective gestures. Error bars show standard error.", "tokens": ["Considering", "Wake", "Gestures", "for", "Smart", "Assistant", "Use", "Figure", "2", ":", "RSME", "measurements", "for", "the", "respective", "gestures", ".", "Error", "bars", "show", "standard", "error", "."]}, "filename": "ccbb556035dcb992b4b966f03ce8a59b20c064aa_Image_004.png", "orig_filename": "ccbb556035dcb992b4b966f03ce8a59b20c064aa", "split": "train"}, {"article_id": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences", "description": {"raw": "The Benchmark data feature provides box plot for each construct. Age, genre, gender and game based filters can be altered to analyse the data in customized way.", "tokens": ["The", "Benchmark", "data", "feature", "provides", "box", "plot", "for", "each", "construct", ".", "Age", ",", "genre", ",", "gender", "and", "game", "based", "filters", "can", "be", "altered", "to", "analyse", "the", "data", "in", "customized", "way", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences ", "tokens": ["The", "Player", "Experience", "Inventory", "Bench", ":", "Providing", "Games", "User", "Researchers", "Actionable", "Insight", "into", "Player", "Experiences"]}, "filename": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef_Image_005.jpg", "orig_filename": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef", "split": "train"}, {"article_id": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults", "description": {"raw": "Description: image of a chart that has manual adaptation and no notification selected", "tokens": ["Description", ":", "image", "of", "a", "chart", "that", "has", "manual", "adaptation", "and", "no", "notification", "selected"]}, "caption": {"raw": "Participants: YA2, YA16, OA1, OA3, OA4", "tokens": ["Participants", ":", "YA2", ",", "YA16", ",", "OA1", ",", "OA3", ",", "OA4"]}, "context": {"raw": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults Participants: YA2, YA16, OA1, OA3, OA4", "tokens": ["Understanding", "design", "considerations", "for", "adaptive", "user", "interfaces", "for", "accessible", "pointing", "with", "older", "and", "younger", "adults", "Participants", ":", "YA2", ",", "YA16", ",", "OA1", ",", "OA3", ",", "OA4"]}, "filename": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9_Image_009.png", "orig_filename": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9", "split": "train"}, {"article_id": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals", "description": {"raw": "This figure shows a stacked bar plot of percentages of a time-spent scale with 6 points. The y-axis shows the percentage of participants. The x-axis has X categories \"On-screen\" and \"Paper\".  The following scale responses are shown from bottom to top: \"> 60 minutes\", \"30-60 minutes\", \"15-29 minutes\", \"10-15 minutes\", \"5-9 minutes\", \"1-4 minutes\", \"None at all\".  For \"On-screen\",  47% responded \"> 60 minutes\", 31 % responded \"30-60 minutes\",  6% responded \"15-29 minutes\",  6% responded \"10-15 minutes\",  6% responded \"5-9 minutes\", 3% responded \"1-4 minutes\".  For \"Paper\",  31% responded \"> 60 minutes\",  16% responded \"30-60 minutes\",  12.5% responded \"15-29 minutes\",  31% responded \"10-15 minutes\",  12.5% responded \"5-9 minutes\",  3% responded \"1-4 minutes\",  22% responded \"None at all\".", "tokens": ["This", "figure", "shows", "a", "stacked", "bar", "plot", "of", "percentages", "of", "a", "time-spent", "scale", "with", "6", "points", ".", "The", "y-axis", "shows", "the", "percentage", "of", "participants", ".", "The", "x-axis", "has", "X", "categories", "``", "On-screen", "''", "and", "``", "Paper", "''", ".", "The", "following", "scale", "responses", "are", "shown", "from", "bottom", "to", "top", ":", "``", ">", "60", "minutes", "''", ",", "``", "30-60", "minutes", "''", ",", "``", "15-29", "minutes", "''", ",", "``", "10-15", "minutes", "''", ",", "``", "5-9", "minutes", "''", ",", "``", "1-4", "minutes", "''", ",", "``", "None", "at", "all", "''", ".", "For", "``", "On-screen", "''", ",", "47", "%", "responded", "``", ">", "60", "minutes", "''", ",", "31", "%", "responded", "``", "30-60", "minutes", "''", ",", "6", "%", "responded", "``", "15-29", "minutes", "''", ",", "6", "%", "responded", "``", "10-15", "minutes", "''", ",", "6", "%", "responded", "``", "5-9", "minutes", "''", ",", "3", "%", "responded", "``", "1-4", "minutes", "''", ".", "For", "``", "Paper", "''", ",", "31", "%", "responded", "``", ">", "60", "minutes", "''", ",", "16", "%", "responded", "``", "30-60", "minutes", "''", ",", "12.5", "%", "responded", "``", "15-29", "minutes", "''", ",", "31", "%", "responded", "``", "10-15", "minutes", "''", ",", "12.5", "%", "responded", "``", "5-9", "minutes", "''", ",", "3", "%", "responded", "``", "1-4", "minutes", "''", ",", "22", "%", "responded", "``", "None", "at", "all", "''", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Reading Experiences and Interest in Reading-Assistance Tools Among Deaf and Hard-of-Hearing Computing Professionals ", "tokens": ["Reading", "Experiences", "and", "Interest", "in", "Reading-Assistance", "Tools", "Among", "Deaf", "and", "Hard-of-Hearing", "Computing", "Professionals"]}, "filename": "b61e1747b1fef3eedde7aa87f26338f61d52ce93_Image_004.jpg", "orig_filename": "b61e1747b1fef3eedde7aa87f26338f61d52ce93", "split": "train"}, {"article_id": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "description": {"raw": "A scatter plot making comet like images for each cardinal direction tracked. There's one dot for each trial. The nucleus of the comet is where the controller was pointing when the user clicked, the tail is where the HMD was looking when the user clicked", "tokens": ["A", "scatter", "plot", "making", "comet", "like", "images", "for", "each", "cardinal", "direction", "tracked", ".", "There", "'s", "one", "dot", "for", "each", "trial", ".", "The", "nucleus", "of", "the", "comet", "is", "where", "the", "controller", "was", "pointing", "when", "the", "user", "clicked", ",", "the", "tail", "is", "where", "the", "HMD", "was", "looking", "when", "the", "user", "clicked"]}, "caption": {"raw": "Figure 9. The HMD and Controller angular movements, with respect to the Angular Distance to the target. Error bars represent standard error.", "tokens": ["Figure", "9", ".", "The", "HMD", "and", "Controller", "angular", "movements", ",", "with", "respect", "to", "the", "Angular", "Distance", "to", "the", "target", ".", "Error", "bars", "represent", "standard", "error", "."]}, "context": {"raw": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR Figure 9. The HMD and Controller angular movements, with respect to the Angular Distance to the target. Error bars represent standard error.", "tokens": ["Head-Coupled", "Kinematic", "Template", "Matching", ":", "A", "Prediction", "Model", "for", "Ray", "Pointing", "in", "VR", "Figure", "9", ".", "The", "HMD", "and", "Controller", "angular", "movements", ",", "with", "respect", "to", "the", "Angular", "Distance", "to", "the", "target", ".", "Error", "bars", "represent", "standard", "error", "."]}, "filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_011.png", "orig_filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "split": "train"}, {"article_id": "No Touch Pig!: Investigating Child-Parent Use of a System for Training Executive Function", "description": {"raw": "Line graph comparing the standardized MEFS scores between children who played CMC and children who played DT from baseline to the short-term post-test. From baseline to short-term post-test, the CMC line goes up from about 0.4 to 0.5. From baseline to short-term post-test, the DT line goes slightly down from 0.2 to about -0.5.", "tokens": ["Line", "graph", "comparing", "the", "standardized", "MEFS", "scores", "between", "children", "who", "played", "CMC", "and", "children", "who", "played", "DT", "from", "baseline", "to", "the", "short-term", "post-test", ".", "From", "baseline", "to", "short-term", "post-test", ",", "the", "CMC", "line", "goes", "up", "from", "about", "0.4", "to", "0.5", ".", "From", "baseline", "to", "short-term", "post-test", ",", "the", "DT", "line", "goes", "slightly", "down", "from", "0.2", "to", "about", "-0.5", "."]}, "caption": {"raw": "Mother: The pig. C26:    I will!", "tokens": ["Mother", ":", "The", "pig", ".", "C26", ":", "I", "will", "!"]}, "context": {"raw": "No Touch Pig!: Investigating Child-Parent Use of a System for Training Executive Function Mother: The pig. C26:    I will!", "tokens": ["No", "Touch", "Pig", "!", ":", "Investigating", "Child-Parent", "Use", "of", "a", "System", "for", "Training", "Executive", "Function", "Mother", ":", "The", "pig", ".", "C26", ":", "I", "will", "!"]}, "filename": "beec9b27e310c14ccf0235582c11ce4aa9e4214c_Image_006.jpg", "orig_filename": "beec9b27e310c14ccf0235582c11ce4aa9e4214c", "split": "train"}, {"article_id": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "description": {"raw": "A stacked bar graph titled Figure 4: H1-c for ASR Did a Good Job (Likert). The figure is split into three facets (subgraphs), from left to right: WRAT-L, WRAT-M, and WRAT-H. The vertical axis ranges from 0% to 100%, with ticks at 10% increments. On the horizontal axis are the labels for the WER accuracy levels in the study: Desktop, Cloud, and Human. The bar graph displays the percentage of answers for each choice: Strongly Disagree, Disagree, Neither agree nor disagree, Agree, Strongly Agree. In the WRAT-L facet, all pairwise comparison lines are not significant and the overall percentages are: 15% Strongly Disagree, 15% Disagree, 25% Neither agree nor disagree, 25% Agree, 10% Strongly Agree. In the WRAT-M facet, the Desktop-Cloud pairwise comparison was significant with two stars, the Desktop-Human pairwise comparison was significant with three stars, and the overall percentages are: 15% Strongly Disagree, 15% Disagree, 15% Neither agree nor disagree, 15% Agree, 25% Strongly Agree. In the WRAT-H facet, both Desktop-Cloud and Desktop-Human had significant pairwise comparisons with three stars, and the overall percentages are: 25% Strongly Disagree, 20% Disagree, 10% Neither agree nor disagree, 10% Agree, 20% Strongly Agree.", "tokens": ["A", "stacked", "bar", "graph", "titled", "Figure", "4", ":", "H1-c", "for", "ASR", "Did", "a", "Good", "Job", "(", "Likert", ")", ".", "The", "figure", "is", "split", "into", "three", "facets", "(", "subgraphs", ")", ",", "from", "left", "to", "right", ":", "WRAT-L", ",", "WRAT-M", ",", "and", "WRAT-H", ".", "The", "vertical", "axis", "ranges", "from", "0", "%", "to", "100", "%", ",", "with", "ticks", "at", "10", "%", "increments", ".", "On", "the", "horizontal", "axis", "are", "the", "labels", "for", "the", "WER", "accuracy", "levels", "in", "the", "study", ":", "Desktop", ",", "Cloud", ",", "and", "Human", ".", "The", "bar", "graph", "displays", "the", "percentage", "of", "answers", "for", "each", "choice", ":", "Strongly", "Disagree", ",", "Disagree", ",", "Neither", "agree", "nor", "disagree", ",", "Agree", ",", "Strongly", "Agree", ".", "In", "the", "WRAT-L", "facet", ",", "all", "pairwise", "comparison", "lines", "are", "not", "significant", "and", "the", "overall", "percentages", "are", ":", "15", "%", "Strongly", "Disagree", ",", "15", "%", "Disagree", ",", "25", "%", "Neither", "agree", "nor", "disagree", ",", "25", "%", "Agree", ",", "10", "%", "Strongly", "Agree", ".", "In", "the", "WRAT-M", "facet", ",", "the", "Desktop-Cloud", "pairwise", "comparison", "was", "significant", "with", "two", "stars", ",", "the", "Desktop-Human", "pairwise", "comparison", "was", "significant", "with", "three", "stars", ",", "and", "the", "overall", "percentages", "are", ":", "15", "%", "Strongly", "Disagree", ",", "15", "%", "Disagree", ",", "15", "%", "Neither", "agree", "nor", "disagree", ",", "15", "%", "Agree", ",", "25", "%", "Strongly", "Agree", ".", "In", "the", "WRAT-H", "facet", ",", "both", "Desktop-Cloud", "and", "Desktop-Human", "had", "significant", "pairwise", "comparisons", "with", "three", "stars", ",", "and", "the", "overall", "percentages", "are", ":", "25", "%", "Strongly", "Disagree", ",", "20", "%", "Disagree", ",", "10", "%", "Neither", "agree", "nor", "disagree", ",", "10", "%", "Agree", ",", "20", "%", "Strongly", "Agree", "."]}, "caption": {"raw": "Figure 4. H1-c for ASR Did a Good Job (Likert)", "tokens": ["Figure", "4", ".", "H1-c", "for", "ASR", "Did", "a", "Good", "Job", "(", "Likert", ")"]}, "context": {"raw": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels Figure 4. H1-c for ASR Did a Good Job (Likert)", "tokens": ["Methods", "for", "Evaluation", "of", "Imperfect", "Captioning", "Tools", "by", "Deaf", "or", "Hard-of-Hearing", "Users", "at", "Different", "Reading", "Literacy", "Levels", "Figure", "4", ".", "H1-c", "for", "ASR", "Did", "a", "Good", "Job", "(", "Likert", ")"]}, "filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_004.jpg", "orig_filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "split": "train"}, {"article_id": "Diagramming Working Field Theories for Design in the HCI Classroom", "description": {"raw": "Figure 2: Diagram that represents the concepts that surround and inspire Field Theories. Field Theory is the central concept. Field theories synthesize field research that is inspired in technology examples, and that is analyzed using sensitizing concepts. Field theories are iterative: field theories are used to ideate prototype designs, the designs are tested in field trials, and data from field trials is used to refine the field theory.", "tokens": ["Figure", "2", ":", "Diagram", "that", "represents", "the", "concepts", "that", "surround", "and", "inspire", "Field", "Theories", ".", "Field", "Theory", "is", "the", "central", "concept", ".", "Field", "theories", "synthesize", "field", "research", "that", "is", "inspired", "in", "technology", "examples", ",", "and", "that", "is", "analyzed", "using", "sensitizing", "concepts", ".", "Field", "theories", "are", "iterative", ":", "field", "theories", "are", "used", "to", "ideate", "prototype", "designs", ",", "the", "designs", "are", "tested", "in", "field", "trials", ",", "and", "data", "from", "field", "trials", "is", "used", "to", "refine", "the", "field", "theory", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Diagramming Working Field Theories for Design in the HCI Classroom ", "tokens": ["Diagramming", "Working", "Field", "Theories", "for", "Design", "in", "the", "HCI", "Classroom"]}, "filename": "680984459a2ab9dcaed587a6e5775a25ce140804_Image_003.jpg", "orig_filename": "680984459a2ab9dcaed587a6e5775a25ce140804", "split": "train"}, {"article_id": "Investigating the feasibility of extracting tool demonstrations from in-situ video content", "description": {"raw": "Architecture for remotely gathering synchronized screen recording and log data. A web server running on Amazon S3/EC2 serves a web-based image editor. As the user uses the image editor, log events are sent to a screen recording and logging utility installed on the user's computer. This utility uploads the recorded data back to us using Amazon web services.", "tokens": ["Architecture", "for", "remotely", "gathering", "synchronized", "screen", "recording", "and", "log", "data", ".", "A", "web", "server", "running", "on", "Amazon", "S3/EC2", "serves", "a", "web-based", "image", "editor", ".", "As", "the", "user", "uses", "the", "image", "editor", ",", "log", "events", "are", "sent", "to", "a", "screen", "recording", "and", "logging", "utility", "installed", "on", "the", "user", "'s", "computer", ".", "This", "utility", "uploads", "the", "recorded", "data", "back", "to", "us", "using", "Amazon", "web", "services", "."]}, "caption": {"raw": "Figure 1. Architecture for remotely gathering synchronized screen recording and log data.", "tokens": ["Figure", "1", ".", "Architecture", "for", "remotely", "gathering", "synchronized", "screen", "recording", "and", "log", "data", "."]}, "context": {"raw": "Investigating the feasibility of extracting tool demonstrations from in-situ video content Figure 1. Architecture for remotely gathering synchronized screen recording and log data.", "tokens": ["Investigating", "the", "feasibility", "of", "extracting", "tool", "demonstrations", "from", "in-situ", "video", "content", "Figure", "1", ".", "Architecture", "for", "remotely", "gathering", "synchronized", "screen", "recording", "and", "log", "data", "."]}, "filename": "9c3dff78167c8fa4d89107bcf0921c6f46ddb569_Image_001.jpg", "orig_filename": "9c3dff78167c8fa4d89107bcf0921c6f46ddb569", "split": "train"}, {"article_id": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "description": {"raw": "The main board is a large rectangular board that includes an arduino board (bottom) and circuitry for data (left) and power (right) transfer. Card boards are smaller rectangular boards that can be attached to the main board. A card board includes twelve linear servo motors that each drive an acrylic pin. The display is assembled by attaching the card boards on the main board and fastening an acrylic cover on top to only reveal a rectangular area of pins. On the top left corner of the cover is an optical marker used to track the location of the display. Textured stickers are attached on pins located at the tips of the thumb, middle finger, and little finger of the non-dominant hand. The user participates in the experiment by placing the non-dominant hand on top of the pin display while seated. An optical marker is attached on the nail of the index finger of the dominant hand. Cameras for optical tracking are fixed above the display. The experimenter inspects the status of the pin display through the monitor screen.", "tokens": ["The", "main", "board", "is", "a", "large", "rectangular", "board", "that", "includes", "an", "arduino", "board", "(", "bottom", ")", "and", "circuitry", "for", "data", "(", "left", ")", "and", "power", "(", "right", ")", "transfer", ".", "Card", "boards", "are", "smaller", "rectangular", "boards", "that", "can", "be", "attached", "to", "the", "main", "board", ".", "A", "card", "board", "includes", "twelve", "linear", "servo", "motors", "that", "each", "drive", "an", "acrylic", "pin", ".", "The", "display", "is", "assembled", "by", "attaching", "the", "card", "boards", "on", "the", "main", "board", "and", "fastening", "an", "acrylic", "cover", "on", "top", "to", "only", "reveal", "a", "rectangular", "area", "of", "pins", ".", "On", "the", "top", "left", "corner", "of", "the", "cover", "is", "an", "optical", "marker", "used", "to", "track", "the", "location", "of", "the", "display", ".", "Textured", "stickers", "are", "attached", "on", "pins", "located", "at", "the", "tips", "of", "the", "thumb", ",", "middle", "finger", ",", "and", "little", "finger", "of", "the", "non-dominant", "hand", ".", "The", "user", "participates", "in", "the", "experiment", "by", "placing", "the", "non-dominant", "hand", "on", "top", "of", "the", "pin", "display", "while", "seated", ".", "An", "optical", "marker", "is", "attached", "on", "the", "nail", "of", "the", "index", "finger", "of", "the", "dominant", "hand", ".", "Cameras", "for", "optical", "tracking", "are", "fixed", "above", "the", "display", ".", "The", "experimenter", "inspects", "the", "status", "of", "the", "pin", "display", "through", "the", "monitor", "screen", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects ", "tokens": ["ThroughHand", ":", "2D", "Tactile", "Interaction", "to", "Simultaneously", "Recognize", "and", "Touch", "Multiple", "Objects"]}, "filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_009.png", "orig_filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "split": "train"}, {"article_id": "Designing an Animated Character System for American Sign Language", "description": {"raw": "Figure 4: Materials participants reported wanting to read in ASL text. This figure presents a bar chart, with separate bars for DHH (light blue) and hearing (dark blue) populations. Y-axis is % participants, ranging from 0-70. X-axis is Material Desired in ASL Text. sorted by DHH popularity (most popular first): Website content, Printed content, Email, Texts/SMS, Video captions, Other, None.", "tokens": ["Figure", "4", ":", "Materials", "participants", "reported", "wanting", "to", "read", "in", "ASL", "text", ".", "This", "figure", "presents", "a", "bar", "chart", ",", "with", "separate", "bars", "for", "DHH", "(", "light", "blue", ")", "and", "hearing", "(", "dark", "blue", ")", "populations", ".", "Y-axis", "is", "%", "participants", ",", "ranging", "from", "0-70", ".", "X-axis", "is", "Material", "Desired", "in", "ASL", "Text", ".", "sorted", "by", "DHH", "popularity", "(", "most", "popular", "first", ")", ":", "Website", "content", ",", "Printed", "content", ",", "Email", ",", "Texts/SMS", ",", "Video", "captions", ",", "Other", ",", "None", "."]}, "caption": {"raw": "question provided fve multiple-choice answer options resem- bling the sign in question, ordered randomly. We selected featurally similar answer choices through Latent Semantic Analysis over a dataset of crowdsourced feature evaluations [8]. Each multiple-choice option was represented as an En- glish word, with a link to a signed video from SigningSavvy [48], an online English-to-ASL dictionary.", "tokens": ["question", "provided", "fve", "multiple-choice", "answer", "options", "resem-", "bling", "the", "sign", "in", "question", ",", "ordered", "randomly", ".", "We", "selected", "featurally", "similar", "answer", "choices", "through", "Latent", "Semantic", "Analysis", "over", "a", "dataset", "of", "crowdsourced", "feature", "evaluations", "[", "8", "]", ".", "Each", "multiple-choice", "option", "was", "represented", "as", "an", "En-", "glish", "word", ",", "with", "a", "link", "to", "a", "signed", "video", "from", "SigningSavvy", "[", "48", "]", ",", "an", "online", "English-to-ASL", "dictionary", "."]}, "context": {"raw": "Designing an Animated Character System for American Sign Language question provided fve multiple-choice answer options resem- bling the sign in question, ordered randomly. We selected featurally similar answer choices through Latent Semantic Analysis over a dataset of crowdsourced feature evaluations [8]. Each multiple-choice option was represented as an En- glish word, with a link to a signed video from SigningSavvy [48], an online English-to-ASL dictionary.", "tokens": ["Designing", "an", "Animated", "Character", "System", "for", "American", "Sign", "Language", "question", "provided", "fve", "multiple-choice", "answer", "options", "resem-", "bling", "the", "sign", "in", "question", ",", "ordered", "randomly", ".", "We", "selected", "featurally", "similar", "answer", "choices", "through", "Latent", "Semantic", "Analysis", "over", "a", "dataset", "of", "crowdsourced", "feature", "evaluations", "[", "8", "]", ".", "Each", "multiple-choice", "option", "was", "represented", "as", "an", "En-", "glish", "word", ",", "with", "a", "link", "to", "a", "signed", "video", "from", "SigningSavvy", "[", "48", "]", ",", "an", "online", "English-to-ASL", "dictionary", "."]}, "filename": "8209b931c94fea5d107e6ef2461b64e00fd52249_Image_006.jpg", "orig_filename": "8209b931c94fea5d107e6ef2461b64e00fd52249", "split": "train"}, {"article_id": "Reading and Learning Smartfonts", "description": {"raw": "All 2x3 characters presented as a 2D graph, where similar characters are closer together. The selected characters for Visibraille 2 are darker than those not used for Visibraille 2.", "tokens": ["All", "2x3", "characters", "presented", "as", "a", "2D", "graph", ",", "where", "similar", "characters", "are", "closer", "together", ".", "The", "selected", "characters", "for", "Visibraille", "2", "are", "darker", "than", "those", "not", "used", "for", "Visibraille", "2", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Reading and Learning Smartfonts ", "tokens": ["Reading", "and", "Learning", "Smartfonts"]}, "filename": "3da381501e7555b2f454c9e2f74830f40f90a5f1_Image_016.jpg", "orig_filename": "3da381501e7555b2f454c9e2f74830f40f90a5f1", "split": "train"}, {"article_id": "Exploring Text Revision with Backspace and Caret in Virtual Reality", "description": {"raw": "Bar chart with error bars presents the backspace time for four VR text revision techniques when dealing with different types of revision targets. Overall, using the character-level bacskapce requires more time to delete characters than the word-level backspace.", "tokens": ["Bar", "chart", "with", "error", "bars", "presents", "the", "backspace", "time", "for", "four", "VR", "text", "revision", "techniques", "when", "dealing", "with", "different", "types", "of", "revision", "targets", ".", "Overall", ",", "using", "the", "character-level", "bacskapce", "requires", "more", "time", "to", "delete", "characters", "than", "the", "word-level", "backspace", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Exploring Text Revision with Backspace and Caret in Virtual Reality ", "tokens": ["Exploring", "Text", "Revision", "with", "Backspace", "and", "Caret", "in", "Virtual", "Reality"]}, "filename": "5894fd4581a79a7067102891bc3db5738195941f_Image_008.jpg", "orig_filename": "5894fd4581a79a7067102891bc3db5738195941f", "split": "train"}, {"article_id": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning", "description": {"raw": "An empty radar chart with 10 axis: Outcome, Collaboration, Organization process, Classroom dynamics, Confidence, Behaviour, Motivation, Language, Skillfullness, and Thinking skills.", "tokens": ["An", "empty", "radar", "chart", "with", "10", "axis", ":", "Outcome", ",", "Collaboration", ",", "Organization", "process", ",", "Classroom", "dynamics", ",", "Confidence", ",", "Behaviour", ",", "Motivation", ",", "Language", ",", "Skillfullness", ",", "and", "Thinking", "skills", "."]}, "caption": {"raw": "Figure 3. First version radar chart; superset of all possible axes as inspired by Activity Theory’s activity triangle.", "tokens": ["Figure", "3", ".", "First", "version", "radar", "chart", ";", "superset", "of", "all", "possible", "axes", "as", "inspired", "by", "Activity", "Theory", "’", "s", "activity", "triangle", "."]}, "context": {"raw": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning Figure 3. First version radar chart; superset of all possible axes as inspired by Activity Theory’s activity triangle.", "tokens": ["Group", "Spinner", ":", "Recognizing", "&", "Visualizing", "Learning", "in", "the", "Classroom", "for", "Reflection", ",", "Communication", "&", "Planning", "Figure", "3", ".", "First", "version", "radar", "chart", ";", "superset", "of", "all", "possible", "axes", "as", "inspired", "by", "Activity", "Theory", "’", "s", "activity", "triangle", "."]}, "filename": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a_Image_008.gif", "orig_filename": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a", "split": "train"}, {"article_id": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards", "description": {"raw": "Keyboard focus is on Shape Fill. Below the menu ribbon is a blue triangle. Part of a PowerPoint menu ribbon is visible, and the top left corner of a white canvas with a blue triangle on it, with bounding box visible and rotated 90 degrees. The Shape Fill button is styled as one button and a focus border is around it, but the left side of it, a paint can icon with an orange bar underneath, is darker than the rest of it, indicating focus on that part of the button but not the other side, which has a triangle pointing down that indicates it is a dropdown menu.", "tokens": ["Keyboard", "focus", "is", "on", "Shape", "Fill", ".", "Below", "the", "menu", "ribbon", "is", "a", "blue", "triangle", ".", "Part", "of", "a", "PowerPoint", "menu", "ribbon", "is", "visible", ",", "and", "the", "top", "left", "corner", "of", "a", "white", "canvas", "with", "a", "blue", "triangle", "on", "it", ",", "with", "bounding", "box", "visible", "and", "rotated", "90", "degrees", ".", "The", "Shape", "Fill", "button", "is", "styled", "as", "one", "button", "and", "a", "focus", "border", "is", "around", "it", ",", "but", "the", "left", "side", "of", "it", ",", "a", "paint", "can", "icon", "with", "an", "orange", "bar", "underneath", ",", "is", "darker", "than", "the", "rest", "of", "it", ",", "indicating", "focus", "on", "that", "part", "of", "the", "button", "but", "not", "the", "other", "side", ",", "which", "has", "a", "triangle", "pointing", "down", "that", "indicates", "it", "is", "a", "dropdown", "menu", "."]}, "caption": {"raw": "P2 locates the Shape Fill button, which is actually two but- tons: the left one flls the selected shape with a default color and the right one opens a dropdown menu with more colors.The artboard’s state after P2 clicked the button, which flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle.The artboard’s state after P2 changed the artboard red. A key command she tried silently switched focus to the art- board and opened a “Format Background” task pane, which she used to change the color of the artboard’s background, leaving her with an orange triangle and a red artboard.Figure 12: P2’s artboard state as she attempts to change the color of the triangle in the upper left corner from blue to red, without any feedback indicating the color changes were successful or what objects the changes were applied to.he would “never dare present my PowerPoint presentation” without someone sighted looking at it frst.Accidental manipulations occurred several times during the task- based usability study due to the lack of feedback both for manipula- tions and object focus, as screen readers did not always announce when the keyboard focus changed from one object to another, orthe announcement was nested in a long string of announcements and was missed by the user. For example, when P2 attempted to change the color of the triangle in task #16 (referenced in Figure 1 and Figure 12), she selected the \"Shape Fill\" button, which flls the selected shape with a default color or can be opened as a dropdown menu to choose more colors (see Figure 12a). When she clicked the button, it flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle (see Figure 12b). P2 was confused and navigated back to the Shape Fill button and clicked it again, and it switched focus back to her triangle once more, prompting her to voice confusion and frustration: “What? Why won’t it do ... It won’t let me get into ...” She tried a key command, which silently switched focus to the artboard and opened a “Format Background” task pane with a dropdown fll button, which she used to select a shade of red that was applied to the artboard, leaving her with an orange triangle and a red artboard (see Figure 12c). After reading through the other formatting options in the task pane and noticing they referred to the “background,” she switched keyboard focus and found that it switched to the triangle, which she had thought she already had focus on. At that point, she decided to end the task, saying, “Uh, I tried. Did not know if it worked, or if I made the whole slide red. [laughs] I did what I could, I don’t know what else I can do.”", "tokens": ["P2", "locates", "the", "Shape", "Fill", "button", ",", "which", "is", "actually", "two", "but-", "tons", ":", "the", "left", "one", "flls", "the", "selected", "shape", "with", "a", "default", "color", "and", "the", "right", "one", "opens", "a", "dropdown", "menu", "with", "more", "colors.The", "artboard", "’", "s", "state", "after", "P2", "clicked", "the", "button", ",", "which", "flled", "her", "triangle", "with", "the", "default", "color", "(", "orange", ")", "without", "announcing", "the", "change", ",", "and", "switched", "focus", "from", "the", "menu", "back", "to", "her", "triangle.The", "artboard", "’", "s", "state", "after", "P2", "changed", "the", "artboard", "red", ".", "A", "key", "command", "she", "tried", "silently", "switched", "focus", "to", "the", "art-", "board", "and", "opened", "a", "“", "Format", "Background", "”", "task", "pane", ",", "which", "she", "used", "to", "change", "the", "color", "of", "the", "artboard", "’", "s", "background", ",", "leaving", "her", "with", "an", "orange", "triangle", "and", "a", "red", "artboard.Figure", "12", ":", "P2", "’", "s", "artboard", "state", "as", "she", "attempts", "to", "change", "the", "color", "of", "the", "triangle", "in", "the", "upper", "left", "corner", "from", "blue", "to", "red", ",", "without", "any", "feedback", "indicating", "the", "color", "changes", "were", "successful", "or", "what", "objects", "the", "changes", "were", "applied", "to.he", "would", "“", "never", "dare", "present", "my", "PowerPoint", "presentation", "”", "without", "someone", "sighted", "looking", "at", "it", "frst.Accidental", "manipulations", "occurred", "several", "times", "during", "the", "task-", "based", "usability", "study", "due", "to", "the", "lack", "of", "feedback", "both", "for", "manipula-", "tions", "and", "object", "focus", ",", "as", "screen", "readers", "did", "not", "always", "announce", "when", "the", "keyboard", "focus", "changed", "from", "one", "object", "to", "another", ",", "orthe", "announcement", "was", "nested", "in", "a", "long", "string", "of", "announcements", "and", "was", "missed", "by", "the", "user", ".", "For", "example", ",", "when", "P2", "attempted", "to", "change", "the", "color", "of", "the", "triangle", "in", "task", "#", "16", "(", "referenced", "in", "Figure", "1", "and", "Figure", "12", ")", ",", "she", "selected", "the", "``", "Shape", "Fill", "''", "button", ",", "which", "flls", "the", "selected", "shape", "with", "a", "default", "color", "or", "can", "be", "opened", "as", "a", "dropdown", "menu", "to", "choose", "more", "colors", "(", "see", "Figure", "12a", ")", ".", "When", "she", "clicked", "the", "button", ",", "it", "flled", "her", "triangle", "with", "the", "default", "color", "(", "orange", ")", "without", "announcing", "the", "change", ",", "and", "switched", "focus", "from", "the", "menu", "back", "to", "her", "triangle", "(", "see", "Figure", "12b", ")", ".", "P2", "was", "confused", "and", "navigated", "back", "to", "the", "Shape", "Fill", "button", "and", "clicked", "it", "again", ",", "and", "it", "switched", "focus", "back", "to", "her", "triangle", "once", "more", ",", "prompting", "her", "to", "voice", "confusion", "and", "frustration", ":", "“", "What", "?", "Why", "won", "’", "t", "it", "do", "...", "It", "won", "’", "t", "let", "me", "get", "into", "...", "”", "She", "tried", "a", "key", "command", ",", "which", "silently", "switched", "focus", "to", "the", "artboard", "and", "opened", "a", "“", "Format", "Background", "”", "task", "pane", "with", "a", "dropdown", "fll", "button", ",", "which", "she", "used", "to", "select", "a", "shade", "of", "red", "that", "was", "applied", "to", "the", "artboard", ",", "leaving", "her", "with", "an", "orange", "triangle", "and", "a", "red", "artboard", "(", "see", "Figure", "12c", ")", ".", "After", "reading", "through", "the", "other", "formatting", "options", "in", "the", "task", "pane", "and", "noticing", "they", "referred", "to", "the", "“", "background", ",", "”", "she", "switched", "keyboard", "focus", "and", "found", "that", "it", "switched", "to", "the", "triangle", ",", "which", "she", "had", "thought", "she", "already", "had", "focus", "on", ".", "At", "that", "point", ",", "she", "decided", "to", "end", "the", "task", ",", "saying", ",", "“", "Uh", ",", "I", "tried", ".", "Did", "not", "know", "if", "it", "worked", ",", "or", "if", "I", "made", "the", "whole", "slide", "red", ".", "[", "laughs", "]", "I", "did", "what", "I", "could", ",", "I", "don", "’", "t", "know", "what", "else", "I", "can", "do", ".", "”"]}, "context": {"raw": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards P2 locates the Shape Fill button, which is actually two but- tons: the left one flls the selected shape with a default color and the right one opens a dropdown menu with more colors.The artboard’s state after P2 clicked the button, which flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle.The artboard’s state after P2 changed the artboard red. A key command she tried silently switched focus to the art- board and opened a “Format Background” task pane, which she used to change the color of the artboard’s background, leaving her with an orange triangle and a red artboard.Figure 12: P2’s artboard state as she attempts to change the color of the triangle in the upper left corner from blue to red, without any feedback indicating the color changes were successful or what objects the changes were applied to.he would “never dare present my PowerPoint presentation” without someone sighted looking at it frst.Accidental manipulations occurred several times during the task- based usability study due to the lack of feedback both for manipula- tions and object focus, as screen readers did not always announce when the keyboard focus changed from one object to another, orthe announcement was nested in a long string of announcements and was missed by the user. For example, when P2 attempted to change the color of the triangle in task #16 (referenced in Figure 1 and Figure 12), she selected the \"Shape Fill\" button, which flls the selected shape with a default color or can be opened as a dropdown menu to choose more colors (see Figure 12a). When she clicked the button, it flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle (see Figure 12b). P2 was confused and navigated back to the Shape Fill button and clicked it again, and it switched focus back to her triangle once more, prompting her to voice confusion and frustration: “What? Why won’t it do ... It won’t let me get into ...” She tried a key command, which silently switched focus to the artboard and opened a “Format Background” task pane with a dropdown fll button, which she used to select a shade of red that was applied to the artboard, leaving her with an orange triangle and a red artboard (see Figure 12c). After reading through the other formatting options in the task pane and noticing they referred to the “background,” she switched keyboard focus and found that it switched to the triangle, which she had thought she already had focus on. At that point, she decided to end the task, saying, “Uh, I tried. Did not know if it worked, or if I made the whole slide red. [laughs] I did what I could, I don’t know what else I can do.”", "tokens": ["Understanding", "Blind", "Screen-Reader", "Users", "’", "Experiences", "of", "Digital", "Artboards", "P2", "locates", "the", "Shape", "Fill", "button", ",", "which", "is", "actually", "two", "but-", "tons", ":", "the", "left", "one", "flls", "the", "selected", "shape", "with", "a", "default", "color", "and", "the", "right", "one", "opens", "a", "dropdown", "menu", "with", "more", "colors.The", "artboard", "’", "s", "state", "after", "P2", "clicked", "the", "button", ",", "which", "flled", "her", "triangle", "with", "the", "default", "color", "(", "orange", ")", "without", "announcing", "the", "change", ",", "and", "switched", "focus", "from", "the", "menu", "back", "to", "her", "triangle.The", "artboard", "’", "s", "state", "after", "P2", "changed", "the", "artboard", "red", ".", "A", "key", "command", "she", "tried", "silently", "switched", "focus", "to", "the", "art-", "board", "and", "opened", "a", "“", "Format", "Background", "”", "task", "pane", ",", "which", "she", "used", "to", "change", "the", "color", "of", "the", "artboard", "’", "s", "background", ",", "leaving", "her", "with", "an", "orange", "triangle", "and", "a", "red", "artboard.Figure", "12", ":", "P2", "’", "s", "artboard", "state", "as", "she", "attempts", "to", "change", "the", "color", "of", "the", "triangle", "in", "the", "upper", "left", "corner", "from", "blue", "to", "red", ",", "without", "any", "feedback", "indicating", "the", "color", "changes", "were", "successful", "or", "what", "objects", "the", "changes", "were", "applied", "to.he", "would", "“", "never", "dare", "present", "my", "PowerPoint", "presentation", "”", "without", "someone", "sighted", "looking", "at", "it", "frst.Accidental", "manipulations", "occurred", "several", "times", "during", "the", "task-", "based", "usability", "study", "due", "to", "the", "lack", "of", "feedback", "both", "for", "manipula-", "tions", "and", "object", "focus", ",", "as", "screen", "readers", "did", "not", "always", "announce", "when", "the", "keyboard", "focus", "changed", "from", "one", "object", "to", "another", ",", "orthe", "announcement", "was", "nested", "in", "a", "long", "string", "of", "announcements", "and", "was", "missed", "by", "the", "user", ".", "For", "example", ",", "when", "P2", "attempted", "to", "change", "the", "color", "of", "the", "triangle", "in", "task", "#", "16", "(", "referenced", "in", "Figure", "1", "and", "Figure", "12", ")", ",", "she", "selected", "the", "``", "Shape", "Fill", "''", "button", ",", "which", "flls", "the", "selected", "shape", "with", "a", "default", "color", "or", "can", "be", "opened", "as", "a", "dropdown", "menu", "to", "choose", "more", "colors", "(", "see", "Figure", "12a", ")", ".", "When", "she", "clicked", "the", "button", ",", "it", "flled", "her", "triangle", "with", "the", "default", "color", "(", "orange", ")", "without", "announcing", "the", "change", ",", "and", "switched", "focus", "from", "the", "menu", "back", "to", "her", "triangle", "(", "see", "Figure", "12b", ")", ".", "P2", "was", "confused", "and", "navigated", "back", "to", "the", "Shape", "Fill", "button", "and", "clicked", "it", "again", ",", "and", "it", "switched", "focus", "back", "to", "her", "triangle", "once", "more", ",", "prompting", "her", "to", "voice", "confusion", "and", "frustration", ":", "“", "What", "?", "Why", "won", "’", "t", "it", "do", "...", "It", "won", "’", "t", "let", "me", "get", "into", "...", "”", "She", "tried", "a", "key", "command", ",", "which", "silently", "switched", "focus", "to", "the", "artboard", "and", "opened", "a", "“", "Format", "Background", "”", "task", "pane", "with", "a", "dropdown", "fll", "button", ",", "which", "she", "used", "to", "select", "a", "shade", "of", "red", "that", "was", "applied", "to", "the", "artboard", ",", "leaving", "her", "with", "an", "orange", "triangle", "and", "a", "red", "artboard", "(", "see", "Figure", "12c", ")", ".", "After", "reading", "through", "the", "other", "formatting", "options", "in", "the", "task", "pane", "and", "noticing", "they", "referred", "to", "the", "“", "background", ",", "”", "she", "switched", "keyboard", "focus", "and", "found", "that", "it", "switched", "to", "the", "triangle", ",", "which", "she", "had", "thought", "she", "already", "had", "focus", "on", ".", "At", "that", "point", ",", "she", "decided", "to", "end", "the", "task", ",", "saying", ",", "“", "Uh", ",", "I", "tried", ".", "Did", "not", "know", "if", "it", "worked", ",", "or", "if", "I", "made", "the", "whole", "slide", "red", ".", "[", "laughs", "]", "I", "did", "what", "I", "could", ",", "I", "don", "’", "t", "know", "what", "else", "I", "can", "do", ".", "”"]}, "filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_020.jpg", "orig_filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "split": "train"}, {"article_id": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences", "description": {"raw": "After successful upload of CSV data, user will be able to see three different visualizations on basis of the data uploaded. These visualizations are gender pie chart, age based gender wise distribution bar chart and per construct box plots", "tokens": ["After", "successful", "upload", "of", "CSV", "data", ",", "user", "will", "be", "able", "to", "see", "three", "different", "visualizations", "on", "basis", "of", "the", "data", "uploaded", ".", "These", "visualizations", "are", "gender", "pie", "chart", ",", "age", "based", "gender", "wise", "distribution", "bar", "chart", "and", "per", "construct", "box", "plots"]}, "caption": {"raw": "Figure 2. Overview of the data analysis module as presented by the PXI Bench. On the top right and left, visualizations illustrate demographic information. At the bottom, the actual analysis of the PXI data.", "tokens": ["Figure", "2", ".", "Overview", "of", "the", "data", "analysis", "module", "as", "presented", "by", "the", "PXI", "Bench", ".", "On", "the", "top", "right", "and", "left", ",", "visualizations", "illustrate", "demographic", "information", ".", "At", "the", "bottom", ",", "the", "actual", "analysis", "of", "the", "PXI", "data", "."]}, "context": {"raw": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences Figure 2. Overview of the data analysis module as presented by the PXI Bench. On the top right and left, visualizations illustrate demographic information. At the bottom, the actual analysis of the PXI data.", "tokens": ["The", "Player", "Experience", "Inventory", "Bench", ":", "Providing", "Games", "User", "Researchers", "Actionable", "Insight", "into", "Player", "Experiences", "Figure", "2", ".", "Overview", "of", "the", "data", "analysis", "module", "as", "presented", "by", "the", "PXI", "Bench", ".", "On", "the", "top", "right", "and", "left", ",", "visualizations", "illustrate", "demographic", "information", ".", "At", "the", "bottom", ",", "the", "actual", "analysis", "of", "the", "PXI", "data", "."]}, "filename": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef_Image_002.jpg", "orig_filename": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef", "split": "train"}, {"article_id": "HapticClench: Investigating Squeeze Sensations using Memory Alloys", "description": {"raw": "Bar graph depicting 75%, 45%, 80% accuracy for hold, continuous, staggered respectively for 30s. And 85%, 65%, 90% accuracy for hold, continuous, staggered, respectively for 60s.", "tokens": ["Bar", "graph", "depicting", "75", "%", ",", "45", "%", ",", "80", "%", "accuracy", "for", "hold", ",", "continuous", ",", "staggered", "respectively", "for", "30s", ".", "And", "85", "%", ",", "65", "%", ",", "90", "%", "accuracy", "for", "hold", ",", "continuous", ",", "staggered", ",", "respectively", "for", "60s", "."]}, "caption": {"raw": "Figure 10: Mean Accuracy % for all three pulses for both durations. Continuous pulse is least accurate. [95% CI]", "tokens": ["Figure", "10", ":", "Mean", "Accuracy", "%", "for", "all", "three", "pulses", "for", "both", "durations", ".", "Continuous", "pulse", "is", "least", "accurate", ".", "[", "95", "%", "CI", "]"]}, "context": {"raw": "HapticClench: Investigating Squeeze Sensations using Memory Alloys Figure 10: Mean Accuracy % for all three pulses for both durations. Continuous pulse is least accurate. [95% CI]", "tokens": ["HapticClench", ":", "Investigating", "Squeeze", "Sensations", "using", "Memory", "Alloys", "Figure", "10", ":", "Mean", "Accuracy", "%", "for", "all", "three", "pulses", "for", "both", "durations", ".", "Continuous", "pulse", "is", "least", "accurate", ".", "[", "95", "%", "CI", "]"]}, "filename": "4147ba46985a1fa81abcf649811afaf1353fdc6a_Image_015.jpg", "orig_filename": "4147ba46985a1fa81abcf649811afaf1353fdc6a", "split": "train"}, {"article_id": "User Experiences with Online Status Indicators", "description": {"raw": "Four plots are shown, corresponding to the four experimental conditions. The y-axis on each plot shows the percent of participants who correctly identified that the dot was an OSI. The x-axis has 5 marks to correspond to the 5 images shown to participants. All four graphs show a monotonically increasing number of participants getting the correct response. The plot for the green condition shows a significantly greater area under the curve that connects the 5 data points, because more participants answered correctly for earlier images in the sequence.", "tokens": ["Four", "plots", "are", "shown", ",", "corresponding", "to", "the", "four", "experimental", "conditions", ".", "The", "y-axis", "on", "each", "plot", "shows", "the", "percent", "of", "participants", "who", "correctly", "identified", "that", "the", "dot", "was", "an", "OSI", ".", "The", "x-axis", "has", "5", "marks", "to", "correspond", "to", "the", "5", "images", "shown", "to", "participants", ".", "All", "four", "graphs", "show", "a", "monotonically", "increasing", "number", "of", "participants", "getting", "the", "correct", "response", ".", "The", "plot", "for", "the", "green", "condition", "shows", "a", "significantly", "greater", "area", "under", "the", "curve", "that", "connects", "the", "5", "data", "points", ",", "because", "more", "participants", "answered", "correctly", "for", "earlier", "images", "in", "the", "sequence", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "User Experiences with Online Status Indicators ", "tokens": ["User", "Experiences", "with", "Online", "Status", "Indicators"]}, "filename": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b_Image_005.png", "orig_filename": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b", "split": "train"}, {"article_id": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "description": {"raw": "Bar chart with perceived dificulty for both user groups and conditions. There are no significant differences beetween control and SlidePacer conditions.", "tokens": ["Bar", "chart", "with", "perceived", "dificulty", "for", "both", "user", "groups", "and", "conditions", ".", "There", "are", "no", "significant", "differences", "beetween", "control", "and", "SlidePacer", "conditions", "."]}, "caption": {"raw": "Figure 7. Perceived lecture difficulty for both user groups and conditions.", "tokens": ["Figure", "7", ".", "Perceived", "lecture", "difficulty", "for", "both", "user", "groups", "and", "conditions", "."]}, "context": {"raw": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students Figure 7. Perceived lecture difficulty for both user groups and conditions.", "tokens": ["SlidePacer", ":", "A", "Presentation", "Delivery", "Tool", "for", "Instructors", "of", "Deaf", "and", "Hard", "of", "Hearing", "Students", "Figure", "7", ".", "Perceived", "lecture", "difficulty", "for", "both", "user", "groups", "and", "conditions", "."]}, "filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_011.png", "orig_filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "split": "train"}, {"article_id": "PassChords: secure multi-touch authentication for blind people", "description": {"raw": "A bar graph with all possible finger tap patterns on the x-axis and percentages on the y-axis.", "tokens": ["A", "bar", "graph", "with", "all", "possible", "finger", "tap", "patterns", "on", "the", "x-axis", "and", "percentages", "on", "the", "y-axis", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "PassChords: secure multi-touch authentication for blind people ", "tokens": ["PassChords", ":", "secure", "multi-touch", "authentication", "for", "blind", "people"]}, "filename": "2d955bfeb3b7b43d5d1f45161b068a705190fde9_Image_005.jpg", "orig_filename": "2d955bfeb3b7b43d5d1f45161b068a705190fde9", "split": "train"}, {"article_id": "Tactile graphics with a voice: using QR codes to access text in tactile graphics", "description": {"raw": "A line chart showing the average accuracies of the different modes for the Bar Chart task. The first session is the lowest and then the lines tend to increase. The fifth session has a large drop in accuracy for the silent and verbal modes. The Silent and Verbal lines tend to be similar and the Finger Pointing line is higher in the last three sessions. There is a single point for the Braille modes in session six which is between Silent and Verbal Modes and below Finger Pointing.", "tokens": ["A", "line", "chart", "showing", "the", "average", "accuracies", "of", "the", "different", "modes", "for", "the", "Bar", "Chart", "task", ".", "The", "first", "session", "is", "the", "lowest", "and", "then", "the", "lines", "tend", "to", "increase", ".", "The", "fifth", "session", "has", "a", "large", "drop", "in", "accuracy", "for", "the", "silent", "and", "verbal", "modes", ".", "The", "Silent", "and", "Verbal", "lines", "tend", "to", "be", "similar", "and", "the", "Finger", "Pointing", "line", "is", "higher", "in", "the", "last", "three", "sessions", ".", "There", "is", "a", "single", "point", "for", "the", "Braille", "modes", "in", "session", "six", "which", "is", "between", "Silent", "and", "Verbal", "Modes", "and", "below", "Finger", "Pointing", "."]}, "caption": {"raw": "Figure 3. A comparison of the average accuracy for the Bar Chart task across the six sessions for the three modes (n=10) and Braille (n=6) on the last session. Participants were asked to find the range of the tallest bar and their answer could be 0, 50 or 100% correct.", "tokens": ["Figure", "3", ".", "A", "comparison", "of", "the", "average", "accuracy", "for", "the", "Bar", "Chart", "task", "across", "the", "six", "sessions", "for", "the", "three", "modes", "(", "n=10", ")", "and", "Braille", "(", "n=6", ")", "on", "the", "last", "session", ".", "Participants", "were", "asked", "to", "find", "the", "range", "of", "the", "tallest", "bar", "and", "their", "answer", "could", "be", "0", ",", "50", "or", "100", "%", "correct", "."]}, "context": {"raw": "Tactile graphics with a voice: using QR codes to access text in tactile graphics Figure 3. A comparison of the average accuracy for the Bar Chart task across the six sessions for the three modes (n=10) and Braille (n=6) on the last session. Participants were asked to find the range of the tallest bar and their answer could be 0, 50 or 100% correct.", "tokens": ["Tactile", "graphics", "with", "a", "voice", ":", "using", "QR", "codes", "to", "access", "text", "in", "tactile", "graphics", "Figure", "3", ".", "A", "comparison", "of", "the", "average", "accuracy", "for", "the", "Bar", "Chart", "task", "across", "the", "six", "sessions", "for", "the", "three", "modes", "(", "n=10", ")", "and", "Braille", "(", "n=6", ")", "on", "the", "last", "session", ".", "Participants", "were", "asked", "to", "find", "the", "range", "of", "the", "tallest", "bar", "and", "their", "answer", "could", "be", "0", ",", "50", "or", "100", "%", "correct", "."]}, "filename": "fd420c53c56d844ccd48efd6a261907b2bc53de9_Image_008.jpg", "orig_filename": "fd420c53c56d844ccd48efd6a261907b2bc53de9", "split": "train"}, {"article_id": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "description": {"raw": "Bar chart of the regularity of which participants check their tracker data, 63% of the participants check their data at least once a day.", "tokens": ["Bar", "chart", "of", "the", "regularity", "of", "which", "participants", "check", "their", "tracker", "data", ",", "63", "%", "of", "the", "participants", "check", "their", "data", "at", "least", "once", "a", "day", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection ", "tokens": ["The", "Technology-Mediated", "Reflection", "Model", ":", "Barriers", "and", "Assistance", "in", "Data-Driven", "Reflection"]}, "filename": "09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_006.jpg", "orig_filename": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "split": "train"}, {"article_id": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality", "description": {"raw": "Bar chart of median completion time in milliseconds for the 7 pointing conditions in the smartphone study. The y-axis indicates median completion time and the x-axis indicates the pointing condition. The first bar is VC, coloured grey, which is faster than other pointing conditions. The 2nd, 3rd, and 4th bars, coloured yellow, represent pointing techniques with VB, which are faster than pointing techniques with WD (5th, 6th, 7th bar), coloured blue.  The significant difference between each other is also visualised.", "tokens": ["Bar", "chart", "of", "median", "completion", "time", "in", "milliseconds", "for", "the", "7", "pointing", "conditions", "in", "the", "smartphone", "study", ".", "The", "y-axis", "indicates", "median", "completion", "time", "and", "the", "x-axis", "indicates", "the", "pointing", "condition", ".", "The", "first", "bar", "is", "VC", ",", "coloured", "grey", ",", "which", "is", "faster", "than", "other", "pointing", "conditions", ".", "The", "2nd", ",", "3rd", ",", "and", "4th", "bars", ",", "coloured", "yellow", ",", "represent", "pointing", "techniques", "with", "VB", ",", "which", "are", "faster", "than", "pointing", "techniques", "with", "WD", "(", "5th", ",", "6th", ",", "7th", "bar", ")", ",", "coloured", "blue", ".", "The", "significant", "difference", "between", "each", "other", "is", "also", "visualised", "."]}, "caption": {"raw": "Figure 6. Median CT for pointing conditions. The statistical signiﬁ- cances evaluated by pairwise t-test are marked with stars (∗∗ = p < 0.01 and ∗ = p < 0.05).", "tokens": ["Figure", "6", ".", "Median", "CT", "for", "pointing", "conditions", ".", "The", "statistical", "signiﬁ-", "cances", "evaluated", "by", "pairwise", "t-test", "are", "marked", "with", "stars", "(", "∗∗", "=", "p", "<", "0.01", "and", "∗", "=", "p", "<", "0.05", ")", "."]}, "context": {"raw": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality Figure 6. Median CT for pointing conditions. The statistical signiﬁ- cances evaluated by pairwise t-test are marked with stars (∗∗ = p < 0.01 and ∗ = p < 0.05).", "tokens": ["Understanding", "Viewport-", "and", "World-based", "Pointing", "with", "Everyday", "Smart", "Devices", "in", "Immersive", "Augmented", "Reality", "Figure", "6", ".", "Median", "CT", "for", "pointing", "conditions", ".", "The", "statistical", "signiﬁ-", "cances", "evaluated", "by", "pairwise", "t-test", "are", "marked", "with", "stars", "(", "∗∗", "=", "p", "<", "0.01", "and", "∗", "=", "p", "<", "0.05", ")", "."]}, "filename": "7fa3a268386650d6940c6450eb41de2f28ff0a6a_Image_007.jpg", "orig_filename": "7fa3a268386650d6940c6450eb41de2f28ff0a6a", "split": "train"}, {"article_id": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "description": {"raw": "Bar chart of number of participants that reflect on their data spontaneously, shows that 85% of the participants somewhat to strongly agree that reflection happens spontaneously.", "tokens": ["Bar", "chart", "of", "number", "of", "participants", "that", "reflect", "on", "their", "data", "spontaneously", ",", "shows", "that", "85", "%", "of", "the", "participants", "somewhat", "to", "strongly", "agree", "that", "reflection", "happens", "spontaneously", "."]}, "caption": {"raw": "(a)", "tokens": ["(", "a", ")"]}, "context": {"raw": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection (a)", "tokens": ["The", "Technology-Mediated", "Reflection", "Model", ":", "Barriers", "and", "Assistance", "in", "Data-Driven", "Reflection", "(", "a", ")"]}, "filename": "09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_008.jpg", "orig_filename": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "split": "train"}, {"article_id": "In the blink of an eye: investigating latency perception during stylus interaction", "description": {"raw": "Bar graph of the JND latency thresholds gathered from each participant during the large box dragging task.", "tokens": ["Bar", "graph", "of", "the", "JND", "latency", "thresholds", "gathered", "from", "each", "participant", "during", "the", "large", "box", "dragging", "task", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "In the blink of an eye: investigating latency perception during stylus interaction ", "tokens": ["In", "the", "blink", "of", "an", "eye", ":", "investigating", "latency", "perception", "during", "stylus", "interaction"]}, "filename": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_009.jpg", "orig_filename": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "split": "train"}, {"article_id": "Engineering Gender-Inclusivity into Software: Tales from the Trenches", "description": {"raw": "Flow chart:  First Node: Find [feature] leads to second node: select academic year leads to third node select academic period Splits and leads to two separate nodes, 4a and 4b. 4a: Click on var that enables [feature] 4b: click on dropdown that contains [feature]. Both 4a and 4b come back together and point to node 5: view [feature].", "tokens": ["Flow", "chart", ":", "First", "Node", ":", "Find", "[", "feature", "]", "leads", "to", "second", "node", ":", "select", "academic", "year", "leads", "to", "third", "node", "select", "academic", "period", "Splits", "and", "leads", "to", "two", "separate", "nodes", ",", "4a", "and", "4b", ".", "4a", ":", "Click", "on", "var", "that", "enables", "[", "feature", "]", "4b", ":", "click", "on", "dropdown", "that", "contains", "[", "feature", "]", ".", "Both", "4a", "and", "4b", "come", "back", "together", "and", "point", "to", "node", "5", ":", "view", "[", "feature", "]", "."]}, "caption": {"raw": "Figure 4: Team C evaluated both of the small paths that Abi could take to reach the same subgoal.", "tokens": ["Figure", "4", ":", "Team", "C", "evaluated", "both", "of", "the", "small", "paths", "that", "Abi", "could", "take", "to", "reach", "the", "same", "subgoal", "."]}, "context": {"raw": "Engineering Gender-Inclusivity into Software: Tales from the Trenches Figure 4: Team C evaluated both of the small paths that Abi could take to reach the same subgoal.", "tokens": ["Engineering", "Gender-Inclusivity", "into", "Software", ":", "Tales", "from", "the", "Trenches", "Figure", "4", ":", "Team", "C", "evaluated", "both", "of", "the", "small", "paths", "that", "Abi", "could", "take", "to", "reach", "the", "same", "subgoal", "."]}, "filename": "1d1c5c4c1b37d15a0bc9af702c53293e3b45e458_Image_004.png", "orig_filename": "1d1c5c4c1b37d15a0bc9af702c53293e3b45e458", "split": "train"}, {"article_id": "From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People", "description": {"raw": "Figure 1 is an illustration of an AI-powered chatbot user interface. It is one of many similar systems that are widely adopted by healthcare end-customers. User can have diagnosis advices through natural language question and answering processes. As illustrated in the chart, by the end of the conversation flow between the user and the chatbot, the chatbot predict the patient has Acute Upper Respiratory Infection and recommend the patient to see a doctor immediately.", "tokens": ["Figure", "1", "is", "an", "illustration", "of", "an", "AI-powered", "chatbot", "user", "interface", ".", "It", "is", "one", "of", "many", "similar", "systems", "that", "are", "widely", "adopted", "by", "healthcare", "end-customers", ".", "User", "can", "have", "diagnosis", "advices", "through", "natural", "language", "question", "and", "answering", "processes", ".", "As", "illustrated", "in", "the", "chart", ",", "by", "the", "end", "of", "the", "conversation", "flow", "between", "the", "user", "and", "the", "chatbot", ",", "the", "chatbot", "predict", "the", "patient", "has", "Acute", "Upper", "Respiratory", "Infection", "and", "recommend", "the", "patient", "to", "see", "a", "doctor", "immediately", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People ", "tokens": ["From", "Human-Human", "Collaboration", "to", "Human-AI", "Collaboration", ":", "Designing", "AI", "Systems", "That", "Can", "Work", "Together", "with", "People"]}, "filename": "660a2244efa8af0b77fd314a1c75dcc01aa677fe_Image_002.jpg", "orig_filename": "660a2244efa8af0b77fd314a1c75dcc01aa677fe", "split": "train"}, {"article_id": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning", "description": {"raw": "The Group Spinner interface showing the radar chart for a group of students for the current and previous sessions. It also shows the indicators for the selected group so the teacher can update the graph based on the indicator values.", "tokens": ["The", "Group", "Spinner", "interface", "showing", "the", "radar", "chart", "for", "a", "group", "of", "students", "for", "the", "current", "and", "previous", "sessions", ".", "It", "also", "shows", "the", "indicators", "for", "the", "selected", "group", "so", "the", "teacher", "can", "update", "the", "graph", "based", "on", "the", "indicator", "values", "."]}, "caption": {"raw": "Figure 1. Annotated crop of Group Spinner’s interface showing current and previous session graphs along with some indicators.", "tokens": ["Figure", "1", ".", "Annotated", "crop", "of", "Group", "Spinner", "’", "s", "interface", "showing", "current", "and", "previous", "session", "graphs", "along", "with", "some", "indicators", "."]}, "context": {"raw": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning Figure 1. Annotated crop of Group Spinner’s interface showing current and previous session graphs along with some indicators.", "tokens": ["Group", "Spinner", ":", "Recognizing", "&", "Visualizing", "Learning", "in", "the", "Classroom", "for", "Reflection", ",", "Communication", "&", "Planning", "Figure", "1", ".", "Annotated", "crop", "of", "Group", "Spinner", "’", "s", "interface", "showing", "current", "and", "previous", "session", "graphs", "along", "with", "some", "indicators", "."]}, "filename": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a_Image_006.jpg", "orig_filename": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a", "split": "train"}, {"article_id": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "description": {"raw": "This Figure is divided into three parts: a, b, and c.\n\nPart (a) of this figure shows a stacked bar plot showing percentages of 5-point likert-scale responses. The title is “This text was easy to read”, the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The likert-scale responses for “Neutral” is centered, and “Disagree” and “Strongly Disagree” are on the left side, and “Agree” and “Strongly Agree” are on the right side. For all the conditions, the majority of responses are on the right side, and there is very little on the left side.\n\nFor \"Original\" 4% responded “Strongly Disagree”, 4% for “Disagree”, 24% for “Neutral”, 56% for “Agree”, and 12% for “Strongly Agree”. \nFor \"Automatic\" 0% responded “Strongly Disagree”, 4% for “Disagree”, 8% for “Neutral”, 60% for “Agree”, and 28% for “Strongly Agree”.\nFor \"Pop-up\" 0% responded “Strongly Disagree”, 8% for “Disagree”, 12% for “Neutral”, 48% for “Agree”, and 32% for “Strongly Agree”. \nFor \"Decoration\" 0% responded “Strongly Disagree”, 4% for “Disagree”, 12% for “Neutral”, 52% for “Agree”, and 32% for “Strongly Agree”.\n\n\nPart (b) of this figure shows a stacked bar plot showing percentages of 5-point Likert-scale responses. The title is “I was able to understand this text well.”, the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. There is a bracket indicating p<.05 significance between the “Original” and “Pop-up” plots. There is also a bracket indicating p<.05 significance between the “Original” and “Decoration” plots. The Likert-scale responses for “Neutral” is centered, and “Disagree” and “Strongly Disagree” are on the left side, and “Agree” and “Strongly Agree” are on the right side. For all conditions, the majority of responses are on the right side.\n\nFor \"Original\" 0% responded “Strongly Disagree”, 12% for “Disagree”, 20% for “Neutral”, 60% for “Agree”, and 8% for “Strongly Agree”. \nFor \"Automatic\" 0% responded “Strongly Disagree”, 4% for “Disagree”, 20% for “Neutral”, 40% for “Agree”, and 36% for “Strongly Agree”.\nFor \"Pop-up\" 0% responded “Strongly Disagree”, 0% for “Disagree”, 12% for “Neutral”, 48% for “Agree”, and 40% for “Strongly Agree”. \nFor \"Decoration\" 0% responded “Strongly Disagree”, 0% for “Disagree”, 12% for “Neutral”, 44% for “Agree”, and 44% for “Strongly Agree”.\n\n\nPart (c) of this figure shows a boxplot for Comprehension Scores for four different conditions, “Original”, “Automatic”, “Pop-up”, and “Decoration”, which are on the x-axis. The y-axis is the score, and is a percentage from 0 to 100. For “Original” and “Automatic” the boxplots appear to be almost equal and most of the boxplot is between 30% and 70%. For “Pop-up”, it is higher up, with the first quartile starting at around 70%. For “Decoration” there is a wider range, the first quartile is at around 30% and the third quartile is at 100%.\n\nThe “Original” boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 62.70%, 3rd quartile 66.67%, and a maximum of 100%.\nThe “Automatic” boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.7%, mean 58.7%, 3rd quartile 66.67%, and a maximum of 100%.\nThe “Pop-up” boxplot has a minimum of 33.33%, 1st quartile 66.67%, median 66.7%, mean 76%, 3rd quartile 100%, and a maximum of 100%.\nThe “Decoration” boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 60%, 3rd quartile 100%, and a maximum of 100%.", "tokens": ["This", "Figure", "is", "divided", "into", "three", "parts", ":", "a", ",", "b", ",", "and", "c.", "Part", "(", "a", ")", "of", "this", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "5-point", "likert-scale", "responses", ".", "The", "title", "is", "“", "This", "text", "was", "easy", "to", "read", "”", ",", "the", "y-axis", "has", "4", "categories", ";", "``", "Original", "''", ",", "``", "Automatic", "''", ",", "``", "Pop-up", "''", ",", "and", "``", "Decoration", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "The", "likert-scale", "responses", "for", "“", "Neutral", "”", "is", "centered", ",", "and", "“", "Disagree", "”", "and", "“", "Strongly", "Disagree", "”", "are", "on", "the", "left", "side", ",", "and", "“", "Agree", "”", "and", "“", "Strongly", "Agree", "”", "are", "on", "the", "right", "side", ".", "For", "all", "the", "conditions", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ",", "and", "there", "is", "very", "little", "on", "the", "left", "side", ".", "For", "``", "Original", "''", "4", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "24", "%", "for", "“", "Neutral", "”", ",", "56", "%", "for", "“", "Agree", "”", ",", "and", "12", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Automatic", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "8", "%", "for", "“", "Neutral", "”", ",", "60", "%", "for", "“", "Agree", "”", ",", "and", "28", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Pop-up", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "8", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "48", "%", "for", "“", "Agree", "”", ",", "and", "32", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Decoration", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "52", "%", "for", "“", "Agree", "”", ",", "and", "32", "%", "for", "“", "Strongly", "Agree", "”", ".", "Part", "(", "b", ")", "of", "this", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "5-point", "Likert-scale", "responses", ".", "The", "title", "is", "“", "I", "was", "able", "to", "understand", "this", "text", "well.", "”", ",", "the", "y-axis", "has", "4", "categories", ";", "``", "Original", "''", ",", "``", "Automatic", "''", ",", "``", "Pop-up", "''", ",", "and", "``", "Decoration", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "There", "is", "a", "bracket", "indicating", "p", "<", ".05", "significance", "between", "the", "“", "Original", "”", "and", "“", "Pop-up", "”", "plots", ".", "There", "is", "also", "a", "bracket", "indicating", "p", "<", ".05", "significance", "between", "the", "“", "Original", "”", "and", "“", "Decoration", "”", "plots", ".", "The", "Likert-scale", "responses", "for", "“", "Neutral", "”", "is", "centered", ",", "and", "“", "Disagree", "”", "and", "“", "Strongly", "Disagree", "”", "are", "on", "the", "left", "side", ",", "and", "“", "Agree", "”", "and", "“", "Strongly", "Agree", "”", "are", "on", "the", "right", "side", ".", "For", "all", "conditions", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ".", "For", "``", "Original", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "12", "%", "for", "“", "Disagree", "”", ",", "20", "%", "for", "“", "Neutral", "”", ",", "60", "%", "for", "“", "Agree", "”", ",", "and", "8", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Automatic", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "20", "%", "for", "“", "Neutral", "”", ",", "40", "%", "for", "“", "Agree", "”", ",", "and", "36", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Pop-up", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "0", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "48", "%", "for", "“", "Agree", "”", ",", "and", "40", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Decoration", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "0", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "44", "%", "for", "“", "Agree", "”", ",", "and", "44", "%", "for", "“", "Strongly", "Agree", "”", ".", "Part", "(", "c", ")", "of", "this", "figure", "shows", "a", "boxplot", "for", "Comprehension", "Scores", "for", "four", "different", "conditions", ",", "“", "Original", "”", ",", "“", "Automatic", "”", ",", "“", "Pop-up", "”", ",", "and", "“", "Decoration", "”", ",", "which", "are", "on", "the", "x-axis", ".", "The", "y-axis", "is", "the", "score", ",", "and", "is", "a", "percentage", "from", "0", "to", "100", ".", "For", "“", "Original", "”", "and", "“", "Automatic", "”", "the", "boxplots", "appear", "to", "be", "almost", "equal", "and", "most", "of", "the", "boxplot", "is", "between", "30", "%", "and", "70", "%", ".", "For", "“", "Pop-up", "”", ",", "it", "is", "higher", "up", ",", "with", "the", "first", "quartile", "starting", "at", "around", "70", "%", ".", "For", "“", "Decoration", "”", "there", "is", "a", "wider", "range", ",", "the", "first", "quartile", "is", "at", "around", "30", "%", "and", "the", "third", "quartile", "is", "at", "100", "%", ".", "The", "“", "Original", "”", "boxplot", "has", "a", "minimum", "of", "0", "%", ",", "1st", "quartile", "33.33", "%", ",", "median", "66.67", "%", ",", "mean", "62.70", "%", ",", "3rd", "quartile", "66.67", "%", ",", "and", "a", "maximum", "of", "100", "%", ".", "The", "“", "Automatic", "”", "boxplot", "has", "a", "minimum", "of", "0", "%", ",", "1st", "quartile", "33.33", "%", ",", "median", "66.7", "%", ",", "mean", "58.7", "%", ",", "3rd", "quartile", "66.67", "%", ",", "and", "a", "maximum", "of", "100", "%", ".", "The", "“", "Pop-up", "”", "boxplot", "has", "a", "minimum", "of", "33.33", "%", ",", "1st", "quartile", "66.67", "%", ",", "median", "66.7", "%", ",", "mean", "76", "%", ",", "3rd", "quartile", "100", "%", ",", "and", "a", "maximum", "of", "100", "%", ".", "The", "“", "Decoration", "”", "boxplot", "has", "a", "minimum", "of", "0", "%", ",", "1st", "quartile", "33.33", "%", ",", "median", "66.67", "%", ",", "mean", "60", "%", ",", "3rd", "quartile", "100", "%", ",", "and", "a", "maximum", "of", "100", "%", "."]}, "caption": {"raw": "(c)  Figure 4. Participants’ responses to questions about all four conditions in the experimental study, including subjective Likert-scale responses for (a) the text was easy to read and (b) I was able to understand this text well, with significant pairwise differences marked with asterisks (* p <0.05). In (c), analysis on objective comprehension questions did not reveal any significant differences between the four conditions.", "tokens": ["(", "c", ")", "Figure", "4", ".", "Participants", "’", "responses", "to", "questions", "about", "all", "four", "conditions", "in", "the", "experimental", "study", ",", "including", "subjective", "Likert-scale", "responses", "for", "(", "a", ")", "the", "text", "was", "easy", "to", "read", "and", "(", "b", ")", "I", "was", "able", "to", "understand", "this", "text", "well", ",", "with", "significant", "pairwise", "differences", "marked", "with", "asterisks", "(", "*", "p", "<", "0.05", ")", ".", "In", "(", "c", ")", ",", "analysis", "on", "objective", "comprehension", "questions", "did", "not", "reveal", "any", "significant", "differences", "between", "the", "four", "conditions", "."]}, "context": {"raw": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy (c)  Figure 4. Participants’ responses to questions about all four conditions in the experimental study, including subjective Likert-scale responses for (a) the text was easy to read and (b) I was able to understand this text well, with significant pairwise differences marked with asterisks (* p <0.05). In (c), analysis on objective comprehension questions did not reveal any significant differences between the four conditions.", "tokens": ["Automatic", "Text", "Simplification", "Tools", "for", "Deaf", "and", "Hard", "of", "Hearing", "Adults", ":", "Benefits", "of", "Lexical", "Simplification", "and", "Providing", "Users", "with", "Autonomy", "(", "c", ")", "Figure", "4", ".", "Participants", "’", "responses", "to", "questions", "about", "all", "four", "conditions", "in", "the", "experimental", "study", ",", "including", "subjective", "Likert-scale", "responses", "for", "(", "a", ")", "the", "text", "was", "easy", "to", "read", "and", "(", "b", ")", "I", "was", "able", "to", "understand", "this", "text", "well", ",", "with", "significant", "pairwise", "differences", "marked", "with", "asterisks", "(", "*", "p", "<", "0.05", ")", ".", "In", "(", "c", ")", ",", "analysis", "on", "objective", "comprehension", "questions", "did", "not", "reveal", "any", "significant", "differences", "between", "the", "four", "conditions", "."]}, "filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_005.jpg", "orig_filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "split": "train"}, {"article_id": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "description": {"raw": "This image illustrates how the three components of the Splash framework integrate with an existing data server and visualization client.", "tokens": ["This", "image", "illustrates", "how", "the", "three", "components", "of", "the", "Splash", "framework", "integrate", "with", "an", "existing", "data", "server", "and", "visualization", "client", "."]}, "caption": {"raw": "Figure 4: Overview of the Splash framework. The five steps required to integrate Splash into server- and client-side are divided amongst the data curator and visualization developer roles. Implementation responsibilities of each are indicated by asterisks.", "tokens": ["Figure", "4", ":", "Overview", "of", "the", "Splash", "framework", ".", "The", "five", "steps", "required", "to", "integrate", "Splash", "into", "server-", "and", "client-side", "are", "divided", "amongst", "the", "data", "curator", "and", "visualization", "developer", "roles", ".", "Implementation", "responsibilities", "of", "each", "are", "indicated", "by", "asterisks", "."]}, "context": {"raw": "Dive in!: enabling progressive loading for real-time navigation of data visualizations Figure 4: Overview of the Splash framework. The five steps required to integrate Splash into server- and client-side are divided amongst the data curator and visualization developer roles. Implementation responsibilities of each are indicated by asterisks.", "tokens": ["Dive", "in", "!", ":", "enabling", "progressive", "loading", "for", "real-time", "navigation", "of", "data", "visualizations", "Figure", "4", ":", "Overview", "of", "the", "Splash", "framework", ".", "The", "five", "steps", "required", "to", "integrate", "Splash", "into", "server-", "and", "client-side", "are", "divided", "amongst", "the", "data", "curator", "and", "visualization", "developer", "roles", ".", "Implementation", "responsibilities", "of", "each", "are", "indicated", "by", "asterisks", "."]}, "filename": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_004.jpg", "orig_filename": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "split": "train"}, {"article_id": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code", "description": {"raw": "This is a bar chart of the mean time to complete the tasks.    With Keywords: Without Tool 3:57, with tool 3:32  Without Keywords: Without Tool 4:25, with tool 2:25  Conditions: Without Tool 5:01, with tool 2:24  All: Without Tool 4:28, with tool 2:47", "tokens": ["This", "is", "a", "bar", "chart", "of", "the", "mean", "time", "to", "complete", "the", "tasks", ".", "With", "Keywords", ":", "Without", "Tool", "3:57", ",", "with", "tool", "3:32", "Without", "Keywords", ":", "Without", "Tool", "4:25", ",", "with", "tool", "2:25", "Conditions", ":", "Without", "Tool", "5:01", ",", "with", "tool", "2:24", "All", ":", "Without", "Tool", "4:28", ",", "with", "tool", "2:47"]}, "caption": {"raw": "Figure 5. This chart shows the average completion time that it took participants to complete the three tasks bro- ken down by type. The bars represent the standard error.", "tokens": ["Figure", "5", ".", "This", "chart", "shows", "the", "average", "completion", "time", "that", "it", "took", "participants", "to", "complete", "the", "three", "tasks", "bro-", "ken", "down", "by", "type", ".", "The", "bars", "represent", "the", "standard", "error", "."]}, "context": {"raw": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code Figure 5. This chart shows the average completion time that it took participants to complete the three tasks bro- ken down by type. The bars represent the standard error.", "tokens": ["StructJumper", ":", "A", "Tool", "to", "Help", "Blind", "Programmers", "Navigate", "and", "Understand", "the", "Structure", "of", "Code", "Figure", "5", ".", "This", "chart", "shows", "the", "average", "completion", "time", "that", "it", "took", "participants", "to", "complete", "the", "three", "tasks", "bro-", "ken", "down", "by", "type", ".", "The", "bars", "represent", "the", "standard", "error", "."]}, "filename": "13eccfcfd4ce807e14da418115a386b918870582_Image_005.jpg", "orig_filename": "13eccfcfd4ce807e14da418115a386b918870582", "split": "train"}, {"article_id": "Designing an Animated Character System for American Sign Language", "description": {"raw": "Figure 3: How people communicate in ASL a) digitally, b) when taking notes, and c) when using an ASL writing system. This figure presents three bar charts (a, b, and c). The y-axis is % Participants, ranging from 0-90. The x-axis is a) Digital Communication Format, b) ASL Note Format, and c) Known Writing Systems. Each bar chart provides separate bars for DHH (light blue) and Hearing (dark blue) populations.    a) Digital ASL Communication Formats, sorted by DHH popularity (most popular first): Animated emoji, English gloss, Live video chat, Recorded videos, English descriptions, Non-animated emoji, Other, N/A.  b) ASL Note Formats, sorted by DHH popularity (most popular first): English gloss, English translation, ASL writing system, ASL video recording, English descriptions, Drawings of signs, Other, N/A.  c)Known ASL Writing Systems, sorted by DHH popularity (most popular first): English gloss, SignWriting, HamNoSys, Stokoe notation, si5s, Other, None.", "tokens": ["Figure", "3", ":", "How", "people", "communicate", "in", "ASL", "a", ")", "digitally", ",", "b", ")", "when", "taking", "notes", ",", "and", "c", ")", "when", "using", "an", "ASL", "writing", "system", ".", "This", "figure", "presents", "three", "bar", "charts", "(", "a", ",", "b", ",", "and", "c", ")", ".", "The", "y-axis", "is", "%", "Participants", ",", "ranging", "from", "0-90", ".", "The", "x-axis", "is", "a", ")", "Digital", "Communication", "Format", ",", "b", ")", "ASL", "Note", "Format", ",", "and", "c", ")", "Known", "Writing", "Systems", ".", "Each", "bar", "chart", "provides", "separate", "bars", "for", "DHH", "(", "light", "blue", ")", "and", "Hearing", "(", "dark", "blue", ")", "populations", ".", "a", ")", "Digital", "ASL", "Communication", "Formats", ",", "sorted", "by", "DHH", "popularity", "(", "most", "popular", "first", ")", ":", "Animated", "emoji", ",", "English", "gloss", ",", "Live", "video", "chat", ",", "Recorded", "videos", ",", "English", "descriptions", ",", "Non-animated", "emoji", ",", "Other", ",", "N/A", ".", "b", ")", "ASL", "Note", "Formats", ",", "sorted", "by", "DHH", "popularity", "(", "most", "popular", "first", ")", ":", "English", "gloss", ",", "English", "translation", ",", "ASL", "writing", "system", ",", "ASL", "video", "recording", ",", "English", "descriptions", ",", "Drawings", "of", "signs", ",", "Other", ",", "N/A", ".", "c", ")", "Known", "ASL", "Writing", "Systems", ",", "sorted", "by", "DHH", "popularity", "(", "most", "popular", "first", ")", ":", "English", "gloss", ",", "SignWriting", ",", "HamNoSys", ",", "Stokoe", "notation", ",", "si5s", ",", "Other", ",", "None", "."]}, "caption": {"raw": "Figure 3: How people communicate in ASL a) digitally, b) when taking notes, and c) when using an ASL writing system.‌", "tokens": ["Figure", "3", ":", "How", "people", "communicate", "in", "ASL", "a", ")", "digitally", ",", "b", ")", "when", "taking", "notes", ",", "and", "c", ")", "when", "using", "an", "ASL", "writing", "system.‌"]}, "context": {"raw": "Designing an Animated Character System for American Sign Language Figure 3: How people communicate in ASL a) digitally, b) when taking notes, and c) when using an ASL writing system.‌", "tokens": ["Designing", "an", "Animated", "Character", "System", "for", "American", "Sign", "Language", "Figure", "3", ":", "How", "people", "communicate", "in", "ASL", "a", ")", "digitally", ",", "b", ")", "when", "taking", "notes", ",", "and", "c", ")", "when", "using", "an", "ASL", "writing", "system.‌"]}, "filename": "8209b931c94fea5d107e6ef2461b64e00fd52249_Image_005.jpg", "orig_filename": "8209b931c94fea5d107e6ef2461b64e00fd52249", "split": "train"}, {"article_id": "Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information", "description": {"raw": "(a)(b): ClassBeacons system (c): each lamp subtly depicts how long the teacher has been around it by changing from yellow (no time spent) to green (440 seconds spent) (d):  the system supports teachers’ reflection-in-action on how they have divided time and attention over students in the classroom (e): the information display is based on teachers’ real-time positioning data", "tokens": ["(", "a", ")", "(", "b", ")", ":", "ClassBeacons", "system", "(", "c", ")", ":", "each", "lamp", "subtly", "depicts", "how", "long", "the", "teacher", "has", "been", "around", "it", "by", "changing", "from", "yellow", "(", "no", "time", "spent", ")", "to", "green", "(", "440", "seconds", "spent", ")", "(", "d", ")", ":", "the", "system", "supports", "teachers", "’", "reflection-in-action", "on", "how", "they", "have", "divided", "time", "and", "attention", "over", "students", "in", "the", "classroom", "(", "e", ")", ":", "the", "information", "display", "is", "based", "on", "teachers", "’", "real-time", "positioning", "data"]}, "caption": {"raw": "Figure 1. (a)(b): ClassBeacons system (c): each lamp depicts how long the teacher has been around it by changing from yellow (no time spent) to green (440 seconds spent) (d): the system supports teachers’ reflection-in-action on how they have divided time and attention over students in the classroom (e): the display is based on teachers’ real-time positioning data.", "tokens": ["Figure", "1", ".", "(", "a", ")", "(", "b", ")", ":", "ClassBeacons", "system", "(", "c", ")", ":", "each", "lamp", "depicts", "how", "long", "the", "teacher", "has", "been", "around", "it", "by", "changing", "from", "yellow", "(", "no", "time", "spent", ")", "to", "green", "(", "440", "seconds", "spent", ")", "(", "d", ")", ":", "the", "system", "supports", "teachers", "’", "reflection-in-action", "on", "how", "they", "have", "divided", "time", "and", "attention", "over", "students", "in", "the", "classroom", "(", "e", ")", ":", "the", "display", "is", "based", "on", "teachers", "’", "real-time", "positioning", "data", "."]}, "context": {"raw": "Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information Figure 1. (a)(b): ClassBeacons system (c): each lamp depicts how long the teacher has been around it by changing from yellow (no time spent) to green (440 seconds spent) (d): the system supports teachers’ reflection-in-action on how they have divided time and attention over students in the classroom (e): the display is based on teachers’ real-time positioning data.", "tokens": ["Unobtrusively", "Enhancing", "Reflection-in-Action", "of", "Teachers", "through", "Spatially", "Distributed", "Ambient", "Information", "Figure", "1", ".", "(", "a", ")", "(", "b", ")", ":", "ClassBeacons", "system", "(", "c", ")", ":", "each", "lamp", "depicts", "how", "long", "the", "teacher", "has", "been", "around", "it", "by", "changing", "from", "yellow", "(", "no", "time", "spent", ")", "to", "green", "(", "440", "seconds", "spent", ")", "(", "d", ")", ":", "the", "system", "supports", "teachers", "’", "reflection-in-action", "on", "how", "they", "have", "divided", "time", "and", "attention", "over", "students", "in", "the", "classroom", "(", "e", ")", ":", "the", "display", "is", "based", "on", "teachers", "’", "real-time", "positioning", "data", "."]}, "filename": "a9acd1907c8be605fb24753d4209d44ca82d4b26_Image_001.jpg", "orig_filename": "a9acd1907c8be605fb24753d4209d44ca82d4b26", "split": "train"}, {"article_id": "A readability evaluation of real-time crowd captions in the classroom", "description": {"raw": "Figure 5: A graph of the latencies for each transcript (professional, automatic and crowd). Professional and crowd captions have latencies under 5 seconds which allows students to keep up with the lecture.", "tokens": ["Figure", "5", ":", "A", "graph", "of", "the", "latencies", "for", "each", "transcript", "(", "professional", ",", "automatic", "and", "crowd", ")", ".", "Professional", "and", "crowd", "captions", "have", "latencies", "under", "5", "seconds", "which", "allows", "students", "to", "keep", "up", "with", "the", "lecture", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "A readability evaluation of real-time crowd captions in the classroom ", "tokens": ["A", "readability", "evaluation", "of", "real-time", "crowd", "captions", "in", "the", "classroom"]}, "filename": "5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc_Image_008.png", "orig_filename": "5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc", "split": "train"}, {"article_id": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "description": {"raw": "A line graph showing 4 curves, one each for HMD position, HMD angle, Controller position, and Controller Angle. HMD curves are a tiny bit apart showing a lower value up until about 60%. After this the Controller angle drops below the other curves. The Controller position does as well, but not as significantly.", "tokens": ["A", "line", "graph", "showing", "4", "curves", ",", "one", "each", "for", "HMD", "position", ",", "HMD", "angle", ",", "Controller", "position", ",", "and", "Controller", "Angle", ".", "HMD", "curves", "are", "a", "tiny", "bit", "apart", "showing", "a", "lower", "value", "up", "until", "about", "60", "%", ".", "After", "this", "the", "Controller", "angle", "drops", "below", "the", "other", "curves", ".", "The", "Controller", "position", "does", "as", "well", ",", "but", "not", "as", "significantly", "."]}, "caption": {"raw": "Figure 12. The prediction accuracy derived from each input channel at different stages of the movement.", "tokens": ["Figure", "12", ".", "The", "prediction", "accuracy", "derived", "from", "each", "input", "channel", "at", "different", "stages", "of", "the", "movement", "."]}, "context": {"raw": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR Figure 12. The prediction accuracy derived from each input channel at different stages of the movement.", "tokens": ["Head-Coupled", "Kinematic", "Template", "Matching", ":", "A", "Prediction", "Model", "for", "Ray", "Pointing", "in", "VR", "Figure", "12", ".", "The", "prediction", "accuracy", "derived", "from", "each", "input", "channel", "at", "different", "stages", "of", "the", "movement", "."]}, "filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_013.jpg", "orig_filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "split": "train"}, {"article_id": "Assessment of Semantic Taxonomies for Blind Indoor Navigation Based on a Shopping Center Use Case", "description": {"raw": "Bar chart showing mean ratings about usefulness of vocal messages, grouped by previous experience of participants with smartphones and voice navigation apps. Values are explained along subsection 3.3 entitled subjective ratings.", "tokens": ["Bar", "chart", "showing", "mean", "ratings", "about", "usefulness", "of", "vocal", "messages", ",", "grouped", "by", "previous", "experience", "of", "participants", "with", "smartphones", "and", "voice", "navigation", "apps", ".", "Values", "are", "explained", "along", "subsection", "3.3", "entitled", "subjective", "ratings", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Assessment of Semantic Taxonomies for Blind Indoor Navigation Based on a Shopping Center Use Case ", "tokens": ["Assessment", "of", "Semantic", "Taxonomies", "for", "Blind", "Indoor", "Navigation", "Based", "on", "a", "Shopping", "Center", "Use", "Case"]}, "filename": "ca084360fd522adf8970c75d12cef70a4865d10f_Image_003.jpg", "orig_filename": "ca084360fd522adf8970c75d12cef70a4865d10f", "split": "train"}, {"article_id": "MEMEography: Understanding Users Through Internet Memes", "description": {"raw": "Column chart of amount of posts (ranging from 0 to about 300) over the time span 2015 until 2020, where bars show increasing numbers of posts over time. Stacked bars differentiate still image posts from video posts. In all cases the amount of video posts is lower than the amount of still image posts.", "tokens": ["Column", "chart", "of", "amount", "of", "posts", "(", "ranging", "from", "0", "to", "about", "300", ")", "over", "the", "time", "span", "2015", "until", "2020", ",", "where", "bars", "show", "increasing", "numbers", "of", "posts", "over", "time", ".", "Stacked", "bars", "differentiate", "still", "image", "posts", "from", "video", "posts", ".", "In", "all", "cases", "the", "amount", "of", "video", "posts", "is", "lower", "than", "the", "amount", "of", "still", "image", "posts", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "MEMEography: Understanding Users Through Internet Memes ", "tokens": ["MEMEography", ":", "Understanding", "Users", "Through", "Internet", "Memes"]}, "filename": "9683f0243631f6b8ecaa8bece24ea0fb848f2dde_Image_003.jpg", "orig_filename": "9683f0243631f6b8ecaa8bece24ea0fb848f2dde", "split": "train"}, {"article_id": "Effect of target size on non-visual text-entry", "description": {"raw": "The graph shows a gradual increase of about 10% of the amount of total errors from large to tiny. On the other hand, the uncorrected error rate remains constant.", "tokens": ["The", "graph", "shows", "a", "gradual", "increase", "of", "about", "10", "%", "of", "the", "amount", "of", "total", "errors", "from", "large", "to", "tiny", ".", "On", "the", "other", "hand", ",", "the", "uncorrected", "error", "rate", "remains", "constant", "."]}, "caption": {"raw": "Figure 4 - Total and Uncorrected error rate of the four conditions.", "tokens": ["Figure", "4", "-", "Total", "and", "Uncorrected", "error", "rate", "of", "the", "four", "conditions", "."]}, "context": {"raw": "Effect of target size on non-visual text-entry Figure 4 - Total and Uncorrected error rate of the four conditions.", "tokens": ["Effect", "of", "target", "size", "on", "non-visual", "text-entry", "Figure", "4", "-", "Total", "and", "Uncorrected", "error", "rate", "of", "the", "four", "conditions", "."]}, "filename": "f65de96c15e33484bc85079b354f053fd9fe8ccc_Image_004.jpg", "orig_filename": "f65de96c15e33484bc85079b354f053fd9fe8ccc", "split": "train"}, {"article_id": "Galileo: Citizen-led Experimentation Using a Social Computing System", "description": {"raw": "Experiment results from 3 experiments. A boxplot comparison of people's stool consistency when they drank kombucha vs those who did not. No kombucha's stool consistency mean is 4.2 while yes kombucha's is 3.9. Two more graphs showing regression line and bar plots.", "tokens": ["Experiment", "results", "from", "3", "experiments", ".", "A", "boxplot", "comparison", "of", "people", "'s", "stool", "consistency", "when", "they", "drank", "kombucha", "vs", "those", "who", "did", "not", ".", "No", "kombucha", "'s", "stool", "consistency", "mean", "is", "4.2", "while", "yes", "kombucha", "'s", "is", "3.9", ".", "Two", "more", "graphs", "showing", "regression", "line", "and", "bar", "plots", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Galileo: Citizen-led Experimentation Using a Social Computing System ", "tokens": ["Galileo", ":", "Citizen-led", "Experimentation", "Using", "a", "Social", "Computing", "System"]}, "filename": "8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_012.jpg", "orig_filename": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "split": "train"}, {"article_id": "Optimizing Portrait Lighting at Capture-Time Using a 360 Camera as a Light Probe", "description": {"raw": "Figure 4 shows graphs of the image quality metric for two scenes. For each graph, it shows an image/PRT render pair that correspond to min, median, and max image quality metric scores.", "tokens": ["Figure", "4", "shows", "graphs", "of", "the", "image", "quality", "metric", "for", "two", "scenes", ".", "For", "each", "graph", ",", "it", "shows", "an", "image/PRT", "render", "pair", "that", "correspond", "to", "min", ",", "median", ",", "and", "max", "image", "quality", "metric", "scores", "."]}, "caption": {"raw": "Figure 4. The image quality metric varies as we rotate the environment. Maximizing the metric produces a good match (both photo and PRT render) to the target facial appearance and the bright/dark areas in the environment map match those in the pre-integrated target weighting function Ft . At the median and minimum metric values the matches are not as good.", "tokens": ["Figure", "4", ".", "The", "image", "quality", "metric", "varies", "as", "we", "rotate", "the", "environment", ".", "Maximizing", "the", "metric", "produces", "a", "good", "match", "(", "both", "photo", "and", "PRT", "render", ")", "to", "the", "target", "facial", "appearance", "and", "the", "bright/dark", "areas", "in", "the", "environment", "map", "match", "those", "in", "the", "pre-integrated", "target", "weighting", "function", "Ft", ".", "At", "the", "median", "and", "minimum", "metric", "values", "the", "matches", "are", "not", "as", "good", "."]}, "context": {"raw": "Optimizing Portrait Lighting at Capture-Time Using a 360 Camera as a Light Probe Figure 4. The image quality metric varies as we rotate the environment. Maximizing the metric produces a good match (both photo and PRT render) to the target facial appearance and the bright/dark areas in the environment map match those in the pre-integrated target weighting function Ft . At the median and minimum metric values the matches are not as good.", "tokens": ["Optimizing", "Portrait", "Lighting", "at", "Capture-Time", "Using", "a", "360", "Camera", "as", "a", "Light", "Probe", "Figure", "4", ".", "The", "image", "quality", "metric", "varies", "as", "we", "rotate", "the", "environment", ".", "Maximizing", "the", "metric", "produces", "a", "good", "match", "(", "both", "photo", "and", "PRT", "render", ")", "to", "the", "target", "facial", "appearance", "and", "the", "bright/dark", "areas", "in", "the", "environment", "map", "match", "those", "in", "the", "pre-integrated", "target", "weighting", "function", "Ft", ".", "At", "the", "median", "and", "minimum", "metric", "values", "the", "matches", "are", "not", "as", "good", "."]}, "filename": "7ea75488547826945f768cd13897b7c559139a8d_Image_004.png", "orig_filename": "7ea75488547826945f768cd13897b7c559139a8d", "split": "train"}, {"article_id": "Hands Holding Clues for Object Recognition in Teachable Machines", "description": {"raw": "The figure consists of three graphs that show the average accuracy depending on the sample size during training, referred to as k-shot where k is the number of images used for training --- three sample sizes (20, 5, 1).  When using 20 images per object for training, the model achieves 0.618 (std: 0.114), 0.705 (std: 0.075), and 0.608 (std: 0.098) on the HO, CO, and O methods with the B data, respectively, and 0.855 (std: 0.087), 0.888 (std: 0.064), and 0.859 (std: 0.039) on the HO, CO, and O methods with the S data, respectively.  When using 5 images per object for training, the model achieves 0.539 (std: 0.099), 0.595 (std: 0.112), and 0.550 (std: 0.098) on the HO, CO, and O methods with the B data, respectively, and 0.788 (std: 0.098), 0.818 (std: 0.058), and 0.811 (std: 0.052) on the HO, CO, and O methods with the S data, respectively.  When using 1 image per object for training, the model achieves 0.400 (std: 0.068), 0.450 (std: 0.090), and 0.424 (std: 0.053) on the HO, CO, and O methods with the B data, respectively, and 0.587 (std: 0.056), 0.614 (std: 0.094), and 0.611 (std: 0.056) on the HO, CO, and O methods with the S data, respectively.", "tokens": ["The", "figure", "consists", "of", "three", "graphs", "that", "show", "the", "average", "accuracy", "depending", "on", "the", "sample", "size", "during", "training", ",", "referred", "to", "as", "k-shot", "where", "k", "is", "the", "number", "of", "images", "used", "for", "training", "--", "-", "three", "sample", "sizes", "(", "20", ",", "5", ",", "1", ")", ".", "When", "using", "20", "images", "per", "object", "for", "training", ",", "the", "model", "achieves", "0.618", "(", "std", ":", "0.114", ")", ",", "0.705", "(", "std", ":", "0.075", ")", ",", "and", "0.608", "(", "std", ":", "0.098", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "methods", "with", "the", "B", "data", ",", "respectively", ",", "and", "0.855", "(", "std", ":", "0.087", ")", ",", "0.888", "(", "std", ":", "0.064", ")", ",", "and", "0.859", "(", "std", ":", "0.039", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "methods", "with", "the", "S", "data", ",", "respectively", ".", "When", "using", "5", "images", "per", "object", "for", "training", ",", "the", "model", "achieves", "0.539", "(", "std", ":", "0.099", ")", ",", "0.595", "(", "std", ":", "0.112", ")", ",", "and", "0.550", "(", "std", ":", "0.098", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "methods", "with", "the", "B", "data", ",", "respectively", ",", "and", "0.788", "(", "std", ":", "0.098", ")", ",", "0.818", "(", "std", ":", "0.058", ")", ",", "and", "0.811", "(", "std", ":", "0.052", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "methods", "with", "the", "S", "data", ",", "respectively", ".", "When", "using", "1", "image", "per", "object", "for", "training", ",", "the", "model", "achieves", "0.400", "(", "std", ":", "0.068", ")", ",", "0.450", "(", "std", ":", "0.090", ")", ",", "and", "0.424", "(", "std", ":", "0.053", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "methods", "with", "the", "B", "data", ",", "respectively", ",", "and", "0.587", "(", "std", ":", "0.056", ")", ",", "0.614", "(", "std", ":", "0.094", ")", ",", "and", "0.611", "(", "std", ":", "0.056", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "methods", "with", "the", "S", "data", ",", "respectively", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Hands Holding Clues for Object Recognition in Teachable Machines ", "tokens": ["Hands", "Holding", "Clues", "for", "Object", "Recognition", "in", "Teachable", "Machines"]}, "filename": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_088.jpg", "orig_filename": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "split": "train"}, {"article_id": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults", "description": {"raw": "Description: image of a chart that has automatic adapation and no notification selected", "tokens": ["Description", ":", "image", "of", "a", "chart", "that", "has", "automatic", "adapation", "and", "no", "notification", "selected"]}, "caption": {"raw": "Table 4. Personas representing three common preferences towards notification and automatic adaptations.", "tokens": ["Table", "4", ".", "Personas", "representing", "three", "common", "preferences", "towards", "notification", "and", "automatic", "adaptations", "."]}, "context": {"raw": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults Table 4. Personas representing three common preferences towards notification and automatic adaptations.", "tokens": ["Understanding", "design", "considerations", "for", "adaptive", "user", "interfaces", "for", "accessible", "pointing", "with", "older", "and", "younger", "adults", "Table", "4", ".", "Personas", "representing", "three", "common", "preferences", "towards", "notification", "and", "automatic", "adaptations", "."]}, "filename": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9_Image_008.png", "orig_filename": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9", "split": "train"}, {"article_id": "Comparing Methods of Displaying Language Feedback for Student Videos of American Sign Language", "description": {"raw": "Box plot of ASL Instructor Score Improvement from Round 1 to Round 1.  Comparison of the VIDEO condition and the NOTES + POPUP condition.  VIDEO median 0.05 quartiles 0 and 0.5. NOTES + POPUP median 0.5 quartiles 1.5 and 0.", "tokens": ["Box", "plot", "of", "ASL", "Instructor", "Score", "Improvement", "from", "Round", "1", "to", "Round", "1", ".", "Comparison", "of", "the", "VIDEO", "condition", "and", "the", "NOTES", "+", "POPUP", "condition", ".", "VIDEO", "median", "0.05", "quartiles", "0", "and", "0.5", ".", "NOTES", "+", "POPUP", "median", "0.5", "quartiles", "1.5", "and", "0", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Comparing Methods of Displaying Language Feedback for Student Videos of American Sign Language ", "tokens": ["Comparing", "Methods", "of", "Displaying", "Language", "Feedback", "for", "Student", "Videos", "of", "American", "Sign", "Language"]}, "filename": "0907dfb73fbc31f4f0035aa7f391bbeab2c55bd4_Image_007.jpg", "orig_filename": "0907dfb73fbc31f4f0035aa7f391bbeab2c55bd4", "split": "train"}, {"article_id": "Real-Time Mobile Personalized Simulations of Impaired Colour Vision", "description": {"raw": "Line graph showing histogram of differences between personalized and adjustable simulation  results for protan and deutan simulations. Differences measured in CIE Luv units) Both lines rise sharply from 0 and drop sharply at 20 units, with a long tail to around 65 units.", "tokens": ["Line", "graph", "showing", "histogram", "of", "differences", "between", "personalized", "and", "adjustable", "simulation", "results", "for", "protan", "and", "deutan", "simulations", ".", "Differences", "measured", "in", "CIE", "Luv", "units", ")", "Both", "lines", "rise", "sharply", "from", "0", "and", "drop", "sharply", "at", "20", "units", ",", "with", "a", "long", "tail", "to", "around", "65", "units", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Real-Time Mobile Personalized Simulations of Impaired Colour Vision ", "tokens": ["Real-Time", "Mobile", "Personalized", "Simulations", "of", "Impaired", "Colour", "Vision"]}, "filename": "db59784588a97b0a1147fab50c76a495253232ff_Image_004.png", "orig_filename": "db59784588a97b0a1147fab50c76a495253232ff", "split": "train"}, {"article_id": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation", "description": {"raw": "SHAP feature importance measured as the mean absolute Shapley values. The rating of how much a participant desired to turn PIV vibrations off was the most important feature, changing the predicted anxiety level on average by 1.42 points. Right: SHAP summary plot showing the importance and the effect of features. Low numbers of willingness to turn-off vibrations contribute to anxiety drop, and large numbers to increase in anxiety.", "tokens": ["SHAP", "feature", "importance", "measured", "as", "the", "mean", "absolute", "Shapley", "values", ".", "The", "rating", "of", "how", "much", "a", "participant", "desired", "to", "turn", "PIV", "vibrations", "off", "was", "the", "most", "important", "feature", ",", "changing", "the", "predicted", "anxiety", "level", "on", "average", "by", "1.42", "points", ".", "Right", ":", "SHAP", "summary", "plot", "showing", "the", "importance", "and", "the", "effect", "of", "features", ".", "Low", "numbers", "of", "willingness", "to", "turn-off", "vibrations", "contribute", "to", "anxiety", "drop", ",", "and", "large", "numbers", "to", "increase", "in", "anxiety", "."]}, "caption": {"raw": "Figure 6. Left: SHAP feature importance measured as the mean absolute Shapley values. The rating of how much a participant desired to turn PIV vibrations off was the most important feature, changing the predicted anxiety level on average by 1.42 points. Right: SHAP summary plot showing the importance and the effect of features. Low numbers of willingness to turn-off vibrations contribute to anxiety drop, and large numbers to increase in anxiety.", "tokens": ["Figure", "6", ".", "Left", ":", "SHAP", "feature", "importance", "measured", "as", "the", "mean", "absolute", "Shapley", "values", ".", "The", "rating", "of", "how", "much", "a", "participant", "desired", "to", "turn", "PIV", "vibrations", "off", "was", "the", "most", "important", "feature", ",", "changing", "the", "predicted", "anxiety", "level", "on", "average", "by", "1.42", "points", ".", "Right", ":", "SHAP", "summary", "plot", "showing", "the", "importance", "and", "the", "effect", "of", "features", ".", "Low", "numbers", "of", "willingness", "to", "turn-off", "vibrations", "contribute", "to", "anxiety", "drop", ",", "and", "large", "numbers", "to", "increase", "in", "anxiety", "."]}, "context": {"raw": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation Figure 6. Left: SHAP feature importance measured as the mean absolute Shapley values. The rating of how much a participant desired to turn PIV vibrations off was the most important feature, changing the predicted anxiety level on average by 1.42 points. Right: SHAP summary plot showing the importance and the effect of features. Low numbers of willingness to turn-off vibrations contribute to anxiety drop, and large numbers to increase in anxiety.", "tokens": ["Evaluating", "a", "Personalizable", ",", "Inconspicuous", "Vibrotactile", "(", "PIV", ")", "Breathing", "Pacer", "for", "In-the-Moment", "Affect", "Regulation", "Figure", "6", ".", "Left", ":", "SHAP", "feature", "importance", "measured", "as", "the", "mean", "absolute", "Shapley", "values", ".", "The", "rating", "of", "how", "much", "a", "participant", "desired", "to", "turn", "PIV", "vibrations", "off", "was", "the", "most", "important", "feature", ",", "changing", "the", "predicted", "anxiety", "level", "on", "average", "by", "1.42", "points", ".", "Right", ":", "SHAP", "summary", "plot", "showing", "the", "importance", "and", "the", "effect", "of", "features", ".", "Low", "numbers", "of", "willingness", "to", "turn-off", "vibrations", "contribute", "to", "anxiety", "drop", ",", "and", "large", "numbers", "to", "increase", "in", "anxiety", "."]}, "filename": "031c5426862746aeef25ca784afa9191fcdb0eff_Image_012.jpg", "orig_filename": "031c5426862746aeef25ca784afa9191fcdb0eff", "split": "train"}, {"article_id": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults", "description": {"raw": "Description: image of a chart that has manual adaptation and notification selected", "tokens": ["Description", ":", "image", "of", "a", "chart", "that", "has", "manual", "adaptation", "and", "notification", "selected"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults ", "tokens": ["Understanding", "design", "considerations", "for", "adaptive", "user", "interfaces", "for", "accessible", "pointing", "with", "older", "and", "younger", "adults"]}, "filename": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9_Image_010.png", "orig_filename": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9", "split": "train"}, {"article_id": "Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching", "description": {"raw": "This graph depicts a series of boxplots for the distrubution of the average number of concurrent touches per trial for each participant. Key takeaway: each participant, except P4, averages more than 1 concurrent touch. The highest average is P10 with an average of about five and a half concurrent touches. Many of the participants average between 2 to 4 concurrent touches.", "tokens": ["This", "graph", "depicts", "a", "series", "of", "boxplots", "for", "the", "distrubution", "of", "the", "average", "number", "of", "concurrent", "touches", "per", "trial", "for", "each", "participant", ".", "Key", "takeaway", ":", "each", "participant", ",", "except", "P4", ",", "averages", "more", "than", "1", "concurrent", "touch", ".", "The", "highest", "average", "is", "P10", "with", "an", "average", "of", "about", "five", "and", "a", "half", "concurrent", "touches", ".", "Many", "of", "the", "participants", "average", "between", "2", "to", "4", "concurrent", "touches", "."]}, "caption": {"raw": "Figure 4. Distribution of the average number of concurrent touches per trial. Due to their touch behavior, participants impacted the screen with various parts of the hand, resulting in multiple registered touches.", "tokens": ["Figure", "4", ".", "Distribution", "of", "the", "average", "number", "of", "concurrent", "touches", "per", "trial", ".", "Due", "to", "their", "touch", "behavior", ",", "participants", "impacted", "the", "screen", "with", "various", "parts", "of", "the", "hand", ",", "resulting", "in", "multiple", "registered", "touches", "."]}, "context": {"raw": "Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching Figure 4. Distribution of the average number of concurrent touches per trial. Due to their touch behavior, participants impacted the screen with various parts of the hand, resulting in multiple registered touches.", "tokens": ["Smart", "Touch", ":", "Improving", "Touch", "Accuracy", "for", "People", "with", "Motor", "Impairments", "with", "Template", "Matching", "Figure", "4", ".", "Distribution", "of", "the", "average", "number", "of", "concurrent", "touches", "per", "trial", ".", "Due", "to", "their", "touch", "behavior", ",", "participants", "impacted", "the", "screen", "with", "various", "parts", "of", "the", "hand", ",", "resulting", "in", "multiple", "registered", "touches", "."]}, "filename": "4e36e8637a6e0892eb2d6cda79537a5e0b189968_Image_004.jpg", "orig_filename": "4e36e8637a6e0892eb2d6cda79537a5e0b189968", "split": "train"}, {"article_id": "HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications", "description": {"raw": "Depiction of the color selection tool. In 2D the tool consists of a circle of varying hues and a bar undearneath it to select saturation. For the 3D version the circle is a cylinder whose height is mapped to the saturation value and the user can interact with it directly by pushing their finger into the cylinder.", "tokens": ["Depiction", "of", "the", "color", "selection", "tool", ".", "In", "2D", "the", "tool", "consists", "of", "a", "circle", "of", "varying", "hues", "and", "a", "bar", "undearneath", "it", "to", "select", "saturation", ".", "For", "the", "3D", "version", "the", "circle", "is", "a", "cylinder", "whose", "height", "is", "mapped", "to", "the", "saturation", "value", "and", "the", "user", "can", "interact", "with", "it", "directly", "by", "pushing", "their", "finger", "into", "the", "cylinder", "."]}, "caption": {"raw": "Figure 10: a) The 2D color palette. b) The full 3D color palette. c) An illustration of the cutaway behavior for the 3D color palette dur- ing direct interaction.", "tokens": ["Figure", "10", ":", "a", ")", "The", "2D", "color", "palette", ".", "b", ")", "The", "full", "3D", "color", "palette", ".", "c", ")", "An", "illustration", "of", "the", "cutaway", "behavior", "for", "the", "3D", "color", "palette", "dur-", "ing", "direct", "interaction", "."]}, "context": {"raw": "HybridSpace: Integrating 3D freehand input and stereo viewing into traditional desktop applications Figure 10: a) The 2D color palette. b) The full 3D color palette. c) An illustration of the cutaway behavior for the 3D color palette dur- ing direct interaction.", "tokens": ["HybridSpace", ":", "Integrating", "3D", "freehand", "input", "and", "stereo", "viewing", "into", "traditional", "desktop", "applications", "Figure", "10", ":", "a", ")", "The", "2D", "color", "palette", ".", "b", ")", "The", "full", "3D", "color", "palette", ".", "c", ")", "An", "illustration", "of", "the", "cutaway", "behavior", "for", "the", "3D", "color", "palette", "dur-", "ing", "direct", "interaction", "."]}, "filename": "cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0_Image_013.jpg", "orig_filename": "cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0", "split": "train"}, {"article_id": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "description": {"raw": "The bar charts show the maximum frequencies under different tubing configurations and different force magnitude.", "tokens": ["The", "bar", "charts", "show", "the", "maximum", "frequencies", "under", "different", "tubing", "configurations", "and", "different", "force", "magnitude", "."]}, "caption": {"raw": "Figure 9: Maximum impulse frequency at diferent force magnitudes for diferent tubing sizes and tubing lengths, cal- culated based on force rise time and fall time. (The 4.0N fre- quency is unavailable for 6mm x 250cm tubing because it could only achieve a maximum force of 3.3N.)", "tokens": ["Figure", "9", ":", "Maximum", "impulse", "frequency", "at", "diferent", "force", "magnitudes", "for", "diferent", "tubing", "sizes", "and", "tubing", "lengths", ",", "cal-", "culated", "based", "on", "force", "rise", "time", "and", "fall", "time", ".", "(", "The", "4.0N", "fre-", "quency", "is", "unavailable", "for", "6mm", "x", "250cm", "tubing", "because", "it", "could", "only", "achieve", "a", "maximum", "force", "of", "3.3N", ".", ")"]}, "context": {"raw": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets Figure 9: Maximum impulse frequency at diferent force magnitudes for diferent tubing sizes and tubing lengths, cal- culated based on force rise time and fall time. (The 4.0N fre- quency is unavailable for 6mm x 250cm tubing because it could only achieve a maximum force of 3.3N.)", "tokens": ["JetController", ":", "High-speed", "Ungrounded", "3-DoF", "Force", "Feedback", "Controllers", "using", "Air", "Propulsion", "Jets", "Figure", "9", ":", "Maximum", "impulse", "frequency", "at", "diferent", "force", "magnitudes", "for", "diferent", "tubing", "sizes", "and", "tubing", "lengths", ",", "cal-", "culated", "based", "on", "force", "rise", "time", "and", "fall", "time", ".", "(", "The", "4.0N", "fre-", "quency", "is", "unavailable", "for", "6mm", "x", "250cm", "tubing", "because", "it", "could", "only", "achieve", "a", "maximum", "force", "of", "3.3N", ".", ")"]}, "filename": "abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_010.png", "orig_filename": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "split": "train"}, {"article_id": "Gesture Knitter: A Hand Gesture Design Tool for Head-Mounted Mixed Reality Applications", "description": {"raw": "Figure 2 shows the preference gallery interface for generating synthetic samples in which two synthetic samples are given to the designer. The designer is then to choose the one that provides significant enough variation without sacrificing the consistency with the original demonstration, which is depicted as an animation in the top frame.    Figure 3 shows the output of the discernability tool, which outputs a bar chart showing the negative log likelihood comparing to all other gestures of the same class. From this data, the designer can compare the negative log likelihood to the actual gesture's class to see if there are any possible recognition issues.", "tokens": ["Figure", "2", "shows", "the", "preference", "gallery", "interface", "for", "generating", "synthetic", "samples", "in", "which", "two", "synthetic", "samples", "are", "given", "to", "the", "designer", ".", "The", "designer", "is", "then", "to", "choose", "the", "one", "that", "provides", "significant", "enough", "variation", "without", "sacrificing", "the", "consistency", "with", "the", "original", "demonstration", ",", "which", "is", "depicted", "as", "an", "animation", "in", "the", "top", "frame", ".", "Figure", "3", "shows", "the", "output", "of", "the", "discernability", "tool", ",", "which", "outputs", "a", "bar", "chart", "showing", "the", "negative", "log", "likelihood", "comparing", "to", "all", "other", "gestures", "of", "the", "same", "class", ".", "From", "this", "data", ",", "the", "designer", "can", "compare", "the", "negative", "log", "likelihood", "to", "the", "actual", "gesture", "'s", "class", "to", "see", "if", "there", "are", "any", "possible", "recognition", "issues", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Gesture Knitter: A Hand Gesture Design Tool for Head-Mounted Mixed Reality Applications ", "tokens": ["Gesture", "Knitter", ":", "A", "Hand", "Gesture", "Design", "Tool", "for", "Head-Mounted", "Mixed", "Reality", "Applications"]}, "filename": "4cc90799668187b150c2647f1f8e41ff3319e49b_Image_026.jpg", "orig_filename": "4cc90799668187b150c2647f1f8e41ff3319e49b", "split": "train"}, {"article_id": "Passively-sensed Behavioral Correlates of Discrimination Events in College Students", "description": {"raw": "scatter plot and correlation line between social support and depression for people who have reported discrimination and those who have not", "tokens": ["scatter", "plot", "and", "correlation", "line", "between", "social", "support", "and", "depression", "for", "people", "who", "have", "reported", "discrimination", "and", "those", "who", "have", "not"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Passively-sensed Behavioral Correlates of Discrimination Events in College Students ", "tokens": ["Passively-sensed", "Behavioral", "Correlates", "of", "Discrimination", "Events", "in", "College", "Students"]}, "filename": "fa83fb659bbf9efb4b4a56e6a933c155c87c201e_Image_013.png", "orig_filename": "fa83fb659bbf9efb4b4a56e6a933c155c87c201e", "split": "train"}, {"article_id": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "description": {"raw": "This figure shows the demonstrations of JetController and a bar chart to display the impulse frequencies of several scenarios.", "tokens": ["This", "figure", "shows", "the", "demonstrations", "of", "JetController", "and", "a", "bar", "chart", "to", "display", "the", "impulse", "frequencies", "of", "several", "scenarios", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets ", "tokens": ["JetController", ":", "High-speed", "Ungrounded", "3-DoF", "Force", "Feedback", "Controllers", "using", "Air", "Propulsion", "Jets"]}, "filename": "abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_001.jpg", "orig_filename": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "split": "train"}, {"article_id": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "description": {"raw": "Figure 1 is a chart illustrating participants' ratings of the usefulness of the information they verbally shared while working. The chart has 6 mini bar charts. On the left are 3 mini bar charts for the concurrent think-aloud condition, and on the right are 3 mini bar charts for the retrospective think-aloud condition. For each condition, there is a mini bar chart per domain (i.e., coding, models, and slides). First we describe the concurrent think-aloud ratings: For concurrent think-aloud for the coding domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 3 (i.e., 3 participants out of 4) for \"moderately useful\", and a blue bar of height 1 for \"very useful\". For concurrent think-aloud for the models domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 4 for \"moderately useful\", and a bar of height 0 for \"very useful\". For concurrent think-aloud for the slides domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 2 for \"moderately useful\", and a blue bar of height 2 for \"very useful\". Next we describe the retrospective think-aloud ratings: For retrospective think-aloud for the coding domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 3 for \"moderately useful\", and a blue bar of height 1 for \"very useful\". For retrospective think-aloud for the models domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 4 for \"moderately useful\", and a bar of height 0 for \"very useful\". For retrospective think-aloud for the slides domain, there are 3 bars, from left: a bar of height 0 for \"not useful\", a pink bar of height 1 for \"moderately useful\", and a blue bar of height 3 for \"very useful\". Note that these categories are aggregations of the participant ratings from the 7-point Likert scale results (between 1-Not Useful, and 7-Extremely Useful). Ratings (1, 2) have been aggregated into \"not useful\", ratings (3, 4, 5) have been aggregated into \"moderately useful\", and ratings (6, 7) have been aggregated into \"very useful\".", "tokens": ["Figure", "1", "is", "a", "chart", "illustrating", "participants", "'", "ratings", "of", "the", "usefulness", "of", "the", "information", "they", "verbally", "shared", "while", "working", ".", "The", "chart", "has", "6", "mini", "bar", "charts", ".", "On", "the", "left", "are", "3", "mini", "bar", "charts", "for", "the", "concurrent", "think-aloud", "condition", ",", "and", "on", "the", "right", "are", "3", "mini", "bar", "charts", "for", "the", "retrospective", "think-aloud", "condition", ".", "For", "each", "condition", ",", "there", "is", "a", "mini", "bar", "chart", "per", "domain", "(", "i.e.", ",", "coding", ",", "models", ",", "and", "slides", ")", ".", "First", "we", "describe", "the", "concurrent", "think-aloud", "ratings", ":", "For", "concurrent", "think-aloud", "for", "the", "coding", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "a", "bar", "of", "height", "0", "for", "``", "not", "useful", "''", ",", "a", "pink", "bar", "of", "height", "3", "(", "i.e.", ",", "3", "participants", "out", "of", "4", ")", "for", "``", "moderately", "useful", "''", ",", "and", "a", "blue", "bar", "of", "height", "1", "for", "``", "very", "useful", "''", ".", "For", "concurrent", "think-aloud", "for", "the", "models", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "a", "bar", "of", "height", "0", "for", "``", "not", "useful", "''", ",", "a", "pink", "bar", "of", "height", "4", "for", "``", "moderately", "useful", "''", ",", "and", "a", "bar", "of", "height", "0", "for", "``", "very", "useful", "''", ".", "For", "concurrent", "think-aloud", "for", "the", "slides", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "a", "bar", "of", "height", "0", "for", "``", "not", "useful", "''", ",", "a", "pink", "bar", "of", "height", "2", "for", "``", "moderately", "useful", "''", ",", "and", "a", "blue", "bar", "of", "height", "2", "for", "``", "very", "useful", "''", ".", "Next", "we", "describe", "the", "retrospective", "think-aloud", "ratings", ":", "For", "retrospective", "think-aloud", "for", "the", "coding", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "a", "bar", "of", "height", "0", "for", "``", "not", "useful", "''", ",", "a", "pink", "bar", "of", "height", "3", "for", "``", "moderately", "useful", "''", ",", "and", "a", "blue", "bar", "of", "height", "1", "for", "``", "very", "useful", "''", ".", "For", "retrospective", "think-aloud", "for", "the", "models", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "a", "bar", "of", "height", "0", "for", "``", "not", "useful", "''", ",", "a", "pink", "bar", "of", "height", "4", "for", "``", "moderately", "useful", "''", ",", "and", "a", "bar", "of", "height", "0", "for", "``", "very", "useful", "''", ".", "For", "retrospective", "think-aloud", "for", "the", "slides", "domain", ",", "there", "are", "3", "bars", ",", "from", "left", ":", "a", "bar", "of", "height", "0", "for", "``", "not", "useful", "''", ",", "a", "pink", "bar", "of", "height", "1", "for", "``", "moderately", "useful", "''", ",", "and", "a", "blue", "bar", "of", "height", "3", "for", "``", "very", "useful", "''", ".", "Note", "that", "these", "categories", "are", "aggregations", "of", "the", "participant", "ratings", "from", "the", "7-point", "Likert", "scale", "results", "(", "between", "1-Not", "Useful", ",", "and", "7-Extremely", "Useful", ")", ".", "Ratings", "(", "1", ",", "2", ")", "have", "been", "aggregated", "into", "``", "not", "useful", "''", ",", "ratings", "(", "3", ",", "4", ",", "5", ")", "have", "been", "aggregated", "into", "``", "moderately", "useful", "''", ",", "and", "ratings", "(", "6", ",", "7", ")", "have", "been", "aggregated", "into", "``", "very", "useful", "''", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture ", "tokens": ["Think-Aloud", "Computing", ":", "Supporting", "Rich", "and", "Low-Effort", "Knowledge", "Capture"]}, "filename": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_001.jpg", "orig_filename": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "split": "train"}, {"article_id": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "description": {"raw": "The figure is composed of four images. The first two represent a state where the instructor changed slide and is waiting for the interpreter to finish. The presenter view is grayed out and shows a red progress bar. The mobile app shows a big red button. The remaining two images represent a state where the interpreter tapped on the mobile screen indicating that s/he is finished with the interpretation. The presenter view is grayed out and shows an orange progress bar. The mobile app show a gray button.", "tokens": ["The", "figure", "is", "composed", "of", "four", "images", ".", "The", "first", "two", "represent", "a", "state", "where", "the", "instructor", "changed", "slide", "and", "is", "waiting", "for", "the", "interpreter", "to", "finish", ".", "The", "presenter", "view", "is", "grayed", "out", "and", "shows", "a", "red", "progress", "bar", ".", "The", "mobile", "app", "shows", "a", "big", "red", "button", ".", "The", "remaining", "two", "images", "represent", "a", "state", "where", "the", "interpreter", "tapped", "on", "the", "mobile", "screen", "indicating", "that", "s/he", "is", "finished", "with", "the", "interpretation", ".", "The", "presenter", "view", "is", "grayed", "out", "and", "shows", "an", "orange", "progress", "bar", ".", "The", "mobile", "app", "show", "a", "gray", "button", "."]}, "caption": {"raw": "Figure 2. From left to right: presenter view waiting for interpreter to finish; mobile app is waiting for interpreter input to signal that interpretation is finished; presenter view waiting for students to read slide content; mobile app is inactive.", "tokens": ["Figure", "2", ".", "From", "left", "to", "right", ":", "presenter", "view", "waiting", "for", "interpreter", "to", "finish", ";", "mobile", "app", "is", "waiting", "for", "interpreter", "input", "to", "signal", "that", "interpretation", "is", "finished", ";", "presenter", "view", "waiting", "for", "students", "to", "read", "slide", "content", ";", "mobile", "app", "is", "inactive", "."]}, "context": {"raw": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students Figure 2. From left to right: presenter view waiting for interpreter to finish; mobile app is waiting for interpreter input to signal that interpretation is finished; presenter view waiting for students to read slide content; mobile app is inactive.", "tokens": ["SlidePacer", ":", "A", "Presentation", "Delivery", "Tool", "for", "Instructors", "of", "Deaf", "and", "Hard", "of", "Hearing", "Students", "Figure", "2", ".", "From", "left", "to", "right", ":", "presenter", "view", "waiting", "for", "interpreter", "to", "finish", ";", "mobile", "app", "is", "waiting", "for", "interpreter", "input", "to", "signal", "that", "interpretation", "is", "finished", ";", "presenter", "view", "waiting", "for", "students", "to", "read", "slide", "content", ";", "mobile", "app", "is", "inactive", "."]}, "filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_003.png", "orig_filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "split": "train"}, {"article_id": "Making GIFs Accessible", "description": {"raw": "A histogram of unique popular GIFs we saw in our Twitter sample. The most popular GIF on the left exceeds 1000 uses, and the graph quickly tapers off after around 200 GIFs. After that, most are below 50 uses. The graph is displayed in a logarithmic scale, yet still tapers off quickly.", "tokens": ["A", "histogram", "of", "unique", "popular", "GIFs", "we", "saw", "in", "our", "Twitter", "sample", ".", "The", "most", "popular", "GIF", "on", "the", "left", "exceeds", "1000", "uses", ",", "and", "the", "graph", "quickly", "tapers", "off", "after", "around", "200", "GIFs", ".", "After", "that", ",", "most", "are", "below", "50", "uses", ".", "The", "graph", "is", "displayed", "in", "a", "logarithmic", "scale", ",", "yet", "still", "tapers", "off", "quickly", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Making GIFs Accessible ", "tokens": ["Making", "GIFs", "Accessible"]}, "filename": "71219f2574c484c41e730c3ab758af92ac942400_Image_005.jpg", "orig_filename": "71219f2574c484c41e730c3ab758af92ac942400", "split": "train"}, {"article_id": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "description": {"raw": "This figure shows two bar charts on the left and the right, respectively.  The left bar chart shows the average accuracy of each participant's model in the vanilla test and the right bar chart shows the average of each participant's model in the wild test.  The average accuracy of the models in the vanilla test P1: 49.86% (min: 44.60%, max: 54.00%, std: 3.16) P2: 62.88% (min: 57.20%, max: 68.00%, std: 3.47) P3: 70.46% (min: 66.20%, max: 76.40%, std: 3.06) P4: 61.78% (min: 57.20%, max: 66.60%, std: 3.57) P5: 53.38% (min: 47.80%, max: 57.60%, std: 2.84) P6: 74.56% (min: 72.60%, max: 77.00%, std: 1.46) P7: 84.14% (min: 80.20%, max: 89.40%, std: 3.32) P8: 55.12% (min: 52.60%, max: 59.80%, std: 2.39) P9: 55.28% (min: 50.00%, max: 59.80%, std: 3.17) S1: 82.30% (min: 79.20%, max: 86.20%, std: 2.38) S2: 94.58% (min: 92.60%, max: 96.00%, std: 1.16)  The average accuracy of the models in the wild test P1: 28.56% (min: 24.40%, max: 33.80%, std: 3.00) P2: 34.44% (min: 29.20%, max: 38.60%, std: 3.08) P3: N/A P4: 54.32% (min: 48.00%, max: 58.00%, std: 3.37) P5: 40.72% (min: 36.40%, max: 45.60%, std: 3.51) P6: 55.82% (min: 52.20%, max: 61.00%, std: 2.22) P7: 58.08% (min: 52.00%, max: 66.00%, std: 4.14) P8: 23.00% (min: 20.00%, max: 26.80%, std: 2.35) P9: 43.28% (min: 39.80%, max: 48.80%, std: 2.97) S1: 76.68% (min: 69.80%, max: 84.00%, std: 4.74) S2: 86.18% (min: 83.20%, max: 89.00%, std: 1.83)", "tokens": ["This", "figure", "shows", "two", "bar", "charts", "on", "the", "left", "and", "the", "right", ",", "respectively", ".", "The", "left", "bar", "chart", "shows", "the", "average", "accuracy", "of", "each", "participant", "'s", "model", "in", "the", "vanilla", "test", "and", "the", "right", "bar", "chart", "shows", "the", "average", "of", "each", "participant", "'s", "model", "in", "the", "wild", "test", ".", "The", "average", "accuracy", "of", "the", "models", "in", "the", "vanilla", "test", "P1", ":", "49.86", "%", "(", "min", ":", "44.60", "%", ",", "max", ":", "54.00", "%", ",", "std", ":", "3.16", ")", "P2", ":", "62.88", "%", "(", "min", ":", "57.20", "%", ",", "max", ":", "68.00", "%", ",", "std", ":", "3.47", ")", "P3", ":", "70.46", "%", "(", "min", ":", "66.20", "%", ",", "max", ":", "76.40", "%", ",", "std", ":", "3.06", ")", "P4", ":", "61.78", "%", "(", "min", ":", "57.20", "%", ",", "max", ":", "66.60", "%", ",", "std", ":", "3.57", ")", "P5", ":", "53.38", "%", "(", "min", ":", "47.80", "%", ",", "max", ":", "57.60", "%", ",", "std", ":", "2.84", ")", "P6", ":", "74.56", "%", "(", "min", ":", "72.60", "%", ",", "max", ":", "77.00", "%", ",", "std", ":", "1.46", ")", "P7", ":", "84.14", "%", "(", "min", ":", "80.20", "%", ",", "max", ":", "89.40", "%", ",", "std", ":", "3.32", ")", "P8", ":", "55.12", "%", "(", "min", ":", "52.60", "%", ",", "max", ":", "59.80", "%", ",", "std", ":", "2.39", ")", "P9", ":", "55.28", "%", "(", "min", ":", "50.00", "%", ",", "max", ":", "59.80", "%", ",", "std", ":", "3.17", ")", "S1", ":", "82.30", "%", "(", "min", ":", "79.20", "%", ",", "max", ":", "86.20", "%", ",", "std", ":", "2.38", ")", "S2", ":", "94.58", "%", "(", "min", ":", "92.60", "%", ",", "max", ":", "96.00", "%", ",", "std", ":", "1.16", ")", "The", "average", "accuracy", "of", "the", "models", "in", "the", "wild", "test", "P1", ":", "28.56", "%", "(", "min", ":", "24.40", "%", ",", "max", ":", "33.80", "%", ",", "std", ":", "3.00", ")", "P2", ":", "34.44", "%", "(", "min", ":", "29.20", "%", ",", "max", ":", "38.60", "%", ",", "std", ":", "3.08", ")", "P3", ":", "N/A", "P4", ":", "54.32", "%", "(", "min", ":", "48.00", "%", ",", "max", ":", "58.00", "%", ",", "std", ":", "3.37", ")", "P5", ":", "40.72", "%", "(", "min", ":", "36.40", "%", ",", "max", ":", "45.60", "%", ",", "std", ":", "3.51", ")", "P6", ":", "55.82", "%", "(", "min", ":", "52.20", "%", ",", "max", ":", "61.00", "%", ",", "std", ":", "2.22", ")", "P7", ":", "58.08", "%", "(", "min", ":", "52.00", "%", ",", "max", ":", "66.00", "%", ",", "std", ":", "4.14", ")", "P8", ":", "23.00", "%", "(", "min", ":", "20.00", "%", ",", "max", ":", "26.80", "%", ",", "std", ":", "2.35", ")", "P9", ":", "43.28", "%", "(", "min", ":", "39.80", "%", ",", "max", ":", "48.80", "%", ",", "std", ":", "2.97", ")", "S1", ":", "76.68", "%", "(", "min", ":", "69.80", "%", ",", "max", ":", "84.00", "%", ",", "std", ":", "4.74", ")", "S2", ":", "86.18", "%", "(", "min", ":", "83.20", "%", ",", "max", ":", "89.00", "%", ",", "std", ":", "1.83", ")"]}, "caption": {"raw": "Figure 8: Average model accuracy per participant.", "tokens": ["Figure", "8", ":", "Average", "model", "accuracy", "per", "participant", "."]}, "context": {"raw": "Revisiting Blind Photography in the Context of Teachable Object Recognizers Figure 8: Average model accuracy per participant.", "tokens": ["Revisiting", "Blind", "Photography", "in", "the", "Context", "of", "Teachable", "Object", "Recognizers", "Figure", "8", ":", "Average", "model", "accuracy", "per", "participant", "."]}, "filename": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_025.jpg", "orig_filename": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "split": "train"}, {"article_id": "Faster Command Selection on Touchscreen Watches", "description": {"raw": "Dot plot showing how the average percentage use of one-step selection changed over the study blocks. Shows a steady progression toward higher use of one-step selection, culminating in nearly 100% use in the final six blocks.", "tokens": ["Dot", "plot", "showing", "how", "the", "average", "percentage", "use", "of", "one-step", "selection", "changed", "over", "the", "study", "blocks", ".", "Shows", "a", "steady", "progression", "toward", "higher", "use", "of", "one-step", "selection", ",", "culminating", "in", "nearly", "100", "%", "use", "in", "the", "final", "six", "blocks", "."]}, "caption": {"raw": "Figure 10. Per-participant average use of the one-step selection method for each block. Error bars indicate standard error.", "tokens": ["Figure", "10", ".", "Per-participant", "average", "use", "of", "the", "one-step", "selection", "method", "for", "each", "block", ".", "Error", "bars", "indicate", "standard", "error", "."]}, "context": {"raw": "Faster Command Selection on Touchscreen Watches Figure 10. Per-participant average use of the one-step selection method for each block. Error bars indicate standard error.", "tokens": ["Faster", "Command", "Selection", "on", "Touchscreen", "Watches", "Figure", "10", ".", "Per-participant", "average", "use", "of", "the", "one-step", "selection", "method", "for", "each", "block", ".", "Error", "bars", "indicate", "standard", "error", "."]}, "filename": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_010.jpg", "orig_filename": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "split": "train"}, {"article_id": "Gaze Guidance for Captioned Videos for DHH Users", "description": {"raw": "Box plot present percent fixation times of native and nonnative. mean fixation times of \"Nonnative\" is higher than \"Native\".", "tokens": ["Box", "plot", "present", "percent", "fixation", "times", "of", "native", "and", "nonnative", ".", "mean", "fixation", "times", "of", "``", "Nonnative", "''", "is", "higher", "than", "``", "Native", "''", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Gaze Guidance for Captioned Videos for DHH Users ", "tokens": ["Gaze", "Guidance", "for", "Captioned", "Videos", "for", "DHH", "Users"]}, "filename": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c_Image_004.jpg", "orig_filename": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c", "split": "train"}, {"article_id": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping", "description": {"raw": "A bar chart showing mean error rates across touch types and interfaces, it shows lower rates for list based area touch, especially on small and dense targets. Error bars show moderate stand deviation.", "tokens": ["A", "bar", "chart", "showing", "mean", "error", "rates", "across", "touch", "types", "and", "interfaces", ",", "it", "shows", "lower", "rates", "for", "list", "based", "area", "touch", ",", "especially", "on", "small", "and", "dense", "targets", ".", "Error", "bars", "show", "moderate", "stand", "deviation", "."]}, "caption": {"raw": "Figure 4. Mean error rates across touch types and interfaces show lower rates for list based area touch, especially on small and dense targets.", "tokens": ["Figure", "4", ".", "Mean", "error", "rates", "across", "touch", "types", "and", "interfaces", "show", "lower", "rates", "for", "list", "based", "area", "touch", ",", "especially", "on", "small", "and", "dense", "targets", "."]}, "context": {"raw": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping Figure 4. Mean error rates across touch types and interfaces show lower rates for list based area touch, especially on small and dense targets.", "tokens": ["Enhancing", "Android", "accessibility", "for", "users", "with", "hand", "tremor", "by", "reducing", "fine", "pointing", "and", "steady", "tapping", "Figure", "4", ".", "Mean", "error", "rates", "across", "touch", "types", "and", "interfaces", "show", "lower", "rates", "for", "list", "based", "area", "touch", ",", "especially", "on", "small", "and", "dense", "targets", "."]}, "filename": "dfa09494b50030571735aafbe5c114cd9ba80eba_Image_004.png", "orig_filename": "dfa09494b50030571735aafbe5c114cd9ba80eba", "split": "train"}, {"article_id": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "description": {"raw": "Figure 8: Box plots for the results of our questionnaires. Horizontal red bars represent medians, and boxes represent the interquartile ranges (IQRs). Whiskers stretch to the data points that are within the median ± 1.5 IQR.", "tokens": ["Figure", "8", ":", "Box", "plots", "for", "the", "results", "of", "our", "questionnaires", ".", "Horizontal", "red", "bars", "represent", "medians", ",", "and", "boxes", "represent", "the", "interquartile", "ranges", "(", "IQRs", ")", ".", "Whiskers", "stretch", "to", "the", "data", "points", "that", "are", "within", "the", "median", "±", "1.5", "IQR", "."]}, "caption": {"raw": "Figure 8: Box plots for the results of our questionnaires. Horizontal red bars represent medians, and boxes represent the interquartile ranges (IQRs). Whiskers stretch to the data points that are within the median ± 1.5 IQR.", "tokens": ["Figure", "8", ":", "Box", "plots", "for", "the", "results", "of", "our", "questionnaires", ".", "Horizontal", "red", "bars", "represent", "medians", ",", "and", "boxes", "represent", "the", "interquartile", "ranges", "(", "IQRs", ")", ".", "Whiskers", "stretch", "to", "the", "data", "points", "that", "are", "within", "the", "median", "±", "1.5", "IQR", "."]}, "context": {"raw": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments Figure 8: Box plots for the results of our questionnaires. Horizontal red bars represent medians, and boxes represent the interquartile ranges (IQRs). Whiskers stretch to the data points that are within the median ± 1.5 IQR.", "tokens": ["Erg-O", ":", "Ergonomic", "Optimization", "of", "Immersive", "Virtual", "Environments", "Figure", "8", ":", "Box", "plots", "for", "the", "results", "of", "our", "questionnaires", ".", "Horizontal", "red", "bars", "represent", "medians", ",", "and", "boxes", "represent", "the", "interquartile", "ranges", "(", "IQRs", ")", ".", "Whiskers", "stretch", "to", "the", "data", "points", "that", "are", "within", "the", "median", "±", "1.5", "IQR", "."]}, "filename": "7abb628d0f9c326e2a4939c70eec4544ea9b4960_Image_011.jpg", "orig_filename": "7abb628d0f9c326e2a4939c70eec4544ea9b4960", "split": "train"}, {"article_id": "A Large Inclusive Study of Human Listening Rates", "description": {"raw": "Line graph of VoiceOver speed (x-axis) vs. Words per Minute (y-axis), with two segmented curves: one for rhyme test questions, and one for transcription and yes/no questions. The exact values, in CSV format, are:  VoiceOver speed, Words per Minute - Rhyme test questions, Words per minute - Transcription and yes/no questions  14, 101, 97  29, 139, 132  43, 233, 208  57, 357, 311  71, 501, 402  86, 651, 462  100, 766, 506", "tokens": ["Line", "graph", "of", "VoiceOver", "speed", "(", "x-axis", ")", "vs", ".", "Words", "per", "Minute", "(", "y-axis", ")", ",", "with", "two", "segmented", "curves", ":", "one", "for", "rhyme", "test", "questions", ",", "and", "one", "for", "transcription", "and", "yes/no", "questions", ".", "The", "exact", "values", ",", "in", "CSV", "format", ",", "are", ":", "VoiceOver", "speed", ",", "Words", "per", "Minute", "-", "Rhyme", "test", "questions", ",", "Words", "per", "minute", "-", "Transcription", "and", "yes/no", "questions", "14", ",", "101", ",", "97", "29", ",", "139", ",", "132", "43", ",", "233", ",", "208", "57", ",", "357", ",", "311", "71", ",", "501", ",", "402", "86", ",", "651", ",", "462", "100", ",", "766", ",", "506"]}, "caption": {"raw": "Figure 2: VoiceOver speeds translated into words per minute, for the rhyme test questions (words) and for the transcription and yes/no questions (sentences). Typical human speaking rate 120-180 WPM corresponds to VoiceOver range 24-38.", "tokens": ["Figure", "2", ":", "VoiceOver", "speeds", "translated", "into", "words", "per", "minute", ",", "for", "the", "rhyme", "test", "questions", "(", "words", ")", "and", "for", "the", "transcription", "and", "yes/no", "questions", "(", "sentences", ")", ".", "Typical", "human", "speaking", "rate", "120-180", "WPM", "corresponds", "to", "VoiceOver", "range", "24-38", "."]}, "context": {"raw": "A Large Inclusive Study of Human Listening Rates Figure 2: VoiceOver speeds translated into words per minute, for the rhyme test questions (words) and for the transcription and yes/no questions (sentences). Typical human speaking rate 120-180 WPM corresponds to VoiceOver range 24-38.", "tokens": ["A", "Large", "Inclusive", "Study", "of", "Human", "Listening", "Rates", "Figure", "2", ":", "VoiceOver", "speeds", "translated", "into", "words", "per", "minute", ",", "for", "the", "rhyme", "test", "questions", "(", "words", ")", "and", "for", "the", "transcription", "and", "yes/no", "questions", "(", "sentences", ")", ".", "Typical", "human", "speaking", "rate", "120-180", "WPM", "corresponds", "to", "VoiceOver", "range", "24-38", "."]}, "filename": "90f8a200755d05d7f41af45fb4883e62a1ca831d_Image_007.jpg", "orig_filename": "90f8a200755d05d7f41af45fb4883e62a1ca831d", "split": "train"}, {"article_id": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning", "description": {"raw": "The graph mode allows for manipulating the graph of any of the selected groups through moving the buble marks for each of the axes. It also shows the graph for the previous session for comparison. The user can show/hide the graph for the previous session and the graph of the students' self assessment.", "tokens": ["The", "graph", "mode", "allows", "for", "manipulating", "the", "graph", "of", "any", "of", "the", "selected", "groups", "through", "moving", "the", "buble", "marks", "for", "each", "of", "the", "axes", ".", "It", "also", "shows", "the", "graph", "for", "the", "previous", "session", "for", "comparison", ".", "The", "user", "can", "show/hide", "the", "graph", "for", "the", "previous", "session", "and", "the", "graph", "of", "the", "students", "'", "self", "assessment", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning ", "tokens": ["Group", "Spinner", ":", "Recognizing", "and", "Visualizing", "Learning", "in", "the", "Classroom", "for", "Reflection", ",", "Communication", ",", "and", "Planning"]}, "filename": "35941a4414b58e76d1f92b495c3c8d90a2593315_Image_005.jpg", "orig_filename": "35941a4414b58e76d1f92b495c3c8d90a2593315", "split": "train"}, {"article_id": "PrivacyMic: Utilizing Inaudible Frequencies for Privacy Preserving Daily Activity Recognition", "description": {"raw": "5-pane figure showing A) PrivacyMic's hardware B) an FFT of unfiltered audio C) the Bode plot of the audible filter D) an FFT of filtered audio and E) PrivacyMic successfully classifying use of the sink", "tokens": ["5-pane", "figure", "showing", "A", ")", "PrivacyMic", "'s", "hardware", "B", ")", "an", "FFT", "of", "unfiltered", "audio", "C", ")", "the", "Bode", "plot", "of", "the", "audible", "filter", "D", ")", "an", "FFT", "of", "filtered", "audio", "and", "E", ")", "PrivacyMic", "successfully", "classifying", "use", "of", "the", "sink"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "PrivacyMic: Utilizing Inaudible Frequencies for Privacy Preserving Daily Activity Recognition ", "tokens": ["PrivacyMic", ":", "Utilizing", "Inaudible", "Frequencies", "for", "Privacy", "Preserving", "Daily", "Activity", "Recognition"]}, "filename": "47728a7d0b7cc0836abbe6ccde4720c52823f3e2_Image_001.jpg", "orig_filename": "47728a7d0b7cc0836abbe6ccde4720c52823f3e2", "split": "train"}, {"article_id": "Transient and transitional states: pressure as an auxiliary input modality for bimanual interaction", "description": {"raw": "Graph showing the mean selection time for for each Target Distance within each Direction. The graph shows, across directions, selection time increased as distance increased. The selection times for the Up Direction are generally faster than the Down Direction.", "tokens": ["Graph", "showing", "the", "mean", "selection", "time", "for", "for", "each", "Target", "Distance", "within", "each", "Direction", ".", "The", "graph", "shows", ",", "across", "directions", ",", "selection", "time", "increased", "as", "distance", "increased", ".", "The", "selection", "times", "for", "the", "Up", "Direction", "are", "generally", "faster", "than", "the", "Down", "Direction", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Transient and transitional states: pressure as an auxiliary input modality for bimanual interaction ", "tokens": ["Transient", "and", "transitional", "states", ":", "pressure", "as", "an", "auxiliary", "input", "modality", "for", "bimanual", "interaction"]}, "filename": "2b270028ed9d0e8f2851f67988e822ab434cdb9d_Image_009.png", "orig_filename": "2b270028ed9d0e8f2851f67988e822ab434cdb9d", "split": "train"}, {"article_id": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing", "description": {"raw": "Caption: Fig 2. For each sensor, each feature was extracted from 45 time slices. First, raw data from the device sensor was preprocessed and then filtered by an epoch and a days-of-the-week option. Features (let NS be number of features derived from each sensor) were then extracted from the selected raw data according to 3 levels of granularity - per semester (NS features), per half-semester (2*NS features), and per week (16*NS features). Description: The figure shows  raw data being filtered by epoch (5 options), then by day of week (3 options) and then selection granularity (3 options) leading to feature extraction. Flowchart for temporal slicing is explained in section 4.1.8.", "tokens": ["Caption", ":", "Fig", "2", ".", "For", "each", "sensor", ",", "each", "feature", "was", "extracted", "from", "45", "time", "slices", ".", "First", ",", "raw", "data", "from", "the", "device", "sensor", "was", "preprocessed", "and", "then", "filtered", "by", "an", "epoch", "and", "a", "days-of-the-week", "option", ".", "Features", "(", "let", "NS", "be", "number", "of", "features", "derived", "from", "each", "sensor", ")", "were", "then", "extracted", "from", "the", "selected", "raw", "data", "according", "to", "3", "levels", "of", "granularity", "-", "per", "semester", "(", "NS", "features", ")", ",", "per", "half-semester", "(", "2", "*", "NS", "features", ")", ",", "and", "per", "week", "(", "16", "*", "NS", "features", ")", ".", "Description", ":", "The", "figure", "shows", "raw", "data", "being", "filtered", "by", "epoch", "(", "5", "options", ")", ",", "then", "by", "day", "of", "week", "(", "3", "options", ")", "and", "then", "selection", "granularity", "(", "3", "options", ")", "leading", "to", "feature", "extraction", ".", "Flowchart", "for", "temporal", "slicing", "is", "explained", "in", "section", "4.1.8", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing ", "tokens": ["Detecting", "Depression", "and", "Predicting", "its", "Onset", "Using", "Longitudinal", "Symptoms", "Captured", "by", "Passive", "Sensing"]}, "filename": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2_Image_007.jpg", "orig_filename": "fb69af5da46713641b0bf69ba57ff1c3c9e124b2", "split": "train"}, {"article_id": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults", "description": {"raw": "This figure shows a bar chart featuring the mean responses to Borg's RPE scale before gameplay and after playing each version of the game. The values are detailed in the following paragraph.", "tokens": ["This", "figure", "shows", "a", "bar", "chart", "featuring", "the", "mean", "responses", "to", "Borg", "'s", "RPE", "scale", "before", "gameplay", "and", "after", "playing", "each", "version", "of", "the", "game", ".", "The", "values", "are", "detailed", "in", "the", "following", "paragraph", "."]}, "caption": {"raw": "Figure 5. Mean response to Borg's RPE scale before gameplay and after playing each version of the game.", "tokens": ["Figure", "5", ".", "Mean", "response", "to", "Borg", "'s", "RPE", "scale", "before", "gameplay", "and", "after", "playing", "each", "version", "of", "the", "game", "."]}, "context": {"raw": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults Figure 5. Mean response to Borg's RPE scale before gameplay and after playing each version of the game.", "tokens": ["Visual", "complexity", ",", "player", "experience", ",", "performance", "and", "physical", "exertion", "in", "motion-based", "games", "for", "older", "adults", "Figure", "5", ".", "Mean", "response", "to", "Borg", "'s", "RPE", "scale", "before", "gameplay", "and", "after", "playing", "each", "version", "of", "the", "game", "."]}, "filename": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e_Image_007.gif", "orig_filename": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e", "split": "train"}, {"article_id": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "description": {"raw": "Figure 5c. DreamCatcher view of a single person with sleep and mood data for a week.", "tokens": ["Figure", "5c", ".", "DreamCatcher", "view", "of", "a", "single", "person", "with", "sleep", "and", "mood", "data", "for", "a", "week", "."]}, "caption": {"raw": "Figure 5. Screens in the final design of DreamCatcher. Figure 1 also shows the Family Daily View. Each example also includes a reflective prompt and an indicator that audio is being recorded.", "tokens": ["Figure", "5", ".", "Screens", "in", "the", "final", "design", "of", "DreamCatcher", ".", "Figure", "1", "also", "shows", "the", "Family", "Daily", "View", ".", "Each", "example", "also", "includes", "a", "reflective", "prompt", "and", "an", "indicator", "that", "audio", "is", "being", "recorded", "."]}, "context": {"raw": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together Figure 5. Screens in the final design of DreamCatcher. Figure 1 also shows the Family Daily View. Each example also includes a reflective prompt and an indicator that audio is being recorded.", "tokens": ["DreamCatcher", ":", "Exploring", "How", "Parents", "and", "School-Age", "Children", "can", "Track", "and", "Review", "Sleep", "Information", "Together", "Figure", "5", ".", "Screens", "in", "the", "final", "design", "of", "DreamCatcher", ".", "Figure", "1", "also", "shows", "the", "Family", "Daily", "View", ".", "Each", "example", "also", "includes", "a", "reflective", "prompt", "and", "an", "indicator", "that", "audio", "is", "being", "recorded", "."]}, "filename": "373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_013.jpg", "orig_filename": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "split": "train"}, {"article_id": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "description": {"raw": "This Figure is divided into three parts: a, b, and c.\n\nPart (a) of this figure shows a stacked bar plot showing percentages of 5-point likert-scale responses. The title is “This text was easy to read”, the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The likert-scale responses for “Neutral” is centered, and “Disagree” and “Strongly Disagree” are on the left side, and “Agree” and “Strongly Agree” are on the right side. For all the conditions, the majority of responses are on the right side, and there is very little on the left side.\n\nFor \"Original\" 4% responded “Strongly Disagree”, 4% for “Disagree”, 24% for “Neutral”, 56% for “Agree”, and 12% for “Strongly Agree”. \nFor \"Automatic\" 0% responded “Strongly Disagree”, 4% for “Disagree”, 8% for “Neutral”, 60% for “Agree”, and 28% for “Strongly Agree”.\nFor \"Pop-up\" 0% responded “Strongly Disagree”, 8% for “Disagree”, 12% for “Neutral”, 48% for “Agree”, and 32% for “Strongly Agree”. \nFor \"Decoration\" 0% responded “Strongly Disagree”, 4% for “Disagree”, 12% for “Neutral”, 52% for “Agree”, and 32% for “Strongly Agree”.\n\n\nPart (b) of this figure shows a stacked bar plot showing percentages of 5-point Likert-scale responses. The title is “I was able to understand this text well.”, the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. There is a bracket indicating p<.05 significance between the “Original” and “Pop-up” plots. There is also a bracket indicating p<.05 significance between the “Original” and “Decoration” plots. The Likert-scale responses for “Neutral” is centered, and “Disagree” and “Strongly Disagree” are on the left side, and “Agree” and “Strongly Agree” are on the right side. For all conditions, the majority of responses are on the right side.\n\nFor \"Original\" 0% responded “Strongly Disagree”, 12% for “Disagree”, 20% for “Neutral”, 60% for “Agree”, and 8% for “Strongly Agree”. \nFor \"Automatic\" 0% responded “Strongly Disagree”, 4% for “Disagree”, 20% for “Neutral”, 40% for “Agree”, and 36% for “Strongly Agree”.\nFor \"Pop-up\" 0% responded “Strongly Disagree”, 0% for “Disagree”, 12% for “Neutral”, 48% for “Agree”, and 40% for “Strongly Agree”. \nFor \"Decoration\" 0% responded “Strongly Disagree”, 0% for “Disagree”, 12% for “Neutral”, 44% for “Agree”, and 44% for “Strongly Agree”.\n\n\nPart (c) of this figure shows a boxplot for Comprehension Scores for four different conditions, “Original”, “Automatic”, “Pop-up”, and “Decoration”, which are on the x-axis. The y-axis is the score, and is a percentage from 0 to 100. For “Original” and “Automatic” the boxplots appear to be almost equal and most of the boxplot is between 30% and 70%. For “Pop-up”, it is higher up, with the first quartile starting at around 70%. For “Decoration” there is a wider range, the first quartile is at around 30% and the third quartile is at 100%.\n\nThe “Original” boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 62.70%, 3rd quartile 66.67%, and a maximum of 100%.\nThe “Automatic” boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.7%, mean 58.7%, 3rd quartile 66.67%, and a maximum of 100%.\nThe “Pop-up” boxplot has a minimum of 33.33%, 1st quartile 66.67%, median 66.7%, mean 76%, 3rd quartile 100%, and a maximum of 100%.\nThe “Decoration” boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 60%, 3rd quartile 100%, and a maximum of 100%.", "tokens": ["This", "Figure", "is", "divided", "into", "three", "parts", ":", "a", ",", "b", ",", "and", "c.", "Part", "(", "a", ")", "of", "this", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "5-point", "likert-scale", "responses", ".", "The", "title", "is", "“", "This", "text", "was", "easy", "to", "read", "”", ",", "the", "y-axis", "has", "4", "categories", ";", "``", "Original", "''", ",", "``", "Automatic", "''", ",", "``", "Pop-up", "''", ",", "and", "``", "Decoration", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "The", "likert-scale", "responses", "for", "“", "Neutral", "”", "is", "centered", ",", "and", "“", "Disagree", "”", "and", "“", "Strongly", "Disagree", "”", "are", "on", "the", "left", "side", ",", "and", "“", "Agree", "”", "and", "“", "Strongly", "Agree", "”", "are", "on", "the", "right", "side", ".", "For", "all", "the", "conditions", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ",", "and", "there", "is", "very", "little", "on", "the", "left", "side", ".", "For", "``", "Original", "''", "4", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "24", "%", "for", "“", "Neutral", "”", ",", "56", "%", "for", "“", "Agree", "”", ",", "and", "12", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Automatic", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "8", "%", "for", "“", "Neutral", "”", ",", "60", "%", "for", "“", "Agree", "”", ",", "and", "28", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Pop-up", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "8", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "48", "%", "for", "“", "Agree", "”", ",", "and", "32", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Decoration", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "52", "%", "for", "“", "Agree", "”", ",", "and", "32", "%", "for", "“", "Strongly", "Agree", "”", ".", "Part", "(", "b", ")", "of", "this", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "5-point", "Likert-scale", "responses", ".", "The", "title", "is", "“", "I", "was", "able", "to", "understand", "this", "text", "well.", "”", ",", "the", "y-axis", "has", "4", "categories", ";", "``", "Original", "''", ",", "``", "Automatic", "''", ",", "``", "Pop-up", "''", ",", "and", "``", "Decoration", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "There", "is", "a", "bracket", "indicating", "p", "<", ".05", "significance", "between", "the", "“", "Original", "”", "and", "“", "Pop-up", "”", "plots", ".", "There", "is", "also", "a", "bracket", "indicating", "p", "<", ".05", "significance", "between", "the", "“", "Original", "”", "and", "“", "Decoration", "”", "plots", ".", "The", "Likert-scale", "responses", "for", "“", "Neutral", "”", "is", "centered", ",", "and", "“", "Disagree", "”", "and", "“", "Strongly", "Disagree", "”", "are", "on", "the", "left", "side", ",", "and", "“", "Agree", "”", "and", "“", "Strongly", "Agree", "”", "are", "on", "the", "right", "side", ".", "For", "all", "conditions", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ".", "For", "``", "Original", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "12", "%", "for", "“", "Disagree", "”", ",", "20", "%", "for", "“", "Neutral", "”", ",", "60", "%", "for", "“", "Agree", "”", ",", "and", "8", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Automatic", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "20", "%", "for", "“", "Neutral", "”", ",", "40", "%", "for", "“", "Agree", "”", ",", "and", "36", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Pop-up", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "0", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "48", "%", "for", "“", "Agree", "”", ",", "and", "40", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Decoration", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "0", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "44", "%", "for", "“", "Agree", "”", ",", "and", "44", "%", "for", "“", "Strongly", "Agree", "”", ".", "Part", "(", "c", ")", "of", "this", "figure", "shows", "a", "boxplot", "for", "Comprehension", "Scores", "for", "four", "different", "conditions", ",", "“", "Original", "”", ",", "“", "Automatic", "”", ",", "“", "Pop-up", "”", ",", "and", "“", "Decoration", "”", ",", "which", "are", "on", "the", "x-axis", ".", "The", "y-axis", "is", "the", "score", ",", "and", "is", "a", "percentage", "from", "0", "to", "100", ".", "For", "“", "Original", "”", "and", "“", "Automatic", "”", "the", "boxplots", "appear", "to", "be", "almost", "equal", "and", "most", "of", "the", "boxplot", "is", "between", "30", "%", "and", "70", "%", ".", "For", "“", "Pop-up", "”", ",", "it", "is", "higher", "up", ",", "with", "the", "first", "quartile", "starting", "at", "around", "70", "%", ".", "For", "“", "Decoration", "”", "there", "is", "a", "wider", "range", ",", "the", "first", "quartile", "is", "at", "around", "30", "%", "and", "the", "third", "quartile", "is", "at", "100", "%", ".", "The", "“", "Original", "”", "boxplot", "has", "a", "minimum", "of", "0", "%", ",", "1st", "quartile", "33.33", "%", ",", "median", "66.67", "%", ",", "mean", "62.70", "%", ",", "3rd", "quartile", "66.67", "%", ",", "and", "a", "maximum", "of", "100", "%", ".", "The", "“", "Automatic", "”", "boxplot", "has", "a", "minimum", "of", "0", "%", ",", "1st", "quartile", "33.33", "%", ",", "median", "66.7", "%", ",", "mean", "58.7", "%", ",", "3rd", "quartile", "66.67", "%", ",", "and", "a", "maximum", "of", "100", "%", ".", "The", "“", "Pop-up", "”", "boxplot", "has", "a", "minimum", "of", "33.33", "%", ",", "1st", "quartile", "66.67", "%", ",", "median", "66.7", "%", ",", "mean", "76", "%", ",", "3rd", "quartile", "100", "%", ",", "and", "a", "maximum", "of", "100", "%", ".", "The", "“", "Decoration", "”", "boxplot", "has", "a", "minimum", "of", "0", "%", ",", "1st", "quartile", "33.33", "%", ",", "median", "66.67", "%", ",", "mean", "60", "%", ",", "3rd", "quartile", "100", "%", ",", "and", "a", "maximum", "of", "100", "%", "."]}, "caption": {"raw": "(c)  Figure 4. Participants’ responses to questions about all four conditions in the experimental study, including subjective Likert-scale responses for (a) the text was easy to read and (b) I was able to understand this text well, with significant pairwise differences marked with asterisks (* p <0.05). In (c), analysis on objective comprehension questions did not reveal any significant differences between the four conditions.", "tokens": ["(", "c", ")", "Figure", "4", ".", "Participants", "’", "responses", "to", "questions", "about", "all", "four", "conditions", "in", "the", "experimental", "study", ",", "including", "subjective", "Likert-scale", "responses", "for", "(", "a", ")", "the", "text", "was", "easy", "to", "read", "and", "(", "b", ")", "I", "was", "able", "to", "understand", "this", "text", "well", ",", "with", "significant", "pairwise", "differences", "marked", "with", "asterisks", "(", "*", "p", "<", "0.05", ")", ".", "In", "(", "c", ")", ",", "analysis", "on", "objective", "comprehension", "questions", "did", "not", "reveal", "any", "significant", "differences", "between", "the", "four", "conditions", "."]}, "context": {"raw": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy (c)  Figure 4. Participants’ responses to questions about all four conditions in the experimental study, including subjective Likert-scale responses for (a) the text was easy to read and (b) I was able to understand this text well, with significant pairwise differences marked with asterisks (* p <0.05). In (c), analysis on objective comprehension questions did not reveal any significant differences between the four conditions.", "tokens": ["Automatic", "Text", "Simplification", "Tools", "for", "Deaf", "and", "Hard", "of", "Hearing", "Adults", ":", "Benefits", "of", "Lexical", "Simplification", "and", "Providing", "Users", "with", "Autonomy", "(", "c", ")", "Figure", "4", ".", "Participants", "’", "responses", "to", "questions", "about", "all", "four", "conditions", "in", "the", "experimental", "study", ",", "including", "subjective", "Likert-scale", "responses", "for", "(", "a", ")", "the", "text", "was", "easy", "to", "read", "and", "(", "b", ")", "I", "was", "able", "to", "understand", "this", "text", "well", ",", "with", "significant", "pairwise", "differences", "marked", "with", "asterisks", "(", "*", "p", "<", "0.05", ")", ".", "In", "(", "c", ")", ",", "analysis", "on", "objective", "comprehension", "questions", "did", "not", "reveal", "any", "significant", "differences", "between", "the", "four", "conditions", "."]}, "filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_007.jpg", "orig_filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "split": "train"}, {"article_id": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments", "description": {"raw": "It shows four screenshots of AIGuide. The top-left screenshot shows the Selection Interface, which contains a list with three items: Fruit Bar, Lipton Iced Tea and Lucky Charms. The top-right shows the guidance interface, which contains two labels. One indicates that the item was found. Other indicates that the item is two feet away, 15 degrees left and 5 inches below the camera view. Also, it has a guide, confirm , exit and restart button. The bottom-left shows the user settings, which contains a toggle switch for camera access, two toggle switches to control haptic and sound feedback, a scrolling bar for the speaking rate and a submenu for the measuring system. The bottom right shows the tutorial interface, which contains a page number at the top, the description of that explains how the localization phase works, a button to play a demo, and two buttons to switch page.", "tokens": ["It", "shows", "four", "screenshots", "of", "AIGuide", ".", "The", "top-left", "screenshot", "shows", "the", "Selection", "Interface", ",", "which", "contains", "a", "list", "with", "three", "items", ":", "Fruit", "Bar", ",", "Lipton", "Iced", "Tea", "and", "Lucky", "Charms", ".", "The", "top-right", "shows", "the", "guidance", "interface", ",", "which", "contains", "two", "labels", ".", "One", "indicates", "that", "the", "item", "was", "found", ".", "Other", "indicates", "that", "the", "item", "is", "two", "feet", "away", ",", "15", "degrees", "left", "and", "5", "inches", "below", "the", "camera", "view", ".", "Also", ",", "it", "has", "a", "guide", ",", "confirm", ",", "exit", "and", "restart", "button", ".", "The", "bottom-left", "shows", "the", "user", "settings", ",", "which", "contains", "a", "toggle", "switch", "for", "camera", "access", ",", "two", "toggle", "switches", "to", "control", "haptic", "and", "sound", "feedback", ",", "a", "scrolling", "bar", "for", "the", "speaking", "rate", "and", "a", "submenu", "for", "the", "measuring", "system", ".", "The", "bottom", "right", "shows", "the", "tutorial", "interface", ",", "which", "contains", "a", "page", "number", "at", "the", "top", ",", "the", "description", "of", "that", "explains", "how", "the", "localization", "phase", "works", ",", "a", "button", "to", "play", "a", "demo", ",", "and", "two", "buttons", "to", "switch", "page", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments ", "tokens": ["AIGuide", ":", "An", "Augmented", "Reality", "Hand", "Guidance", "Application", "for", "People", "with", "Visual", "Impairments"]}, "filename": "9475d822749fd4754e46de81e21b28445e4f98a7_Image_005.jpg", "orig_filename": "9475d822749fd4754e46de81e21b28445e4f98a7", "split": "train"}, {"article_id": "A Large Inclusive Study of Human Listening Rates", "description": {"raw": "Line plot of Age (x-axis) vs. Listening Rate (y-axis) with two segmented curves: one for sighted participants, and one for visually impaired participants. The exact values, in CSV format, are:  Age (years), Listening Rate - sighted, Listening Rate - visually impaired  0-15, 49.86 (std err 7.04), 72.16 (std err 5.25)  15-30, 56.15 (std err 1.46), 70.97 (std err 3.36)  30-45, 59.04 (std err 2.39), 66.78 (std err 3.68)  45-60, 48.97 (std err 3.08), 38.51 (std err 4.70)  60-75, 55.19 (std err 3.38), 29.02 (std err 5.30)", "tokens": ["Line", "plot", "of", "Age", "(", "x-axis", ")", "vs", ".", "Listening", "Rate", "(", "y-axis", ")", "with", "two", "segmented", "curves", ":", "one", "for", "sighted", "participants", ",", "and", "one", "for", "visually", "impaired", "participants", ".", "The", "exact", "values", ",", "in", "CSV", "format", ",", "are", ":", "Age", "(", "years", ")", ",", "Listening", "Rate", "-", "sighted", ",", "Listening", "Rate", "-", "visually", "impaired", "0-15", ",", "49.86", "(", "std", "err", "7.04", ")", ",", "72.16", "(", "std", "err", "5.25", ")", "15-30", ",", "56.15", "(", "std", "err", "1.46", ")", ",", "70.97", "(", "std", "err", "3.36", ")", "30-45", ",", "59.04", "(", "std", "err", "2.39", ")", ",", "66.78", "(", "std", "err", "3.68", ")", "45-60", ",", "48.97", "(", "std", "err", "3.08", ")", ",", "38.51", "(", "std", "err", "4.70", ")", "60-75", ",", "55.19", "(", "std", "err", "3.38", ")", ",", "29.02", "(", "std", "err", "5.30", ")"]}, "caption": {"raw": "Figure 5: Plot of age vs. Listening Rate, for visually impaired and sighted groups.", "tokens": ["Figure", "5", ":", "Plot", "of", "age", "vs", ".", "Listening", "Rate", ",", "for", "visually", "impaired", "and", "sighted", "groups", "."]}, "context": {"raw": "A Large Inclusive Study of Human Listening Rates Figure 5: Plot of age vs. Listening Rate, for visually impaired and sighted groups.", "tokens": ["A", "Large", "Inclusive", "Study", "of", "Human", "Listening", "Rates", "Figure", "5", ":", "Plot", "of", "age", "vs", ".", "Listening", "Rate", ",", "for", "visually", "impaired", "and", "sighted", "groups", "."]}, "filename": "90f8a200755d05d7f41af45fb4883e62a1ca831d_Image_010.jpg", "orig_filename": "90f8a200755d05d7f41af45fb4883e62a1ca831d", "split": "train"}, {"article_id": "Motor-impaired touchscreen interactions in the wild", "description": {"raw": "bar graph showing the three gesture recognisers accuracies; baseline 85%, user specific 79.7% and session specific 95.1%", "tokens": ["bar", "graph", "showing", "the", "three", "gesture", "recognisers", "accuracies", ";", "baseline", "85", "%", ",", "user", "specific", "79.7", "%", "and", "session", "specific", "95.1", "%"]}, "caption": {"raw": "Figure 4 Classification accuracy of gesture recognizers for touch model conditions", "tokens": ["Figure", "4", "Classification", "accuracy", "of", "gesture", "recognizers", "for", "touch", "model", "conditions"]}, "context": {"raw": "Motor-impaired touchscreen interactions in the wild Figure 4 Classification accuracy of gesture recognizers for touch model conditions", "tokens": ["Motor-impaired", "touchscreen", "interactions", "in", "the", "wild", "Figure", "4", "Classification", "accuracy", "of", "gesture", "recognizers", "for", "touch", "model", "conditions"]}, "filename": "c7cbe32734918f335533efd49f9c46e69e2e14b4_Image_005.jpg", "orig_filename": "c7cbe32734918f335533efd49f9c46e69e2e14b4", "split": "train"}, {"article_id": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "description": {"raw": "This figure has two line graphs. A line graph on the left describes the average proportion of photos that included the full, part of, and no object.   Train Full: average=0.68, variance=0.05 Partial: average=0.29, variance=0.05 No: average=0.02, variance=0.001 Vanilla test Full: average=0.77, variance=0.03 Partial: average=0.20, variance=0.02 No: average=0.02, variance=0.002 Wild test Full: average=0.62, variance=0.10 Partial: average=0.30, variance=0.05 No: average=0.08, variance=0.01  The other line graph on the right describes the average proportion of photos that included the full, part of, and no hand.  Train Full: average=0.53, variance=0.14 Partial: average=0.13, variance=0.01 No: average=0.34, variance=0.14 Vanilla test Full: average=0.58, variance=0.18 Partial: average=0.11, variance=0.01 No: average=0.31, variance=0.19 Wild test Full: average=0.66, variance=0.12 Partial: average=0.19, variance=0.01 No: average=0.16, variance=0.08", "tokens": ["This", "figure", "has", "two", "line", "graphs", ".", "A", "line", "graph", "on", "the", "left", "describes", "the", "average", "proportion", "of", "photos", "that", "included", "the", "full", ",", "part", "of", ",", "and", "no", "object", ".", "Train", "Full", ":", "average=0.68", ",", "variance=0.05", "Partial", ":", "average=0.29", ",", "variance=0.05", "No", ":", "average=0.02", ",", "variance=0.001", "Vanilla", "test", "Full", ":", "average=0.77", ",", "variance=0.03", "Partial", ":", "average=0.20", ",", "variance=0.02", "No", ":", "average=0.02", ",", "variance=0.002", "Wild", "test", "Full", ":", "average=0.62", ",", "variance=0.10", "Partial", ":", "average=0.30", ",", "variance=0.05", "No", ":", "average=0.08", ",", "variance=0.01", "The", "other", "line", "graph", "on", "the", "right", "describes", "the", "average", "proportion", "of", "photos", "that", "included", "the", "full", ",", "part", "of", ",", "and", "no", "hand", ".", "Train", "Full", ":", "average=0.53", ",", "variance=0.14", "Partial", ":", "average=0.13", ",", "variance=0.01", "No", ":", "average=0.34", ",", "variance=0.14", "Vanilla", "test", "Full", ":", "average=0.58", ",", "variance=0.18", "Partial", ":", "average=0.11", ",", "variance=0.01", "No", ":", "average=0.31", ",", "variance=0.19", "Wild", "test", "Full", ":", "average=0.66", ",", "variance=0.12", "Partial", ":", "average=0.19", ",", "variance=0.01", "No", ":", "average=0.16", ",", "variance=0.08"]}, "caption": {"raw": "Figure 9: Proportion of photos with fully, partially, or not included objects and hands. Errors bars show variance among participants, who were able to fully capture the object in 60– 80% of their photos and include their hand in more than 50%.", "tokens": ["Figure", "9", ":", "Proportion", "of", "photos", "with", "fully", ",", "partially", ",", "or", "not", "included", "objects", "and", "hands", ".", "Errors", "bars", "show", "variance", "among", "participants", ",", "who", "were", "able", "to", "fully", "capture", "the", "object", "in", "60–", "80", "%", "of", "their", "photos", "and", "include", "their", "hand", "in", "more", "than", "50", "%", "."]}, "context": {"raw": "Revisiting Blind Photography in the Context of Teachable Object Recognizers Figure 9: Proportion of photos with fully, partially, or not included objects and hands. Errors bars show variance among participants, who were able to fully capture the object in 60– 80% of their photos and include their hand in more than 50%.", "tokens": ["Revisiting", "Blind", "Photography", "in", "the", "Context", "of", "Teachable", "Object", "Recognizers", "Figure", "9", ":", "Proportion", "of", "photos", "with", "fully", ",", "partially", ",", "or", "not", "included", "objects", "and", "hands", ".", "Errors", "bars", "show", "variance", "among", "participants", ",", "who", "were", "able", "to", "fully", "capture", "the", "object", "in", "60–", "80", "%", "of", "their", "photos", "and", "include", "their", "hand", "in", "more", "than", "50", "%", "."]}, "filename": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_026.jpg", "orig_filename": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "split": "train"}, {"article_id": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "description": {"raw": "A box plot displaying the Mean Harmonicity across each condition, with units in decibels (dB). First results are presented for the 9 participants who spoke in all three conditions, with median value of 4.826 for Markup, 2.555 for No ASR, and 4.083 for ASR.  There was a significant difference between Markup and No ASR.  Next, results are shown for all 12 participants, with median value of 4.848 for Markup and 4.372 for ASR.  There was no significant difference between these two conditions.", "tokens": ["A", "box", "plot", "displaying", "the", "Mean", "Harmonicity", "across", "each", "condition", ",", "with", "units", "in", "decibels", "(", "dB", ")", ".", "First", "results", "are", "presented", "for", "the", "9", "participants", "who", "spoke", "in", "all", "three", "conditions", ",", "with", "median", "value", "of", "4.826", "for", "Markup", ",", "2.555", "for", "No", "ASR", ",", "and", "4.083", "for", "ASR", ".", "There", "was", "a", "significant", "difference", "between", "Markup", "and", "No", "ASR", ".", "Next", ",", "results", "are", "shown", "for", "all", "12", "participants", ",", "with", "median", "value", "of", "4.848", "for", "Markup", "and", "4.372", "for", "ASR", ".", "There", "was", "no", "significant", "difference", "between", "these", "two", "conditions", "."]}, "caption": {"raw": "Figure 5: Box plots for harmonicity (mean harmonics-to-noise ratio), for each hearing participant, across the three conditions", "tokens": ["Figure", "5", ":", "Box", "plots", "for", "harmonicity", "(", "mean", "harmonics-to-noise", "ratio", ")", ",", "for", "each", "hearing", "participant", ",", "across", "the", "three", "conditions"]}, "context": {"raw": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers Figure 5: Box plots for harmonicity (mean harmonics-to-noise ratio), for each hearing participant, across the three conditions", "tokens": ["Behavioral", "Changes", "in", "Speakers", "who", "are", "Automatically", "Captioned", "in", "Meetings", "with", "Deaf", "or", "Hard-of-Hearing", "Peers", "Figure", "5", ":", "Box", "plots", "for", "harmonicity", "(", "mean", "harmonics-to-noise", "ratio", ")", ",", "for", "each", "hearing", "participant", ",", "across", "the", "three", "conditions"]}, "filename": "4cecd70a9e46a761774a54ed11d613b33721b95d_Image_008.gif", "orig_filename": "4cecd70a9e46a761774a54ed11d613b33721b95d", "split": "train"}, {"article_id": "Uncovering Challenges and Opportunities for 3D Printing Assistive Technology with Physical Therapists", "description": {"raw": "Bar graph which demonstrates the increase level of knowledge gained by students after the 3D printing class sessions.", "tokens": ["Bar", "graph", "which", "demonstrates", "the", "increase", "level", "of", "knowledge", "gained", "by", "students", "after", "the", "3D", "printing", "class", "sessions", "."]}, "caption": {"raw": "Figure 5. Bar graph of students’ change in personal knowledge of 3D printing between Class One and Class Two.", "tokens": ["Figure", "5", ".", "Bar", "graph", "of", "students", "’", "change", "in", "personal", "knowledge", "of", "3D", "printing", "between", "Class", "One", "and", "Class", "Two", "."]}, "context": {"raw": "Uncovering Challenges and Opportunities for 3D Printing Assistive Technology with Physical Therapists Figure 5. Bar graph of students’ change in personal knowledge of 3D printing between Class One and Class Two.", "tokens": ["Uncovering", "Challenges", "and", "Opportunities", "for", "3D", "Printing", "Assistive", "Technology", "with", "Physical", "Therapists", "Figure", "5", ".", "Bar", "graph", "of", "students", "’", "change", "in", "personal", "knowledge", "of", "3D", "printing", "between", "Class", "One", "and", "Class", "Two", "."]}, "filename": "bcb3a46274165a64c4b951ae95fa118493593ddc_Image_006.jpg", "orig_filename": "bcb3a46274165a64c4b951ae95fa118493593ddc", "split": "train"}, {"article_id": "Gesture-based Interaction for Individuals with Developmental Disabilities in India", "description": {"raw": "This is a screenshot of the virtual kirana store. On the left side is a thin light blue sidebar with two items and their price: biscuits for rupees 15 and choco for rupees 35. At the bottom of the side bar there is text that reads- bill rupees 50. In the center is the virtual store with two brown wooden shelves and a wooden table in front of them. The top shelf has eggs for rupees 20, milk for ruppes 20, sliced brown bread of rupees 15 and yogurt for rupees 15. The lower shelf only has a bag of chips for rupees 10 and butter for rupees 30. The table has two items on its left: one packet of chocolate biscuits for rupees 15 and one bar of chocolate for rupees 35.All the items, except eggs, are from brands commonly available in India. On the rigth side is a thin yellow sidebar which has Indian curreny / money placed on it. From the top, there is one 50 rupees note, 20 rupees note, 10 rupees note which is selected by an onscreen hand cursor so that this note is about 40% bigger in size than the other notes. Under the 10 rupees note is a 5 rupees note and finally a 5 rupee coin and 10 rupee coin. At the bottom of the side bar there is text that reads- total is rupees 100.", "tokens": ["This", "is", "a", "screenshot", "of", "the", "virtual", "kirana", "store", ".", "On", "the", "left", "side", "is", "a", "thin", "light", "blue", "sidebar", "with", "two", "items", "and", "their", "price", ":", "biscuits", "for", "rupees", "15", "and", "choco", "for", "rupees", "35", ".", "At", "the", "bottom", "of", "the", "side", "bar", "there", "is", "text", "that", "reads-", "bill", "rupees", "50", ".", "In", "the", "center", "is", "the", "virtual", "store", "with", "two", "brown", "wooden", "shelves", "and", "a", "wooden", "table", "in", "front", "of", "them", ".", "The", "top", "shelf", "has", "eggs", "for", "rupees", "20", ",", "milk", "for", "ruppes", "20", ",", "sliced", "brown", "bread", "of", "rupees", "15", "and", "yogurt", "for", "rupees", "15", ".", "The", "lower", "shelf", "only", "has", "a", "bag", "of", "chips", "for", "rupees", "10", "and", "butter", "for", "rupees", "30", ".", "The", "table", "has", "two", "items", "on", "its", "left", ":", "one", "packet", "of", "chocolate", "biscuits", "for", "rupees", "15", "and", "one", "bar", "of", "chocolate", "for", "rupees", "35.All", "the", "items", ",", "except", "eggs", ",", "are", "from", "brands", "commonly", "available", "in", "India", ".", "On", "the", "rigth", "side", "is", "a", "thin", "yellow", "sidebar", "which", "has", "Indian", "curreny", "/", "money", "placed", "on", "it", ".", "From", "the", "top", ",", "there", "is", "one", "50", "rupees", "note", ",", "20", "rupees", "note", ",", "10", "rupees", "note", "which", "is", "selected", "by", "an", "onscreen", "hand", "cursor", "so", "that", "this", "note", "is", "about", "40", "%", "bigger", "in", "size", "than", "the", "other", "notes", ".", "Under", "the", "10", "rupees", "note", "is", "a", "5", "rupees", "note", "and", "finally", "a", "5", "rupee", "coin", "and", "10", "rupee", "coin", ".", "At", "the", "bottom", "of", "the", "side", "bar", "there", "is", "text", "that", "reads-", "total", "is", "rupees", "100", "."]}, "caption": {"raw": "Figure 6: Kirana- buying items on the table by paying using a 10 rupees note", "tokens": ["Figure", "6", ":", "Kirana-", "buying", "items", "on", "the", "table", "by", "paying", "using", "a", "10", "rupees", "note"]}, "context": {"raw": "Gesture-based Interaction for Individuals with Developmental Disabilities in India Figure 6: Kirana- buying items on the table by paying using a 10 rupees note", "tokens": ["Gesture-based", "Interaction", "for", "Individuals", "with", "Developmental", "Disabilities", "in", "India", "Figure", "6", ":", "Kirana-", "buying", "items", "on", "the", "table", "by", "paying", "using", "a", "10", "rupees", "note"]}, "filename": "fc76954a4eca8adbdbec961d479c22e6d0d1a1cb_Image_008.jpg", "orig_filename": "fc76954a4eca8adbdbec961d479c22e6d0d1a1cb", "split": "train"}, {"article_id": "MEMEography: Understanding Users Through Internet Memes", "description": {"raw": "Stacked area chart of word count cluster amounts over time (2020) where amounts of Nurse word cluster is the highest throughout time. COVID-19 word cluster amounts increase strongly around March, April and May and decrease afterwards.", "tokens": ["Stacked", "area", "chart", "of", "word", "count", "cluster", "amounts", "over", "time", "(", "2020", ")", "where", "amounts", "of", "Nurse", "word", "cluster", "is", "the", "highest", "throughout", "time", ".", "COVID-19", "word", "cluster", "amounts", "increase", "strongly", "around", "March", ",", "April", "and", "May", "and", "decrease", "afterwards", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "MEMEography: Understanding Users Through Internet Memes ", "tokens": ["MEMEography", ":", "Understanding", "Users", "Through", "Internet", "Memes"]}, "filename": "9683f0243631f6b8ecaa8bece24ea0fb848f2dde_Image_006.jpg", "orig_filename": "9683f0243631f6b8ecaa8bece24ea0fb848f2dde", "split": "train"}, {"article_id": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "description": {"raw": "This figure is a graph of the battery power characteristics of the sensors. On the x-axis is time in days and on the y-axis is battery voltage in volts. There are three curves, all trending downward from approximately 4.2 volts. The ultra low standby curve slowly trends downwards over fifty to sixty days. The day-to-day curve trends down quicker over a two week period. Finally, the intensive use curve trends down even quicker still in three to four days.", "tokens": ["This", "figure", "is", "a", "graph", "of", "the", "battery", "power", "characteristics", "of", "the", "sensors", ".", "On", "the", "x-axis", "is", "time", "in", "days", "and", "on", "the", "y-axis", "is", "battery", "voltage", "in", "volts", ".", "There", "are", "three", "curves", ",", "all", "trending", "downward", "from", "approximately", "4.2", "volts", ".", "The", "ultra", "low", "standby", "curve", "slowly", "trends", "downwards", "over", "fifty", "to", "sixty", "days", ".", "The", "day-to-day", "curve", "trends", "down", "quicker", "over", "a", "two", "week", "period", ".", "Finally", ",", "the", "intensive", "use", "curve", "trends", "down", "even", "quicker", "still", "in", "three", "to", "four", "days", "."]}, "caption": {"raw": "Figure 3. Iteration of sensor hardware over time.", "tokens": ["Figure", "3", ".", "Iteration", "of", "sensor", "hardware", "over", "time", "."]}, "context": {"raw": "MANA: Designing and Validating a User-Centered Mobility Analysis System Figure 3. Iteration of sensor hardware over time.", "tokens": ["MANA", ":", "Designing", "and", "Validating", "a", "User-Centered", "Mobility", "Analysis", "System", "Figure", "3", ".", "Iteration", "of", "sensor", "hardware", "over", "time", "."]}, "filename": "2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_004.jpg", "orig_filename": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "split": "train"}, {"article_id": "Digital Ventriloquism: Giving Voice to Everyday Objects", "description": {"raw": "A chart showing the different positions of microphone placement with respect to a placed object ranging from 0 to 180 degrees.", "tokens": ["A", "chart", "showing", "the", "different", "positions", "of", "microphone", "placement", "with", "respect", "to", "a", "placed", "object", "ranging", "from", "0", "to", "180", "degrees", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Digital Ventriloquism: Giving Voice to Everyday Objects ", "tokens": ["Digital", "Ventriloquism", ":", "Giving", "Voice", "to", "Everyday", "Objects"]}, "filename": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7_Image_006.jpg", "orig_filename": "66c87d70cd5e0981ad4c7ae85dff260b2c7e90d7", "split": "train"}, {"article_id": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "description": {"raw": "This line graph shows the training and testing accuracy of our multimodal model over 25 epochs.  The training accuracy consistently rises, but the testing accuracy peaks quickly at around 2 epochs and then falls due to the model overfitting to the training data.", "tokens": ["This", "line", "graph", "shows", "the", "training", "and", "testing", "accuracy", "of", "our", "multimodal", "model", "over", "25", "epochs", ".", "The", "training", "accuracy", "consistently", "rises", ",", "but", "the", "testing", "accuracy", "peaks", "quickly", "at", "around", "2", "epochs", "and", "then", "falls", "due", "to", "the", "model", "overfitting", "to", "the", "training", "data", "."]}, "caption": {"raw": "Multimodal Accuracy", "tokens": ["Multimodal", "Accuracy"]}, "context": {"raw": "Multimodal Deep Learning using Images and Text for Information Graphic Classification Multimodal Accuracy", "tokens": ["Multimodal", "Deep", "Learning", "using", "Images", "and", "Text", "for", "Information", "Graphic", "Classification", "Multimodal", "Accuracy"]}, "filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_008.jpg", "orig_filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "split": "train"}, {"article_id": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "description": {"raw": "The results of the questionaire shown as a bar chart. All bar look quiete similar except for ease of use, efficency to use, user experience and zooming.", "tokens": ["The", "results", "of", "the", "questionaire", "shown", "as", "a", "bar", "chart", ".", "All", "bar", "look", "quiete", "similar", "except", "for", "ease", "of", "use", ",", "efficency", "to", "use", ",", "user", "experience", "and", "zooming", "."]}, "caption": {"raw": "Figure 8: Number of discrete actions (gender neutral).", "tokens": ["Figure", "8", ":", "Number", "of", "discrete", "actions", "(", "gender", "neutral", ")", "."]}, "context": {"raw": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays Figure 8: Number of discrete actions (gender neutral).", "tokens": ["Pinch-drag-flick", "vs.", "spatial", "input", ":", "rethinking", "zoom", "&", "pan", "on", "mobile", "displays", "Figure", "8", ":", "Number", "of", "discrete", "actions", "(", "gender", "neutral", ")", "."]}, "filename": "f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_010.jpg", "orig_filename": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "split": "train"}, {"article_id": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People", "description": {"raw": "The bar chart shows the average selection error (y-axis, in percent) for all feedback conditions in Study 2, and contains 10 vertical bars, one for each condition. The x-axis groups bars by the feedback designs, from left: Geiger, Pitch, Constant and Control. There are three coloured bars in the Geiger, Pitch and Constant, one for each of the Wrist, Object and Both locations. The Control Design only has a single bar (for Object). The values, from left to right, are as follows: Geiger-Wrist = 35.4%, Geiger-Object = 22.9%, Geiger-Both = 14.6%; Pitch-Wrist = 35.4%, Pitch-Object = 27.1%, Pitch-Both = 45.8%; Constant-Wrist = 37.5%, Constant-Object = 20.8%, Constant-Both = 33.3%; Control-Object = 54.2%.", "tokens": ["The", "bar", "chart", "shows", "the", "average", "selection", "error", "(", "y-axis", ",", "in", "percent", ")", "for", "all", "feedback", "conditions", "in", "Study", "2", ",", "and", "contains", "10", "vertical", "bars", ",", "one", "for", "each", "condition", ".", "The", "x-axis", "groups", "bars", "by", "the", "feedback", "designs", ",", "from", "left", ":", "Geiger", ",", "Pitch", ",", "Constant", "and", "Control", ".", "There", "are", "three", "coloured", "bars", "in", "the", "Geiger", ",", "Pitch", "and", "Constant", ",", "one", "for", "each", "of", "the", "Wrist", ",", "Object", "and", "Both", "locations", ".", "The", "Control", "Design", "only", "has", "a", "single", "bar", "(", "for", "Object", ")", ".", "The", "values", ",", "from", "left", "to", "right", ",", "are", "as", "follows", ":", "Geiger-Wrist", "=", "35.4", "%", ",", "Geiger-Object", "=", "22.9", "%", ",", "Geiger-Both", "=", "14.6", "%", ";", "Pitch-Wrist", "=", "35.4", "%", ",", "Pitch-Object", "=", "27.1", "%", ",", "Pitch-Both", "=", "45.8", "%", ";", "Constant-Wrist", "=", "37.5", "%", ",", "Constant-Object", "=", "20.8", "%", ",", "Constant-Both", "=", "33.3", "%", ";", "Control-Object", "=", "54.2", "%", "."]}, "caption": {"raw": "Figure 7: Mean Study 2 target selection error for all Feedback Designs and Speaker Locations. Error bars = 95% CI.", "tokens": ["Figure", "7", ":", "Mean", "Study", "2", "target", "selection", "error", "for", "all", "Feedback", "Designs", "and", "Speaker", "Locations", ".", "Error", "bars", "=", "95", "%", "CI", "."]}, "context": {"raw": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People Figure 7: Mean Study 2 target selection error for all Feedback Designs and Speaker Locations. Error bars = 95% CI.", "tokens": ["Using", "Dynamic", "Audio", "Feedback", "to", "Support", "Peripersonal", "Reaching", "in", "Young", "Visually", "Impaired", "People", "Figure", "7", ":", "Mean", "Study", "2", "target", "selection", "error", "for", "all", "Feedback", "Designs", "and", "Speaker", "Locations", ".", "Error", "bars", "=", "95", "%", "CI", "."]}, "filename": "09f444597b50aebe550b5efd1368a55953d8ddd1_Image_007.jpg", "orig_filename": "09f444597b50aebe550b5efd1368a55953d8ddd1", "split": "train"}, {"article_id": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "description": {"raw": "Frequency of the participants' attempts to delete the visual feedback characters throughout Experiment 3. The error bars in the graph indicate standard deviation.", "tokens": ["Frequency", "of", "the", "participants", "'", "attempts", "to", "delete", "the", "visual", "feedback", "characters", "throughout", "Experiment", "3", ".", "The", "error", "bars", "in", "the", "graph", "indicate", "standard", "deviation", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard ", "tokens": ["Diagnosing", "and", "Coping", "with", "Mode", "Errors", "in", "Korean-English", "Dual-language", "Keyboard"]}, "filename": "408a505662902bbaf20ef32ac6deaf7a78e52650_Image_021.jpg", "orig_filename": "408a505662902bbaf20ef32ac6deaf7a78e52650", "split": "train"}, {"article_id": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment", "description": {"raw": "Prevalence of Disease Determinants. A bar chart where each of the Scanning Errors (determinants) is shown with the number of apps out of 100 that were detected having that error. Values as follows in order the bars appear:    Item Description: 85 apps. Text contrast: 94 apps. Item Label: 94 apps. Item type label 20 apps. Clickable items: 57 apps. Touch Target: 95 apps. Image contrast: 85 apps. Editable Item Label: 10 apps. Link: 1 app.", "tokens": ["Prevalence", "of", "Disease", "Determinants", ".", "A", "bar", "chart", "where", "each", "of", "the", "Scanning", "Errors", "(", "determinants", ")", "is", "shown", "with", "the", "number", "of", "apps", "out", "of", "100", "that", "were", "detected", "having", "that", "error", ".", "Values", "as", "follows", "in", "order", "the", "bars", "appear", ":", "Item", "Description", ":", "85", "apps", ".", "Text", "contrast", ":", "94", "apps", ".", "Item", "Label", ":", "94", "apps", ".", "Item", "type", "label", "20", "apps", ".", "Clickable", "items", ":", "57", "apps", ".", "Touch", "Target", ":", "95", "apps", ".", "Image", "contrast", ":", "85", "apps", ".", "Editable", "Item", "Label", ":", "10", "apps", ".", "Link", ":", "1", "app", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment ", "tokens": ["Epidemiology", "as", "a", "Framework", "for", "Large-Scale", "Mobile", "Application", "Accessibility", "Assessment"]}, "filename": "0fe0270f4de60836fdb150646a6f7d730c8f564d_Image_005.jpg", "orig_filename": "0fe0270f4de60836fdb150646a6f7d730c8f564d", "split": "train"}, {"article_id": "Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills", "description": {"raw": "This figure shows a stacked column graph. The columns are grouped by activity (left to right): Colors, Money, Numbers, and Shapes. Within each activity there is a column for each participant (who demonstrated any reaction while using this activity) which depicts the quantity of negative (black) and positive (gray) reactions he/she demonstrated while trying that activity.\n\nParticipants’ reactions during the Colors activity: For P1, 2 out of 2 were positive. For P2, 86 out of 95 were positive. For P3, 1 out of 1 were positive. For P4, 1 out of 1 were negative. For P5, 21 out of 25 were positive. For P6, 30 out of 30 were positive. P7 did not demonstrate any reactions. For P8, 22 out of 22 were positive. For P9, 31 out of 32 were positive.\n\nParticipants’ reactions during the Money activity: For P1, 5 out of 5 were positive. For P2, 39 out of 39 were positive. For P3, 14 out of 14 were positive. For P4, 2 out of 3 were positive. For P5, 6 out of 6 were positive. For P6, 1 out of 1 were positive. P7 did not demonstrate any reactions. For P8, 13 out of 13 were positive. For P9, 21 out of 21 were positive. P10 did not demonstrate any reactions.\n\nParticipants’ reactions during the Numbers activity: For P1, 8 out of 8 were positive. For P2, 64 out of 73 were positive. For P3, 2 out of 2 were positive. For P4, 10 out of 11 were positive. For P5, 21 out of 24 were positive. For P6, 19 out of 19 were positive. P7 did not demonstrate any reactions. For P8, 13 out of 13 were positive. For P9, 10 out of 10 were positive. P10 did not demonstrate any reactions.\n\nParticipants’ reactions during the Shapes activity: For P1, 5 out of 5 were positive. For P2, 58 out of 59 were positive. For P3, 6 out of 6 were positive. For P4, 10 out of 11 were positive. For P5, 7 out of 8 were positive. For P6, 10 out of 10 were positive. P7 did not demonstrate any reactions. For P8, 8 out of 8 were positive. For P9, 31 out of 31 were positive.", "tokens": ["This", "figure", "shows", "a", "stacked", "column", "graph", ".", "The", "columns", "are", "grouped", "by", "activity", "(", "left", "to", "right", ")", ":", "Colors", ",", "Money", ",", "Numbers", ",", "and", "Shapes", ".", "Within", "each", "activity", "there", "is", "a", "column", "for", "each", "participant", "(", "who", "demonstrated", "any", "reaction", "while", "using", "this", "activity", ")", "which", "depicts", "the", "quantity", "of", "negative", "(", "black", ")", "and", "positive", "(", "gray", ")", "reactions", "he/she", "demonstrated", "while", "trying", "that", "activity", ".", "Participants", "’", "reactions", "during", "the", "Colors", "activity", ":", "For", "P1", ",", "2", "out", "of", "2", "were", "positive", ".", "For", "P2", ",", "86", "out", "of", "95", "were", "positive", ".", "For", "P3", ",", "1", "out", "of", "1", "were", "positive", ".", "For", "P4", ",", "1", "out", "of", "1", "were", "negative", ".", "For", "P5", ",", "21", "out", "of", "25", "were", "positive", ".", "For", "P6", ",", "30", "out", "of", "30", "were", "positive", ".", "P7", "did", "not", "demonstrate", "any", "reactions", ".", "For", "P8", ",", "22", "out", "of", "22", "were", "positive", ".", "For", "P9", ",", "31", "out", "of", "32", "were", "positive", ".", "Participants", "’", "reactions", "during", "the", "Money", "activity", ":", "For", "P1", ",", "5", "out", "of", "5", "were", "positive", ".", "For", "P2", ",", "39", "out", "of", "39", "were", "positive", ".", "For", "P3", ",", "14", "out", "of", "14", "were", "positive", ".", "For", "P4", ",", "2", "out", "of", "3", "were", "positive", ".", "For", "P5", ",", "6", "out", "of", "6", "were", "positive", ".", "For", "P6", ",", "1", "out", "of", "1", "were", "positive", ".", "P7", "did", "not", "demonstrate", "any", "reactions", ".", "For", "P8", ",", "13", "out", "of", "13", "were", "positive", ".", "For", "P9", ",", "21", "out", "of", "21", "were", "positive", ".", "P10", "did", "not", "demonstrate", "any", "reactions", ".", "Participants", "’", "reactions", "during", "the", "Numbers", "activity", ":", "For", "P1", ",", "8", "out", "of", "8", "were", "positive", ".", "For", "P2", ",", "64", "out", "of", "73", "were", "positive", ".", "For", "P3", ",", "2", "out", "of", "2", "were", "positive", ".", "For", "P4", ",", "10", "out", "of", "11", "were", "positive", ".", "For", "P5", ",", "21", "out", "of", "24", "were", "positive", ".", "For", "P6", ",", "19", "out", "of", "19", "were", "positive", ".", "P7", "did", "not", "demonstrate", "any", "reactions", ".", "For", "P8", ",", "13", "out", "of", "13", "were", "positive", ".", "For", "P9", ",", "10", "out", "of", "10", "were", "positive", ".", "P10", "did", "not", "demonstrate", "any", "reactions", ".", "Participants", "’", "reactions", "during", "the", "Shapes", "activity", ":", "For", "P1", ",", "5", "out", "of", "5", "were", "positive", ".", "For", "P2", ",", "58", "out", "of", "59", "were", "positive", ".", "For", "P3", ",", "6", "out", "of", "6", "were", "positive", ".", "For", "P4", ",", "10", "out", "of", "11", "were", "positive", ".", "For", "P5", ",", "7", "out", "of", "8", "were", "positive", ".", "For", "P6", ",", "10", "out", "of", "10", "were", "positive", ".", "P7", "did", "not", "demonstrate", "any", "reactions", ".", "For", "P8", ",", "8", "out", "of", "8", "were", "positive", ".", "For", "P9", ",", "31", "out", "of", "31", "were", "positive", "."]}, "caption": {"raw": "Figure 5. Positive and Negative Reactions with the Colors, Money, Numbers, and Shapes activities.", "tokens": ["Figure", "5", ".", "Positive", "and", "Negative", "Reactions", "with", "the", "Colors", ",", "Money", ",", "Numbers", ",", "and", "Shapes", "activities", "."]}, "context": {"raw": "Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills Figure 5. Positive and Negative Reactions with the Colors, Money, Numbers, and Shapes activities.", "tokens": ["Online", "Learning", "System", "to", "Help", "People", "with", "Developmental", "Disabilities", "Reinforce", "Basic", "Skills", "Figure", "5", ".", "Positive", "and", "Negative", "Reactions", "with", "the", "Colors", ",", "Money", ",", "Numbers", ",", "and", "Shapes", "activities", "."]}, "filename": "dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8_Image_008.png", "orig_filename": "dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8", "split": "train"}, {"article_id": "Exploring Text Revision with Backspace and Caret in Virtual Reality", "description": {"raw": "Bar chart with error bars presents the caret control time for four VR text revision techniques when dealing with different types of revision targets. Overall, using the discrete caret control requires more time to navigate the caret than the continuous caret control.", "tokens": ["Bar", "chart", "with", "error", "bars", "presents", "the", "caret", "control", "time", "for", "four", "VR", "text", "revision", "techniques", "when", "dealing", "with", "different", "types", "of", "revision", "targets", ".", "Overall", ",", "using", "the", "discrete", "caret", "control", "requires", "more", "time", "to", "navigate", "the", "caret", "than", "the", "continuous", "caret", "control", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Exploring Text Revision with Backspace and Caret in Virtual Reality ", "tokens": ["Exploring", "Text", "Revision", "with", "Backspace", "and", "Caret", "in", "Virtual", "Reality"]}, "filename": "5894fd4581a79a7067102891bc3db5738195941f_Image_007.jpg", "orig_filename": "5894fd4581a79a7067102891bc3db5738195941f", "split": "train"}, {"article_id": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "description": {"raw": "A bar graph titled Figure 14: H2-f for Comprehension Quiz Success (Mult. Choice). The vertical axis ranges from 0% to 100%, with ticks at increments of 10%. On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. All of the pairwise comparisons were significant at three stars. Please refer the subsection on the current page for the means and standard errors for the WRAT levels.", "tokens": ["A", "bar", "graph", "titled", "Figure", "14", ":", "H2-f", "for", "Comprehension", "Quiz", "Success", "(", "Mult", ".", "Choice", ")", ".", "The", "vertical", "axis", "ranges", "from", "0", "%", "to", "100", "%", ",", "with", "ticks", "at", "increments", "of", "10", "%", ".", "On", "the", "horizontal", "axis", "are", "the", "labels", "for", "the", "WRAT", "accuracy", "levels", "in", "the", "study", ":", "WRAT-L", ",", "WRAT-M", ",", "and", "WRAT-H.", "All", "of", "the", "pairwise", "comparisons", "were", "significant", "at", "three", "stars", ".", "Please", "refer", "the", "subsection", "on", "the", "current", "page", "for", "the", "means", "and", "standard", "errors", "for", "the", "WRAT", "levels", "."]}, "caption": {"raw": "Figure 14. H2-f for Comprehension Quiz Success (Mult. Choice)", "tokens": ["Figure", "14", ".", "H2-f", "for", "Comprehension", "Quiz", "Success", "(", "Mult", ".", "Choice", ")"]}, "context": {"raw": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels Figure 14. H2-f for Comprehension Quiz Success (Mult. Choice)", "tokens": ["Methods", "for", "Evaluation", "of", "Imperfect", "Captioning", "Tools", "by", "Deaf", "or", "Hard-of-Hearing", "Users", "at", "Different", "Reading", "Literacy", "Levels", "Figure", "14", ".", "H2-f", "for", "Comprehension", "Quiz", "Success", "(", "Mult", ".", "Choice", ")"]}, "filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_010.jpg", "orig_filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "split": "train"}, {"article_id": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment", "description": {"raw": "Number of Determinants per App. A bar chart of the number of apps out of 100 that presented with a given number of determinants out of the nine screened for.    Number of apps that presented:  zero determinants: 0 apps. one determinant: 3 apps. two determinants: 2 apps. three determinants: 2 apps. four determinanats: 9 apps. five determinants: 36 apps. six determinants: 36 apps. seven determinants: 10 apps. eight determinants: 3 apps. all nine determinants: 0 apps.", "tokens": ["Number", "of", "Determinants", "per", "App", ".", "A", "bar", "chart", "of", "the", "number", "of", "apps", "out", "of", "100", "that", "presented", "with", "a", "given", "number", "of", "determinants", "out", "of", "the", "nine", "screened", "for", ".", "Number", "of", "apps", "that", "presented", ":", "zero", "determinants", ":", "0", "apps", ".", "one", "determinant", ":", "3", "apps", ".", "two", "determinants", ":", "2", "apps", ".", "three", "determinants", ":", "2", "apps", ".", "four", "determinanats", ":", "9", "apps", ".", "five", "determinants", ":", "36", "apps", ".", "six", "determinants", ":", "36", "apps", ".", "seven", "determinants", ":", "10", "apps", ".", "eight", "determinants", ":", "3", "apps", ".", "all", "nine", "determinants", ":", "0", "apps", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment ", "tokens": ["Epidemiology", "as", "a", "Framework", "for", "Large-Scale", "Mobile", "Application", "Accessibility", "Assessment"]}, "filename": "0fe0270f4de60836fdb150646a6f7d730c8f564d_Image_006.jpg", "orig_filename": "0fe0270f4de60836fdb150646a6f7d730c8f564d", "split": "train"}, {"article_id": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture", "description": {"raw": "Figure 8 is a chart illustrating participants' ratings of how much effort it took to document knowledge for think-aloud and traditional documentation techniques. The chart has 2 mini bar charts. On the left is the bar chart for the think-aloud condition. It has 3 bars, from left: a red bar of height 3 (i.e., 3 participants out of 12) for the category of \"high effort\", a yellow bar of height 3 for \"medium effort\", and a green bar of height 6 for \"low effort\". On the right is the bar chart for the traditional documentation condition. It has 3 bars, from left: a red bar of height 4 for the \"high effort\", a yellow bar of height 2 for \"medium effort\", and a green bar of height 6 for \"low effort\". Note that these categories are aggregations of the participant ratings from the 7-point Likert scale results (between 1-Very Low Effort, and 7-Very High Effort). Ratings (1, 2, 3) have been aggregated into \"low effort\", rating (4) is \"medium effort\", and ratings (5, 6, 7) have been aggregated into \"high effort\".", "tokens": ["Figure", "8", "is", "a", "chart", "illustrating", "participants", "'", "ratings", "of", "how", "much", "effort", "it", "took", "to", "document", "knowledge", "for", "think-aloud", "and", "traditional", "documentation", "techniques", ".", "The", "chart", "has", "2", "mini", "bar", "charts", ".", "On", "the", "left", "is", "the", "bar", "chart", "for", "the", "think-aloud", "condition", ".", "It", "has", "3", "bars", ",", "from", "left", ":", "a", "red", "bar", "of", "height", "3", "(", "i.e.", ",", "3", "participants", "out", "of", "12", ")", "for", "the", "category", "of", "``", "high", "effort", "''", ",", "a", "yellow", "bar", "of", "height", "3", "for", "``", "medium", "effort", "''", ",", "and", "a", "green", "bar", "of", "height", "6", "for", "``", "low", "effort", "''", ".", "On", "the", "right", "is", "the", "bar", "chart", "for", "the", "traditional", "documentation", "condition", ".", "It", "has", "3", "bars", ",", "from", "left", ":", "a", "red", "bar", "of", "height", "4", "for", "the", "``", "high", "effort", "''", ",", "a", "yellow", "bar", "of", "height", "2", "for", "``", "medium", "effort", "''", ",", "and", "a", "green", "bar", "of", "height", "6", "for", "``", "low", "effort", "''", ".", "Note", "that", "these", "categories", "are", "aggregations", "of", "the", "participant", "ratings", "from", "the", "7-point", "Likert", "scale", "results", "(", "between", "1-Very", "Low", "Effort", ",", "and", "7-Very", "High", "Effort", ")", ".", "Ratings", "(", "1", ",", "2", ",", "3", ")", "have", "been", "aggregated", "into", "``", "low", "effort", "''", ",", "rating", "(", "4", ")", "is", "``", "medium", "effort", "''", ",", "and", "ratings", "(", "5", ",", "6", ",", "7", ")", "have", "been", "aggregated", "into", "``", "high", "effort", "''", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture ", "tokens": ["Think-Aloud", "Computing", ":", "Supporting", "Rich", "and", "Low-Effort", "Knowledge", "Capture"]}, "filename": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf_Image_018.jpg", "orig_filename": "8f18fdea66d3c9439b3ecd0deb656be5f4d2dbcf", "split": "train"}, {"article_id": "Pupil responses during discrete goal-directed movements", "description": {"raw": "Figure 11. Box-whisker plot for Mean duration from tooltip-reach to the moment where the pupil peaked in size for three difficulty IDs.", "tokens": ["Figure", "11", ".", "Box-whisker", "plot", "for", "Mean", "duration", "from", "tooltip-reach", "to", "the", "moment", "where", "the", "pupil", "peaked", "in", "size", "for", "three", "difficulty", "IDs", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Pupil responses during discrete goal-directed movements ", "tokens": ["Pupil", "responses", "during", "discrete", "goal-directed", "movements"]}, "filename": "1a91496ad6d8adf41f12b157343321732872c0c1_Image_014.jpg", "orig_filename": "1a91496ad6d8adf41f12b157343321732872c0c1", "split": "train"}, {"article_id": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "description": {"raw": "This figure presents a line chart with the average WPM for each method, for three age of onset groups (<=5, 6-20 and >=21). The early blind group was slower in every method. THe group with an age of onset between 6 and 20 is the fastest. These differeces in performance are more pronounced in QWERTY and MultiTap than with NavTouch and BrailleTouch.", "tokens": ["This", "figure", "presents", "a", "line", "chart", "with", "the", "average", "WPM", "for", "each", "method", ",", "for", "three", "age", "of", "onset", "groups", "(", "<", "=5", ",", "6-20", "and", ">", "=21", ")", ".", "The", "early", "blind", "group", "was", "slower", "in", "every", "method", ".", "THe", "group", "with", "an", "age", "of", "onset", "between", "6", "and", "20", "is", "the", "fastest", ".", "These", "differeces", "in", "performance", "are", "more", "pronounced", "in", "QWERTY", "and", "MultiTap", "than", "with", "NavTouch", "and", "BrailleTouch", "."]}, "caption": {"raw": "Figure 6. Age of onset impact on WPM.", "tokens": ["Figure", "6", ".", "Age", "of", "onset", "impact", "on", "WPM", "."]}, "context": {"raw": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors Figure 6. Age of onset impact on WPM.", "tokens": ["Blind", "people", "and", "mobile", "touch-based", "text-entry", ":", "acknowledging", "the", "need", "for", "different", "flavors", "Figure", "6", ".", "Age", "of", "onset", "impact", "on", "WPM", "."]}, "filename": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_007.jpg", "orig_filename": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9", "split": "train"}, {"article_id": "Tilt-Responsive Techniques for Digital Drawing Boards", "description": {"raw": "Figure 4 shows the \"App Bar\" which is used to switch between the different applications in our prototype window manager, simply by tapping an icon via direct touch. This app bar appears at the bottom center of the screen.", "tokens": ["Figure", "4", "shows", "the", "``", "App", "Bar", "''", "which", "is", "used", "to", "switch", "between", "the", "different", "applications", "in", "our", "prototype", "window", "manager", ",", "simply", "by", "tapping", "an", "icon", "via", "direct", "touch", ".", "This", "app", "bar", "appears", "at", "the", "bottom", "center", "of", "the", "screen", "."]}, "caption": {"raw": "Figure 4. The App Bar (at bottom center of screen) lets users tap to switch between various task scenarios in our prototype.", "tokens": ["Figure", "4", ".", "The", "App", "Bar", "(", "at", "bottom", "center", "of", "screen", ")", "lets", "users", "tap", "to", "switch", "between", "various", "task", "scenarios", "in", "our", "prototype", "."]}, "context": {"raw": "Tilt-Responsive Techniques for Digital Drawing Boards Figure 4. The App Bar (at bottom center of screen) lets users tap to switch between various task scenarios in our prototype.", "tokens": ["Tilt-Responsive", "Techniques", "for", "Digital", "Drawing", "Boards", "Figure", "4", ".", "The", "App", "Bar", "(", "at", "bottom", "center", "of", "screen", ")", "lets", "users", "tap", "to", "switch", "between", "various", "task", "scenarios", "in", "our", "prototype", "."]}, "filename": "a6ee8d3cb7469494d485a718807d501b22e48be3_Image_003.jpg", "orig_filename": "a6ee8d3cb7469494d485a718807d501b22e48be3", "split": "train"}, {"article_id": "What Makes Smartphone Use Meaningful or Meaningless?", "description": {"raw": "This figure is a bar chart of the average level of  meaningfulness for each of 5 different types of smartphone uses and gratifications. Meaningfulness is shown on the y-axis on a 1-7 point scale. These values are: Productivity (3.6), Information (3.3), Communication (3.3), Entertainment (2.5), and Social Media (2.4).", "tokens": ["This", "figure", "is", "a", "bar", "chart", "of", "the", "average", "level", "of", "meaningfulness", "for", "each", "of", "5", "different", "types", "of", "smartphone", "uses", "and", "gratifications", ".", "Meaningfulness", "is", "shown", "on", "the", "y-axis", "on", "a", "1-7", "point", "scale", ".", "These", "values", "are", ":", "Productivity", "(", "3.6", ")", ",", "Information", "(", "3.3", ")", ",", "Communication", "(", "3.3", ")", ",", "Entertainment", "(", "2.5", ")", ",", "and", "Social", "Media", "(", "2.4", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "What Makes Smartphone Use Meaningful or Meaningless? ", "tokens": ["What", "Makes", "Smartphone", "Use", "Meaningful", "or", "Meaningless", "?"]}, "filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_005.jpg", "orig_filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "split": "train"}, {"article_id": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "description": {"raw": "Figure 3: Three visualizations. At the top, labeled (a), a bar chart of movie Genres on y and counts on x, along with the Dziban code to generate it (`genre = Chart(movies).field('Major_Genre'`) then new line, `genre`). To the left, labeled (b), a binned circle plot with Genre on x, MPAA Rating on y, and count on size, a cold recommendation (`genre.field('MPAA_Rating')`). To the right, labeled (c) a stacked bar chart with Genre on y, counts on x, and MPAA Rating on color---an anchored recommendation (`genre.anchor().field('MPAA_Rating')`).", "tokens": ["Figure", "3", ":", "Three", "visualizations", ".", "At", "the", "top", ",", "labeled", "(", "a", ")", ",", "a", "bar", "chart", "of", "movie", "Genres", "on", "y", "and", "counts", "on", "x", ",", "along", "with", "the", "Dziban", "code", "to", "generate", "it", "(", "`", "genre", "=", "Chart", "(", "movies", ")", ".field", "(", "'Major_Genre", "'", "`", ")", "then", "new", "line", ",", "`", "genre", "`", ")", ".", "To", "the", "left", ",", "labeled", "(", "b", ")", ",", "a", "binned", "circle", "plot", "with", "Genre", "on", "x", ",", "MPAA", "Rating", "on", "y", ",", "and", "count", "on", "size", ",", "a", "cold", "recommendation", "(", "`", "genre.field", "(", "'MPAA_Rating", "'", ")", "`", ")", ".", "To", "the", "right", ",", "labeled", "(", "c", ")", "a", "stacked", "bar", "chart", "with", "Genre", "on", "y", ",", "counts", "on", "x", ",", "and", "MPAA", "Rating", "on", "color", "--", "-an", "anchored", "recommendation", "(", "`", "genre.anchor", "(", ")", ".field", "(", "'MPAA_Rating", "'", ")", "`", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations ", "tokens": ["Dziban", ":", "Balancing", "Agency", "&", "Automation", "in", "Visualization", "Design", "via", "Anchored", "Recommendations"]}, "filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_012.png", "orig_filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "split": "train"}, {"article_id": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students", "description": {"raw": "Fig.1  The left most side of the figure shows the data collection stage, including seven type of data: bluetooth, call logs, screen status, location coordination, campus map, sleep logs and step logs. The grounded truth is from the BDI-II questionnaire. The middle part of the figure shows the feature extraction from the raw data. Then the features is fed into association rule minings. The output rules are fed back on the features to compute contextually filtered features. Finally, the right side of the figure shows the training stage.", "tokens": ["Fig.1", "The", "left", "most", "side", "of", "the", "figure", "shows", "the", "data", "collection", "stage", ",", "including", "seven", "type", "of", "data", ":", "bluetooth", ",", "call", "logs", ",", "screen", "status", ",", "location", "coordination", ",", "campus", "map", ",", "sleep", "logs", "and", "step", "logs", ".", "The", "grounded", "truth", "is", "from", "the", "BDI-II", "questionnaire", ".", "The", "middle", "part", "of", "the", "figure", "shows", "the", "feature", "extraction", "from", "the", "raw", "data", ".", "Then", "the", "features", "is", "fed", "into", "association", "rule", "minings", ".", "The", "output", "rules", "are", "fed", "back", "on", "the", "features", "to", "compute", "contextually", "filtered", "features", ".", "Finally", ",", "the", "right", "side", "of", "the", "figure", "shows", "the", "training", "stage", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students ", "tokens": ["Leveraging", "Routine", "Behavior", "and", "Contextually-Filtered", "Features", "for", "Depression", "Detection", "among", "College", "Students"]}, "filename": "fbd915aa1143821e150396f6e9ac20f2134c400d_Image_004.gif", "orig_filename": "fbd915aa1143821e150396f6e9ac20f2134c400d", "split": "train"}, {"article_id": "Toward User-Driven Sound Recognizer Personalization with People Who Are d/Deaf or Hard of Hearing", "description": {"raw": "Six sound features, labled A through G. A shows the loudness of a sound with a bar meter and dynamic wave design, B provides feedback on the level of background noise with a text status or bar, C shows how many sounds are co-occurring with a text status, D identifies if a recording has background noise with a text status or waveform overlay, E identifies the presences of co-occuring sounds with a text status or waveform overlay, F assesses the quality of a sound with a text status.", "tokens": ["Six", "sound", "features", ",", "labled", "A", "through", "G.", "A", "shows", "the", "loudness", "of", "a", "sound", "with", "a", "bar", "meter", "and", "dynamic", "wave", "design", ",", "B", "provides", "feedback", "on", "the", "level", "of", "background", "noise", "with", "a", "text", "status", "or", "bar", ",", "C", "shows", "how", "many", "sounds", "are", "co-occurring", "with", "a", "text", "status", ",", "D", "identifies", "if", "a", "recording", "has", "background", "noise", "with", "a", "text", "status", "or", "waveform", "overlay", ",", "E", "identifies", "the", "presences", "of", "co-occuring", "sounds", "with", "a", "text", "status", "or", "waveform", "overlay", ",", "F", "assesses", "the", "quality", "of", "a", "sound", "with", "a", "text", "status", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Toward User-Driven Sound Recognizer Personalization with People Who Are d/Deaf or Hard of Hearing ", "tokens": ["Toward", "User-Driven", "Sound", "Recognizer", "Personalization", "with", "People", "Who", "Are", "d/Deaf", "or", "Hard", "of", "Hearing"]}, "filename": "56f28fa646fe7fc259ec368577db46f0acc317a5_Image_007.jpg", "orig_filename": "56f28fa646fe7fc259ec368577db46f0acc317a5", "split": "train"}, {"article_id": "Assessing Virtual Assistant Capabilities with Italian Dysarthric Speech", "description": {"raw": "A box plot showing the WER distribution for Google Assistant, Cortana and Siri. As reported in the text, Google Assistant and Cortana exhibit a lower WER than Siri, with median values of 0, 0.4, and 0.89 respectively.", "tokens": ["A", "box", "plot", "showing", "the", "WER", "distribution", "for", "Google", "Assistant", ",", "Cortana", "and", "Siri", ".", "As", "reported", "in", "the", "text", ",", "Google", "Assistant", "and", "Cortana", "exhibit", "a", "lower", "WER", "than", "Siri", ",", "with", "median", "values", "of", "0", ",", "0.4", ",", "and", "0.89", "respectively", "."]}, "caption": {"raw": "Figure 1. WER distribution for each assistant", "tokens": ["Figure", "1", ".", "WER", "distribution", "for", "each", "assistant"]}, "context": {"raw": "Assessing Virtual Assistant Capabilities with Italian Dysarthric Speech Figure 1. WER distribution for each assistant", "tokens": ["Assessing", "Virtual", "Assistant", "Capabilities", "with", "Italian", "Dysarthric", "Speech", "Figure", "1", ".", "WER", "distribution", "for", "each", "assistant"]}, "filename": "44d33f99a5a9f23387691f99abba5bfc8942f2fb_Image_001.jpg", "orig_filename": "44d33f99a5a9f23387691f99abba5bfc8942f2fb", "split": "train"}, {"article_id": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling", "description": {"raw": "Bar chart showing subjective ratings of the unlockable features on usefulness, fun, and how annoying or disruptive they were perceived to be.", "tokens": ["Bar", "chart", "showing", "subjective", "ratings", "of", "the", "unlockable", "features", "on", "usefulness", ",", "fun", ",", "and", "how", "annoying", "or", "disruptive", "they", "were", "perceived", "to", "be", "."]}, "caption": {"raw": "Figure 8. Subjective ratings of the unlockable features.", "tokens": ["Figure", "8", ".", "Subjective", "ratings", "of", "the", "unlockable", "features", "."]}, "context": {"raw": "Blocks-to-CAD: A Cross-Application Bridge from Minecraft to 3D Modeling Figure 8. Subjective ratings of the unlockable features.", "tokens": ["Blocks-to-CAD", ":", "A", "Cross-Application", "Bridge", "from", "Minecraft", "to", "3D", "Modeling", "Figure", "8", ".", "Subjective", "ratings", "of", "the", "unlockable", "features", "."]}, "filename": "148da4b210ceaa6d9699fd242a97d0739e460e12_Image_009.gif", "orig_filename": "148da4b210ceaa6d9699fd242a97d0739e460e12", "split": "train"}, {"article_id": "Consensus Building in Collaborative Sequencing with Visual Awareness", "description": {"raw": "Figure 8: \"A double bar graph showing the average survey responses for both the Treatment and Control condition. The vertical axis is labelled with the metric measured by the survey questions. 'Perceived Efficiency' and 'Perceived Effectiveness' were both higher in the Treatment condition when compared to the Control condition, and 'Aggregated NASA-TLX' was lower in the Treatment condition when compared to the Control condition.\"", "tokens": ["Figure", "8", ":", "``", "A", "double", "bar", "graph", "showing", "the", "average", "survey", "responses", "for", "both", "the", "Treatment", "and", "Control", "condition", ".", "The", "vertical", "axis", "is", "labelled", "with", "the", "metric", "measured", "by", "the", "survey", "questions", ".", "'Perceived", "Efficiency", "'", "and", "'Perceived", "Effectiveness", "'", "were", "both", "higher", "in", "the", "Treatment", "condition", "when", "compared", "to", "the", "Control", "condition", ",", "and", "'Aggregated", "NASA-TLX", "'", "was", "lower", "in", "the", "Treatment", "condition", "when", "compared", "to", "the", "Control", "condition", ".", "''"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Consensus Building in Collaborative Sequencing with Visual Awareness ", "tokens": ["Consensus", "Building", "in", "Collaborative", "Sequencing", "with", "Visual", "Awareness"]}, "filename": "233dd54bcbf3e6a151dce13a4d01a531a0648b04_Image_018.jpg", "orig_filename": "233dd54bcbf3e6a151dce13a4d01a531a0648b04", "split": "train"}, {"article_id": "Using Data to Approach the Unknown", "description": {"raw": "The image shows a graph representation of the trajectories previously described, summarizing their fertility-related events, treatments and tracking activities, and the healthcare providers they worked with.  Claire: Fertility related events include 4 conceptions, 3 miscarriages, and one partial mole. Tracking activities include tracking period dates, lifestyle measures, temperature, symptoms, intercourse, ovulation prediction kits, and cervical mucus, using both a fertility app and a spreadsheet. Treatments include 3 IUIs. Healthcare providers include infertility specialist, REI, and a Chinese traditional doctor. She also quit both western and Chinese medicine before conceiving.  Anna: Fertility related events include 2 conceptions and 1 miscarriage. Tracking activities include tracking period dates, temperature, cervical mucus, intercourse, symptoms, ovulation prediction kits, and cervical position, using both a fertility app and a spreadsheet. Treatments include progesterone treatment, birth control for intermenstrual bleeding after the miscarriage, and 5 cycles of ovulation stimulation. Healthcare providers include only her midwife.", "tokens": ["The", "image", "shows", "a", "graph", "representation", "of", "the", "trajectories", "previously", "described", ",", "summarizing", "their", "fertility-related", "events", ",", "treatments", "and", "tracking", "activities", ",", "and", "the", "healthcare", "providers", "they", "worked", "with", ".", "Claire", ":", "Fertility", "related", "events", "include", "4", "conceptions", ",", "3", "miscarriages", ",", "and", "one", "partial", "mole", ".", "Tracking", "activities", "include", "tracking", "period", "dates", ",", "lifestyle", "measures", ",", "temperature", ",", "symptoms", ",", "intercourse", ",", "ovulation", "prediction", "kits", ",", "and", "cervical", "mucus", ",", "using", "both", "a", "fertility", "app", "and", "a", "spreadsheet", ".", "Treatments", "include", "3", "IUIs", ".", "Healthcare", "providers", "include", "infertility", "specialist", ",", "REI", ",", "and", "a", "Chinese", "traditional", "doctor", ".", "She", "also", "quit", "both", "western", "and", "Chinese", "medicine", "before", "conceiving", ".", "Anna", ":", "Fertility", "related", "events", "include", "2", "conceptions", "and", "1", "miscarriage", ".", "Tracking", "activities", "include", "tracking", "period", "dates", ",", "temperature", ",", "cervical", "mucus", ",", "intercourse", ",", "symptoms", ",", "ovulation", "prediction", "kits", ",", "and", "cervical", "position", ",", "using", "both", "a", "fertility", "app", "and", "a", "spreadsheet", ".", "Treatments", "include", "progesterone", "treatment", ",", "birth", "control", "for", "intermenstrual", "bleeding", "after", "the", "miscarriage", ",", "and", "5", "cycles", "of", "ovulation", "stimulation", ".", "Healthcare", "providers", "include", "only", "her", "midwife", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Using Data to Approach the Unknown ", "tokens": ["Using", "Data", "to", "Approach", "the", "Unknown"]}, "filename": "e96bad8bbbaa12e14f514008302b680184dc3efb_Image_007.jpg", "orig_filename": "e96bad8bbbaa12e14f514008302b680184dc3efb", "split": "train"}, {"article_id": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "description": {"raw": "A box plot displaying the Speech Rate across each condition, with units in words per minute. First results are presented for the 9 participants who spoke in all three conditions, with median value of 209.828 for Markup, 150.846 for No ASR, and 169.242 for ASR.  There was a significant difference between Markup and No ASR.   Next, results are shown for all 12 participants, with median value of 203.418 for Markup and 171.534 for ASR.  There was no significant difference between these two conditions.", "tokens": ["A", "box", "plot", "displaying", "the", "Speech", "Rate", "across", "each", "condition", ",", "with", "units", "in", "words", "per", "minute", ".", "First", "results", "are", "presented", "for", "the", "9", "participants", "who", "spoke", "in", "all", "three", "conditions", ",", "with", "median", "value", "of", "209.828", "for", "Markup", ",", "150.846", "for", "No", "ASR", ",", "and", "169.242", "for", "ASR", ".", "There", "was", "a", "significant", "difference", "between", "Markup", "and", "No", "ASR", ".", "Next", ",", "results", "are", "shown", "for", "all", "12", "participants", ",", "with", "median", "value", "of", "203.418", "for", "Markup", "and", "171.534", "for", "ASR", ".", "There", "was", "no", "significant", "difference", "between", "these", "two", "conditions", "."]}, "caption": {"raw": "Figure 8: Box plot of results for speech rate, in words per minute, across conditions", "tokens": ["Figure", "8", ":", "Box", "plot", "of", "results", "for", "speech", "rate", ",", "in", "words", "per", "minute", ",", "across", "conditions"]}, "context": {"raw": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers Figure 8: Box plot of results for speech rate, in words per minute, across conditions", "tokens": ["Behavioral", "Changes", "in", "Speakers", "who", "are", "Automatically", "Captioned", "in", "Meetings", "with", "Deaf", "or", "Hard-of-Hearing", "Peers", "Figure", "8", ":", "Box", "plot", "of", "results", "for", "speech", "rate", ",", "in", "words", "per", "minute", ",", "across", "conditions"]}, "filename": "4cecd70a9e46a761774a54ed11d613b33721b95d_Image_011.gif", "orig_filename": "4cecd70a9e46a761774a54ed11d613b33721b95d", "split": "train"}, {"article_id": "Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations", "description": {"raw": "A grouped column graph showing results for means of satisfaction scores in the in-person study for each behavior. Each behavior is listed on the x-axis, with satisfaction scores on the y-axis. The following lists each behavior groups with three means, one for high, medium, and low levels of that behavior. Speech rate had means of 7.1, 7.95, and 6.95 for high, medium, and low, respectively. Voice intensity had means of 6.9, 7.25, and 5.9. Enunciation had means of 6.9, 8.3, and 5.9. Intonation had means of 7.85, 7.95, and 6.4. Eye contact had means of 7.95, 7.8, and 7. Gesturing had means of 8.65, 7.95, and 7.60. Intermittent pausing had means of 6.3, 7.3, and 7.45. A double asterisk is shown next to the behaviors Enunciation and Intonation as they had significant omnibus Friedman test results. A double asterisk, denoting pairwise significant differences are shown between the pairs medium and low enunciation as well as high and low intonation.", "tokens": ["A", "grouped", "column", "graph", "showing", "results", "for", "means", "of", "satisfaction", "scores", "in", "the", "in-person", "study", "for", "each", "behavior", ".", "Each", "behavior", "is", "listed", "on", "the", "x-axis", ",", "with", "satisfaction", "scores", "on", "the", "y-axis", ".", "The", "following", "lists", "each", "behavior", "groups", "with", "three", "means", ",", "one", "for", "high", ",", "medium", ",", "and", "low", "levels", "of", "that", "behavior", ".", "Speech", "rate", "had", "means", "of", "7.1", ",", "7.95", ",", "and", "6.95", "for", "high", ",", "medium", ",", "and", "low", ",", "respectively", ".", "Voice", "intensity", "had", "means", "of", "6.9", ",", "7.25", ",", "and", "5.9", ".", "Enunciation", "had", "means", "of", "6.9", ",", "8.3", ",", "and", "5.9", ".", "Intonation", "had", "means", "of", "7.85", ",", "7.95", ",", "and", "6.4", ".", "Eye", "contact", "had", "means", "of", "7.95", ",", "7.8", ",", "and", "7", ".", "Gesturing", "had", "means", "of", "8.65", ",", "7.95", ",", "and", "7.60", ".", "Intermittent", "pausing", "had", "means", "of", "6.3", ",", "7.3", ",", "and", "7.45", ".", "A", "double", "asterisk", "is", "shown", "next", "to", "the", "behaviors", "Enunciation", "and", "Intonation", "as", "they", "had", "significant", "omnibus", "Friedman", "test", "results", ".", "A", "double", "asterisk", ",", "denoting", "pairwise", "significant", "differences", "are", "shown", "between", "the", "pairs", "medium", "and", "low", "enunciation", "as", "well", "as", "high", "and", "low", "intonation", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations ", "tokens": ["Deaf", "and", "hard-of-hearing", "users", "'", "preferences", "for", "hearing", "speakers", "'", "behavior", "during", "technology-mediated", "in-person", "and", "remote", "conversations"]}, "filename": "377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0_Image_004.jpg", "orig_filename": "377c33d7bc0f4b0d3d66b74536be8aadfe5f13b0", "split": "train"}, {"article_id": "Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information", "description": {"raw": "Heatmaps of positioning data from a lesson of T2 (L) and T5 (R) for illustrating their reported examples.", "tokens": ["Heatmaps", "of", "positioning", "data", "from", "a", "lesson", "of", "T2", "(", "L", ")", "and", "T5", "(", "R", ")", "for", "illustrating", "their", "reported", "examples", "."]}, "caption": {"raw": "Figure 5. Heatmaps of positioning data from a lesson of T2", "tokens": ["Figure", "5", ".", "Heatmaps", "of", "positioning", "data", "from", "a", "lesson", "of", "T2"]}, "context": {"raw": "Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information Figure 5. Heatmaps of positioning data from a lesson of T2", "tokens": ["Unobtrusively", "Enhancing", "Reflection-in-Action", "of", "Teachers", "through", "Spatially", "Distributed", "Ambient", "Information", "Figure", "5", ".", "Heatmaps", "of", "positioning", "data", "from", "a", "lesson", "of", "T2"]}, "filename": "a9acd1907c8be605fb24753d4209d44ca82d4b26_Image_005.jpg", "orig_filename": "a9acd1907c8be605fb24753d4209d44ca82d4b26", "split": "train"}, {"article_id": "CoNotate: Suggesting Queries Based on Notes Promotes Knowledge Discovery", "description": {"raw": "Two sets of phrases from SERPs and notes are extracted, then compared to get gap_phrases. Then further word embedding, clustering and labeling, six noun phrases, highlighted in green and yellow respectively, are added to the original query as NotesOverview suggestions and NotesGap suggestions and presented to the user in random order. Screenshot of CoNotate extension's Suggestion bar is included.", "tokens": ["Two", "sets", "of", "phrases", "from", "SERPs", "and", "notes", "are", "extracted", ",", "then", "compared", "to", "get", "gap_phrases", ".", "Then", "further", "word", "embedding", ",", "clustering", "and", "labeling", ",", "six", "noun", "phrases", ",", "highlighted", "in", "green", "and", "yellow", "respectively", ",", "are", "added", "to", "the", "original", "query", "as", "NotesOverview", "suggestions", "and", "NotesGap", "suggestions", "and", "presented", "to", "the", "user", "in", "random", "order", ".", "Screenshot", "of", "CoNotate", "extension", "'s", "Suggestion", "bar", "is", "included", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "CoNotate: Suggesting Queries Based on Notes Promotes Knowledge Discovery ", "tokens": ["CoNotate", ":", "Suggesting", "Queries", "Based", "on", "Notes", "Promotes", "Knowledge", "Discovery"]}, "filename": "3db37ace99a8714b85ea5773fefd1e29ddc51da3_Image_004.png", "orig_filename": "3db37ace99a8714b85ea5773fefd1e29ddc51da3", "split": "train"}, {"article_id": "Effect of target size on non-visual text-entry", "description": {"raw": "The graph shows a significant increase of the relative path lenght as size gets smaller. From a relative distance smaller than 10 pixels to a 30 pixel difference on tiny. The same effect it is also noticible on the task axis lengh but the difference between tiny and large is of only about 8 relative pixels.", "tokens": ["The", "graph", "shows", "a", "significant", "increase", "of", "the", "relative", "path", "lenght", "as", "size", "gets", "smaller", ".", "From", "a", "relative", "distance", "smaller", "than", "10", "pixels", "to", "a", "30", "pixel", "difference", "on", "tiny", ".", "The", "same", "effect", "it", "is", "also", "noticible", "on", "the", "task", "axis", "lengh", "but", "the", "difference", "between", "tiny", "and", "large", "is", "of", "only", "about", "8", "relative", "pixels", "."]}, "caption": {"raw": "Figure 3- Relative Path Length and Task Axis Length in pixels.", "tokens": ["Figure", "3-", "Relative", "Path", "Length", "and", "Task", "Axis", "Length", "in", "pixels", "."]}, "context": {"raw": "Effect of target size on non-visual text-entry Figure 3- Relative Path Length and Task Axis Length in pixels.", "tokens": ["Effect", "of", "target", "size", "on", "non-visual", "text-entry", "Figure", "3-", "Relative", "Path", "Length", "and", "Task", "Axis", "Length", "in", "pixels", "."]}, "filename": "f65de96c15e33484bc85079b354f053fd9fe8ccc_Image_005.jpg", "orig_filename": "f65de96c15e33484bc85079b354f053fd9fe8ccc", "split": "train"}, {"article_id": "Verge-it: Gaze Interaction for a Binocular Head-Worn Display using Modulated Disparity Vergence Eye Movement", "description": {"raw": "Y axis means participant (numbered with 'P') and X axis means score. There are two bars per participants in horizontal direction. The length of the bar represent the SUS survey scores.   P1. TV condition score is 47.5 and Wall condition score is 35.  P2. TV condition score is 80 and Wall condition score is 75.  P3. TV condition score is 87.5 and Wall condition score is 85.  P4. TV condition score is 82.5 and Wall condition score is 80.  P5. TV condition score is 65 and Wall condition score is 62.5.  P6. TV condition score is 97.5 and Wall condition score is 100.  P7. TV condition score is 87.5 and Wall condition score is 82.5.", "tokens": ["Y", "axis", "means", "participant", "(", "numbered", "with", "'", "P", "'", ")", "and", "X", "axis", "means", "score", ".", "There", "are", "two", "bars", "per", "participants", "in", "horizontal", "direction", ".", "The", "length", "of", "the", "bar", "represent", "the", "SUS", "survey", "scores", ".", "P1", ".", "TV", "condition", "score", "is", "47.5", "and", "Wall", "condition", "score", "is", "35", ".", "P2", ".", "TV", "condition", "score", "is", "80", "and", "Wall", "condition", "score", "is", "75", ".", "P3", ".", "TV", "condition", "score", "is", "87.5", "and", "Wall", "condition", "score", "is", "85", ".", "P4", ".", "TV", "condition", "score", "is", "82.5", "and", "Wall", "condition", "score", "is", "80", ".", "P5", ".", "TV", "condition", "score", "is", "65", "and", "Wall", "condition", "score", "is", "62.5", ".", "P6", ".", "TV", "condition", "score", "is", "97.5", "and", "Wall", "condition", "score", "is", "100", ".", "P7", ".", "TV", "condition", "score", "is", "87.5", "and", "Wall", "condition", "score", "is", "82.5", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Verge-it: Gaze Interaction for a Binocular Head-Worn Display using Modulated Disparity Vergence Eye Movement ", "tokens": ["Verge-it", ":", "Gaze", "Interaction", "for", "a", "Binocular", "Head-Worn", "Display", "using", "Modulated", "Disparity", "Vergence", "Eye", "Movement"]}, "filename": "4f2a57fd4fc6fac14d15cf73711d867f58277500_Image_008.jpg", "orig_filename": "4f2a57fd4fc6fac14d15cf73711d867f58277500", "split": "train"}, {"article_id": "Pupil responses during discrete goal-directed movements", "description": {"raw": "Figure 9. Mean pupil diameter changes for 808 valid moves of 69 trials from 12 subjects. Data were aligned over a 7 second window 3 seconds before the tooltip-start. The baseline is defined as the mean diameter of the pupil over the first second of the window, and the solid black curve is the mean pupil diameter change from the baseline over time. The black vertical dashed line is tooltip-start where all the data are aligned and the vertical solid black line is the average tool-reach time. The error bars for 1 std. dev. are drawn every 400ms.", "tokens": ["Figure", "9", ".", "Mean", "pupil", "diameter", "changes", "for", "808", "valid", "moves", "of", "69", "trials", "from", "12", "subjects", ".", "Data", "were", "aligned", "over", "a", "7", "second", "window", "3", "seconds", "before", "the", "tooltip-start", ".", "The", "baseline", "is", "defined", "as", "the", "mean", "diameter", "of", "the", "pupil", "over", "the", "first", "second", "of", "the", "window", ",", "and", "the", "solid", "black", "curve", "is", "the", "mean", "pupil", "diameter", "change", "from", "the", "baseline", "over", "time", ".", "The", "black", "vertical", "dashed", "line", "is", "tooltip-start", "where", "all", "the", "data", "are", "aligned", "and", "the", "vertical", "solid", "black", "line", "is", "the", "average", "tool-reach", "time", ".", "The", "error", "bars", "for", "1", "std", ".", "dev", ".", "are", "drawn", "every", "400ms", "."]}, "caption": {"raw": "Figure 9. Mean pupil diameter changes for 808 valid moves of 69 trials from 12 subjects. Data were aligned over a 7 second window 3 seconds before the tooltip-start. The baseline is defined as the mean diameter of the pupil over the first second of the window, and the solid black curve is the mean pupil diameter change from the baseline over time. The black vertical dashed line is tooltip-start where all the data are aligned and the vertical solid black line is the average tool-reach time. The error bars for 1 std. dev. are drawn every 400ms.", "tokens": ["Figure", "9", ".", "Mean", "pupil", "diameter", "changes", "for", "808", "valid", "moves", "of", "69", "trials", "from", "12", "subjects", ".", "Data", "were", "aligned", "over", "a", "7", "second", "window", "3", "seconds", "before", "the", "tooltip-start", ".", "The", "baseline", "is", "defined", "as", "the", "mean", "diameter", "of", "the", "pupil", "over", "the", "first", "second", "of", "the", "window", ",", "and", "the", "solid", "black", "curve", "is", "the", "mean", "pupil", "diameter", "change", "from", "the", "baseline", "over", "time", ".", "The", "black", "vertical", "dashed", "line", "is", "tooltip-start", "where", "all", "the", "data", "are", "aligned", "and", "the", "vertical", "solid", "black", "line", "is", "the", "average", "tool-reach", "time", ".", "The", "error", "bars", "for", "1", "std", ".", "dev", ".", "are", "drawn", "every", "400ms", "."]}, "context": {"raw": "Pupil responses during discrete goal-directed movements Figure 9. Mean pupil diameter changes for 808 valid moves of 69 trials from 12 subjects. Data were aligned over a 7 second window 3 seconds before the tooltip-start. The baseline is defined as the mean diameter of the pupil over the first second of the window, and the solid black curve is the mean pupil diameter change from the baseline over time. The black vertical dashed line is tooltip-start where all the data are aligned and the vertical solid black line is the average tool-reach time. The error bars for 1 std. dev. are drawn every 400ms.", "tokens": ["Pupil", "responses", "during", "discrete", "goal-directed", "movements", "Figure", "9", ".", "Mean", "pupil", "diameter", "changes", "for", "808", "valid", "moves", "of", "69", "trials", "from", "12", "subjects", ".", "Data", "were", "aligned", "over", "a", "7", "second", "window", "3", "seconds", "before", "the", "tooltip-start", ".", "The", "baseline", "is", "defined", "as", "the", "mean", "diameter", "of", "the", "pupil", "over", "the", "first", "second", "of", "the", "window", ",", "and", "the", "solid", "black", "curve", "is", "the", "mean", "pupil", "diameter", "change", "from", "the", "baseline", "over", "time", ".", "The", "black", "vertical", "dashed", "line", "is", "tooltip-start", "where", "all", "the", "data", "are", "aligned", "and", "the", "vertical", "solid", "black", "line", "is", "the", "average", "tool-reach", "time", ".", "The", "error", "bars", "for", "1", "std", ".", "dev", ".", "are", "drawn", "every", "400ms", "."]}, "filename": "1a91496ad6d8adf41f12b157343321732872c0c1_Image_012.gif", "orig_filename": "1a91496ad6d8adf41f12b157343321732872c0c1", "split": "train"}, {"article_id": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "description": {"raw": "This image shows a donut chart of the six different categories in our dataset.  Rising trend is 36%, falling trend is 23%, stable trend is 33%, changing trend, big jump, and big fall are significantly lower", "tokens": ["This", "image", "shows", "a", "donut", "chart", "of", "the", "six", "different", "categories", "in", "our", "dataset", ".", "Rising", "trend", "is", "36", "%", ",", "falling", "trend", "is", "23", "%", ",", "stable", "trend", "is", "33", "%", ",", "changing", "trend", ",", "big", "jump", ",", "and", "big", "fall", "are", "significantly", "lower"]}, "caption": {"raw": "Figure 3. Distribution of the six different classes found in our dataset. Our data is imbalanced with the most frequent class represented being “rising trend” at 36%.", "tokens": ["Figure", "3", ".", "Distribution", "of", "the", "six", "different", "classes", "found", "in", "our", "dataset", ".", "Our", "data", "is", "imbalanced", "with", "the", "most", "frequent", "class", "represented", "being", "“", "rising", "trend", "”", "at", "36", "%", "."]}, "context": {"raw": "Multimodal Deep Learning using Images and Text for Information Graphic Classification Figure 3. Distribution of the six different classes found in our dataset. Our data is imbalanced with the most frequent class represented being “rising trend” at 36%.", "tokens": ["Multimodal", "Deep", "Learning", "using", "Images", "and", "Text", "for", "Information", "Graphic", "Classification", "Figure", "3", ".", "Distribution", "of", "the", "six", "different", "classes", "found", "in", "our", "dataset", ".", "Our", "data", "is", "imbalanced", "with", "the", "most", "frequent", "class", "represented", "being", "“", "rising", "trend", "”", "at", "36", "%", "."]}, "filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_003.jpg", "orig_filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "split": "train"}, {"article_id": "Making Mobile Augmented Reality Applications Accessible", "description": {"raw": "Bar chart showing the amount of times each task was observed, broken down by app category.", "tokens": ["Bar", "chart", "showing", "the", "amount", "of", "times", "each", "task", "was", "observed", ",", "broken", "down", "by", "app", "category", "."]}, "caption": {"raw": "Figure 4: A breakdown of each constituent task we identifed and how frequently it appeared in each app category.", "tokens": ["Figure", "4", ":", "A", "breakdown", "of", "each", "constituent", "task", "we", "identifed", "and", "how", "frequently", "it", "appeared", "in", "each", "app", "category", "."]}, "context": {"raw": "Making Mobile Augmented Reality Applications Accessible Figure 4: A breakdown of each constituent task we identifed and how frequently it appeared in each app category.", "tokens": ["Making", "Mobile", "Augmented", "Reality", "Applications", "Accessible", "Figure", "4", ":", "A", "breakdown", "of", "each", "constituent", "task", "we", "identifed", "and", "how", "frequently", "it", "appeared", "in", "each", "app", "category", "."]}, "filename": "8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8_Image_007.png", "orig_filename": "8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8", "split": "train"}, {"article_id": "Learning Cooperative Personalized Policies from Gaze Data", "description": {"raw": "Figure shows the progression of reward of our method and the SVM-baseline over percentage of training samples for the four conditions of the data collection study.", "tokens": ["Figure", "shows", "the", "progression", "of", "reward", "of", "our", "method", "and", "the", "SVM-baseline", "over", "percentage", "of", "training", "samples", "for", "the", "four", "conditions", "of", "the", "data", "collection", "study", "."]}, "caption": {"raw": "Figure 7. Performance comparison between ours (in purple) versus an", "tokens": ["Figure", "7", ".", "Performance", "comparison", "between", "ours", "(", "in", "purple", ")", "versus", "an"]}, "context": {"raw": "Learning Cooperative Personalized Policies from Gaze Data Figure 7. Performance comparison between ours (in purple) versus an", "tokens": ["Learning", "Cooperative", "Personalized", "Policies", "from", "Gaze", "Data", "Figure", "7", ".", "Performance", "comparison", "between", "ours", "(", "in", "purple", ")", "versus", "an"]}, "filename": "1d4484c14344e5bf197b49e1f17fe696d2d4a79d_Image_007.jpg", "orig_filename": "1d4484c14344e5bf197b49e1f17fe696d2d4a79d", "split": "train"}, {"article_id": "Designing an Adaptive Web Navigation Interface for Users with Variable Pointing Performance", "description": {"raw": "An image of two types of visualizations that are provided to the user. The Error Type Graph (top) shows the frequency of different kinds of error types over time. The Website Graph (bottom) shows the most common websites where pointing errors have occurred. When users hover over data points, additional information is shown.", "tokens": ["An", "image", "of", "two", "types", "of", "visualizations", "that", "are", "provided", "to", "the", "user", ".", "The", "Error", "Type", "Graph", "(", "top", ")", "shows", "the", "frequency", "of", "different", "kinds", "of", "error", "types", "over", "time", ".", "The", "Website", "Graph", "(", "bottom", ")", "shows", "the", "most", "common", "websites", "where", "pointing", "errors", "have", "occurred", ".", "When", "users", "hover", "over", "data", "points", ",", "additional", "information", "is", "shown", "."]}, "caption": {"raw": "Figure 5. Pointing History Visualizations: Two types of visualizations are provided to the user. The Error Type Graph (top) shows the frequency of different kinds of error types over time. The Website Graph (bottom) shows the most common websites where pointing errors have occurred. When users hover over data points, additional information is shown.", "tokens": ["Figure", "5", ".", "Pointing", "History", "Visualizations", ":", "Two", "types", "of", "visualizations", "are", "provided", "to", "the", "user", ".", "The", "Error", "Type", "Graph", "(", "top", ")", "shows", "the", "frequency", "of", "different", "kinds", "of", "error", "types", "over", "time", ".", "The", "Website", "Graph", "(", "bottom", ")", "shows", "the", "most", "common", "websites", "where", "pointing", "errors", "have", "occurred", ".", "When", "users", "hover", "over", "data", "points", ",", "additional", "information", "is", "shown", "."]}, "context": {"raw": "Designing an Adaptive Web Navigation Interface for Users with Variable Pointing Performance Figure 5. Pointing History Visualizations: Two types of visualizations are provided to the user. The Error Type Graph (top) shows the frequency of different kinds of error types over time. The Website Graph (bottom) shows the most common websites where pointing errors have occurred. When users hover over data points, additional information is shown.", "tokens": ["Designing", "an", "Adaptive", "Web", "Navigation", "Interface", "for", "Users", "with", "Variable", "Pointing", "Performance", "Figure", "5", ".", "Pointing", "History", "Visualizations", ":", "Two", "types", "of", "visualizations", "are", "provided", "to", "the", "user", ".", "The", "Error", "Type", "Graph", "(", "top", ")", "shows", "the", "frequency", "of", "different", "kinds", "of", "error", "types", "over", "time", ".", "The", "Website", "Graph", "(", "bottom", ")", "shows", "the", "most", "common", "websites", "where", "pointing", "errors", "have", "occurred", ".", "When", "users", "hover", "over", "data", "points", ",", "additional", "information", "is", "shown", "."]}, "filename": "673e92340023a4eb42572964ac9343eda8798a1e_Image_006.jpg", "orig_filename": "673e92340023a4eb42572964ac9343eda8798a1e", "split": "train"}, {"article_id": "Anchored Audio Sampling: A Seamless Method for Exploring Children's Thoughts During Deployment Studies", "description": {"raw": "A horizontal red bar represents the passage of time. A shaded area in the middle represents the recording window. A blue dotted line is drawn through the middle of the shaded area and labeled \"Anchor Event.\" The shaded portion to the left of the blue line is labeled \"Antecedent Recording\" and the shared portion to the righ tof the blue line is labeled \"Ensuing Recording.\"", "tokens": ["A", "horizontal", "red", "bar", "represents", "the", "passage", "of", "time", ".", "A", "shaded", "area", "in", "the", "middle", "represents", "the", "recording", "window", ".", "A", "blue", "dotted", "line", "is", "drawn", "through", "the", "middle", "of", "the", "shaded", "area", "and", "labeled", "``", "Anchor", "Event", ".", "''", "The", "shaded", "portion", "to", "the", "left", "of", "the", "blue", "line", "is", "labeled", "``", "Antecedent", "Recording", "''", "and", "the", "shared", "portion", "to", "the", "righ", "tof", "the", "blue", "line", "is", "labeled", "``", "Ensuing", "Recording", ".", "''"]}, "caption": {"raw": "Figure 1: Timeline of an AAS data collection event. The re- searcher defnes events of interest, which become the an- chor points. Recording occurs during a sliding window sur- rounding the anchor event.", "tokens": ["Figure", "1", ":", "Timeline", "of", "an", "AAS", "data", "collection", "event", ".", "The", "re-", "searcher", "defnes", "events", "of", "interest", ",", "which", "become", "the", "an-", "chor", "points", ".", "Recording", "occurs", "during", "a", "sliding", "window", "sur-", "rounding", "the", "anchor", "event", "."]}, "context": {"raw": "Anchored Audio Sampling: A Seamless Method for Exploring Children's Thoughts During Deployment Studies Figure 1: Timeline of an AAS data collection event. The re- searcher defnes events of interest, which become the an- chor points. Recording occurs during a sliding window sur- rounding the anchor event.", "tokens": ["Anchored", "Audio", "Sampling", ":", "A", "Seamless", "Method", "for", "Exploring", "Children", "'s", "Thoughts", "During", "Deployment", "Studies", "Figure", "1", ":", "Timeline", "of", "an", "AAS", "data", "collection", "event", ".", "The", "re-", "searcher", "defnes", "events", "of", "interest", ",", "which", "become", "the", "an-", "chor", "points", ".", "Recording", "occurs", "during", "a", "sliding", "window", "sur-", "rounding", "the", "anchor", "event", "."]}, "filename": "abfd76ebf841802ffc6c26cb28e9106211a658f3_Image_002.png", "orig_filename": "abfd76ebf841802ffc6c26cb28e9106211a658f3", "split": "train"}, {"article_id": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "description": {"raw": "Figure 2 - Histogram showing the distribution of word error rate of transcriptions generated by individual transcribers. Clips at intelligibility level 50 tended to have low word error rate, those at intelligibility level 30 tended to have high word error rate, and those at intelligibility level 40 had more evenly distributed word error rate.    Figure 3 - Bar graph showing average word error rates at the three intelligibility levels (30, 40, 50), for both automated and individual crowd worker approaches. Word error rate increased as clip intelligibility decreased. Individual transcribers outperformed the automated approach at each intelligibility level.", "tokens": ["Figure", "2", "-", "Histogram", "showing", "the", "distribution", "of", "word", "error", "rate", "of", "transcriptions", "generated", "by", "individual", "transcribers", ".", "Clips", "at", "intelligibility", "level", "50", "tended", "to", "have", "low", "word", "error", "rate", ",", "those", "at", "intelligibility", "level", "30", "tended", "to", "have", "high", "word", "error", "rate", ",", "and", "those", "at", "intelligibility", "level", "40", "had", "more", "evenly", "distributed", "word", "error", "rate", ".", "Figure", "3", "-", "Bar", "graph", "showing", "average", "word", "error", "rates", "at", "the", "three", "intelligibility", "levels", "(", "30", ",", "40", ",", "50", ")", ",", "for", "both", "automated", "and", "individual", "crowd", "worker", "approaches", ".", "Word", "error", "rate", "increased", "as", "clip", "intelligibility", "decreased", ".", "Individual", "transcribers", "outperformed", "the", "automated", "approach", "at", "each", "intelligibility", "level", "."]}, "caption": {"raw": "Figure 2. Word error rate (WER) distribution of transcriptions gen- erated by individual crowd workers, separated by three levels of clip intelligibility. A lower WER is better and indicates a more accu- rate transcription. More intelligible clips tended to have lower WER, while less intelligible clips tended to have higher WER.", "tokens": ["Figure", "2", ".", "Word", "error", "rate", "(", "WER", ")", "distribution", "of", "transcriptions", "gen-", "erated", "by", "individual", "crowd", "workers", ",", "separated", "by", "three", "levels", "of", "clip", "intelligibility", ".", "A", "lower", "WER", "is", "better", "and", "indicates", "a", "more", "accu-", "rate", "transcription", ".", "More", "intelligible", "clips", "tended", "to", "have", "lower", "WER", ",", "while", "less", "intelligible", "clips", "tended", "to", "have", "higher", "WER", "."]}, "context": {"raw": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users Figure 2. Word error rate (WER) distribution of transcriptions gen- erated by individual crowd workers, separated by three levels of clip intelligibility. A lower WER is better and indicates a more accu- rate transcription. More intelligible clips tended to have lower WER, while less intelligible clips tended to have higher WER.", "tokens": ["Towards", "More", "Robust", "Speech", "Interactions", "for", "Deaf", "and", "Hard", "of", "Hearing", "Users", "Figure", "2", ".", "Word", "error", "rate", "(", "WER", ")", "distribution", "of", "transcriptions", "gen-", "erated", "by", "individual", "crowd", "workers", ",", "separated", "by", "three", "levels", "of", "clip", "intelligibility", ".", "A", "lower", "WER", "is", "better", "and", "indicates", "a", "more", "accu-", "rate", "transcription", ".", "More", "intelligible", "clips", "tended", "to", "have", "lower", "WER", ",", "while", "less", "intelligible", "clips", "tended", "to", "have", "higher", "WER", "."]}, "filename": "8bba1845a85370618cd5c400ec8be42208554549_Image_003.jpg", "orig_filename": "8bba1845a85370618cd5c400ec8be42208554549", "split": "train"}, {"article_id": "Designing AI to Work WITH or FOR People?", "description": {"raw": "A screenshot of the UI of IBM AutoAI system. To the top it shows a visualization with tree-based layout, a path represents a pipeline, a node represents a step that pipeline goes through (e.g., data cleaning or model training)", "tokens": ["A", "screenshot", "of", "the", "UI", "of", "IBM", "AutoAI", "system", ".", "To", "the", "top", "it", "shows", "a", "visualization", "with", "tree-based", "layout", ",", "a", "path", "represents", "a", "pipeline", ",", "a", "node", "represents", "a", "step", "that", "pipeline", "goes", "through", "(", "e.g.", ",", "data", "cleaning", "or", "model", "training", ")"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Designing AI to Work WITH or FOR People? ", "tokens": ["Designing", "AI", "to", "Work", "WITH", "or", "FOR", "People", "?"]}, "filename": "0735b46e892be58a7b46ad3cde31fdd96145ebe7_Image_005.jpg", "orig_filename": "0735b46e892be58a7b46ad3cde31fdd96145ebe7", "split": "train"}, {"article_id": "DDFSeeks same: sexual health-related language in online personal ads for men who have sex with men", "description": {"raw": "This figure is a scatterplot showing the relationship between HIV Estimated Prevalence Rate Per 100,000 Population (CDC, 2011) on the x-axis and Percentage of Ads Containing Sexual Health-Related Language on the y-axis in 95 locations. The relationship is estimated linearly by the equation SHR language = 46.360 + 0.009 * prevalence rate, p = 0.009. Each location is represented by a circle, with the size of the circle representing its population relative to the other locations. Outliers with a low HIV prevalence rate and a high % of SHR language in ads include SF Bay Area and Boise, ID. Outliers with a high HIV prevalence rate and a low % of SHR language in ads include Wichita, KS and Jackson, MS. New York City is close to the linear trend, with a high HIV prevalence rate and a high % of SHR language in ads. In general, locations with a higher population tend to have a greater percentage of ads containing SHR language.", "tokens": ["This", "figure", "is", "a", "scatterplot", "showing", "the", "relationship", "between", "HIV", "Estimated", "Prevalence", "Rate", "Per", "100,000", "Population", "(", "CDC", ",", "2011", ")", "on", "the", "x-axis", "and", "Percentage", "of", "Ads", "Containing", "Sexual", "Health-Related", "Language", "on", "the", "y-axis", "in", "95", "locations", ".", "The", "relationship", "is", "estimated", "linearly", "by", "the", "equation", "SHR", "language", "=", "46.360", "+", "0.009", "*", "prevalence", "rate", ",", "p", "=", "0.009", ".", "Each", "location", "is", "represented", "by", "a", "circle", ",", "with", "the", "size", "of", "the", "circle", "representing", "its", "population", "relative", "to", "the", "other", "locations", ".", "Outliers", "with", "a", "low", "HIV", "prevalence", "rate", "and", "a", "high", "%", "of", "SHR", "language", "in", "ads", "include", "SF", "Bay", "Area", "and", "Boise", ",", "ID", ".", "Outliers", "with", "a", "high", "HIV", "prevalence", "rate", "and", "a", "low", "%", "of", "SHR", "language", "in", "ads", "include", "Wichita", ",", "KS", "and", "Jackson", ",", "MS.", "New", "York", "City", "is", "close", "to", "the", "linear", "trend", ",", "with", "a", "high", "HIV", "prevalence", "rate", "and", "a", "high", "%", "of", "SHR", "language", "in", "ads", ".", "In", "general", ",", "locations", "with", "a", "higher", "population", "tend", "to", "have", "a", "greater", "percentage", "of", "ads", "containing", "SHR", "language", "."]}, "caption": {"raw": "SHR language = 46.360 + 0.009 * prevalence rate, p=0.009", "tokens": ["SHR", "language", "=", "46.360", "+", "0.009", "*", "prevalence", "rate", ",", "p=0.009"]}, "context": {"raw": "DDFSeeks same: sexual health-related language in online personal ads for men who have sex with men SHR language = 46.360 + 0.009 * prevalence rate, p=0.009", "tokens": ["DDFSeeks", "same", ":", "sexual", "health-related", "language", "in", "online", "personal", "ads", "for", "men", "who", "have", "sex", "with", "men", "SHR", "language", "=", "46.360", "+", "0.009", "*", "prevalence", "rate", ",", "p=0.009"]}, "filename": "98474c14fa7ce6e06400788c51a7c0619abe26e5_Image_002.jpg", "orig_filename": "98474c14fa7ce6e06400788c51a7c0619abe26e5", "split": "train"}, {"article_id": "Exergames for Physiotherapy and Rehabilitation: A Medium-term Situated Study of Motivational Aspects and Impact on Functional Reach", "description": {"raw": "The chart shows bar plots for the mean difference in the FRT measure for all three conditions.", "tokens": ["The", "chart", "shows", "bar", "plots", "for", "the", "mean", "difference", "in", "the", "FRT", "measure", "for", "all", "three", "conditions", "."]}, "caption": {"raw": "Figure 3: Means and standard errors for the pre-post- differences of the functional reach test for all groups.", "tokens": ["Figure", "3", ":", "Means", "and", "standard", "errors", "for", "the", "pre-post-", "differences", "of", "the", "functional", "reach", "test", "for", "all", "groups", "."]}, "context": {"raw": "Exergames for Physiotherapy and Rehabilitation: A Medium-term Situated Study of Motivational Aspects and Impact on Functional Reach Figure 3: Means and standard errors for the pre-post- differences of the functional reach test for all groups.", "tokens": ["Exergames", "for", "Physiotherapy", "and", "Rehabilitation", ":", "A", "Medium-term", "Situated", "Study", "of", "Motivational", "Aspects", "and", "Impact", "on", "Functional", "Reach", "Figure", "3", ":", "Means", "and", "standard", "errors", "for", "the", "pre-post-", "differences", "of", "the", "functional", "reach", "test", "for", "all", "groups", "."]}, "filename": "00de251fbb4aaeb4cae73a26f69b8a28e365c3a5_Image_003.jpg", "orig_filename": "00de251fbb4aaeb4cae73a26f69b8a28e365c3a5", "split": "train"}, {"article_id": "Gaze Guidance for Captioned Videos for DHH Users", "description": {"raw": "Box plot results of experiment 2. There are two boxes presents the comprehension scores for videos with guidance versus without guidance. Comprehension scores for videos without guidance is higher than comprehension scores for videos with guidance.", "tokens": ["Box", "plot", "results", "of", "experiment", "2", ".", "There", "are", "two", "boxes", "presents", "the", "comprehension", "scores", "for", "videos", "with", "guidance", "versus", "without", "guidance", ".", "Comprehension", "scores", "for", "videos", "without", "guidance", "is", "higher", "than", "comprehension", "scores", "for", "videos", "with", "guidance", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Gaze Guidance for Captioned Videos for DHH Users ", "tokens": ["Gaze", "Guidance", "for", "Captioned", "Videos", "for", "DHH", "Users"]}, "filename": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c_Image_008.jpg", "orig_filename": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c", "split": "train"}, {"article_id": "Crowdsourced Fabrication", "description": {"raw": "Three charts, one for each of the three days of the conference. Each chart shows time (x-axis) and part number (y-axis), and indicates the start through end time for each part with a line. Labels on the charts indicate when individual rings were completed, and show two periods on Day 2 where no building occurred, labeled \"Replacing modules\" and \"Repairing structure\".", "tokens": ["Three", "charts", ",", "one", "for", "each", "of", "the", "three", "days", "of", "the", "conference", ".", "Each", "chart", "shows", "time", "(", "x-axis", ")", "and", "part", "number", "(", "y-axis", ")", ",", "and", "indicates", "the", "start", "through", "end", "time", "for", "each", "part", "with", "a", "line", ".", "Labels", "on", "the", "charts", "indicate", "when", "individual", "rings", "were", "completed", ",", "and", "show", "two", "periods", "on", "Day", "2", "where", "no", "building", "occurred", ",", "labeled", "``", "Replacing", "modules", "''", "and", "``", "Repairing", "structure", "''", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Crowdsourced Fabrication ", "tokens": ["Crowdsourced", "Fabrication"]}, "filename": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59_Image_013.jpg", "orig_filename": "9708dc7e9242fc8d235a0bf4712479c2a5dc8f59", "split": "train"}, {"article_id": "ToonNote: Improving Communication in Computational Notebooks Using Interactive Data Comics", "description": {"raw": "Study materials that are used for the second study. COVID-19 data set and Netflix data set", "tokens": ["Study", "materials", "that", "are", "used", "for", "the", "second", "study", ".", "COVID-19", "data", "set", "and", "Netflix", "data", "set"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "ToonNote: Improving Communication in Computational Notebooks Using Interactive Data Comics ", "tokens": ["ToonNote", ":", "Improving", "Communication", "in", "Computational", "Notebooks", "Using", "Interactive", "Data", "Comics"]}, "filename": "50c0020c1d03ee152d178a4d188036bdc972a6f7_Image_007.jpg", "orig_filename": "50c0020c1d03ee152d178a4d188036bdc972a6f7", "split": "train"}, {"article_id": "Understanding the Role of Technology to Support Breastfeeding", "description": {"raw": "A stacked bar chart illustrating the distributions of categories on how parents with different infant feeding roles make use of digital technology to support their infant feeding practice. The category of technology use includes relating to the feeding experiences of others, assuring infant feeding practice, finding solutions to challenges in infant feeding and logging infant feeding practice. For the category of relating to the feeding experiences of others, 12.8% of breastfeeding parents responded neither agree nor disagree, 38.40% agreed, 36.80% strongly agreed. No bottle-feeding parents responded strongly disagree and disagree. 25.00% responded neither agree nor disagree, 41.67% responded agree, 33.33% responded strongly agree. 23.53% of partners responded responded neither agree nor disagree and 41.18% responded agree. For the assuring infant feeding practice category, 32.54% of breastfeeding parents responded agree while 34.92% responded strongly agree. 50% of bottle-feeding parents responded responded agree and 25.00% responded strongly agree. 23.53% of partners responded disagree, and 41.18% responded agree. For the finding solutions to challenges in infant feeding practice category, 37.7% of breastfeeding parents responded agree and 45.90% responded strongly agree. 33.33% of bottle-feeding parents responded agree and 33.33% responded strongly agree. 37.5% of partners responded agree, 31.25% responded strongly agree. For the logging infant feeding practice category, 34.92% of breastfeeding parents responded strongly disagree, and 25.40% responded strongly agree. 50% of bottle-feeding parents responded strongly agree. 31.25% of partners responded strongly disagree, and 31.25% responded strongly agree.", "tokens": ["A", "stacked", "bar", "chart", "illustrating", "the", "distributions", "of", "categories", "on", "how", "parents", "with", "different", "infant", "feeding", "roles", "make", "use", "of", "digital", "technology", "to", "support", "their", "infant", "feeding", "practice", ".", "The", "category", "of", "technology", "use", "includes", "relating", "to", "the", "feeding", "experiences", "of", "others", ",", "assuring", "infant", "feeding", "practice", ",", "finding", "solutions", "to", "challenges", "in", "infant", "feeding", "and", "logging", "infant", "feeding", "practice", ".", "For", "the", "category", "of", "relating", "to", "the", "feeding", "experiences", "of", "others", ",", "12.8", "%", "of", "breastfeeding", "parents", "responded", "neither", "agree", "nor", "disagree", ",", "38.40", "%", "agreed", ",", "36.80", "%", "strongly", "agreed", ".", "No", "bottle-feeding", "parents", "responded", "strongly", "disagree", "and", "disagree", ".", "25.00", "%", "responded", "neither", "agree", "nor", "disagree", ",", "41.67", "%", "responded", "agree", ",", "33.33", "%", "responded", "strongly", "agree", ".", "23.53", "%", "of", "partners", "responded", "responded", "neither", "agree", "nor", "disagree", "and", "41.18", "%", "responded", "agree", ".", "For", "the", "assuring", "infant", "feeding", "practice", "category", ",", "32.54", "%", "of", "breastfeeding", "parents", "responded", "agree", "while", "34.92", "%", "responded", "strongly", "agree", ".", "50", "%", "of", "bottle-feeding", "parents", "responded", "responded", "agree", "and", "25.00", "%", "responded", "strongly", "agree", ".", "23.53", "%", "of", "partners", "responded", "disagree", ",", "and", "41.18", "%", "responded", "agree", ".", "For", "the", "finding", "solutions", "to", "challenges", "in", "infant", "feeding", "practice", "category", ",", "37.7", "%", "of", "breastfeeding", "parents", "responded", "agree", "and", "45.90", "%", "responded", "strongly", "agree", ".", "33.33", "%", "of", "bottle-feeding", "parents", "responded", "agree", "and", "33.33", "%", "responded", "strongly", "agree", ".", "37.5", "%", "of", "partners", "responded", "agree", ",", "31.25", "%", "responded", "strongly", "agree", ".", "For", "the", "logging", "infant", "feeding", "practice", "category", ",", "34.92", "%", "of", "breastfeeding", "parents", "responded", "strongly", "disagree", ",", "and", "25.40", "%", "responded", "strongly", "agree", ".", "50", "%", "of", "bottle-feeding", "parents", "responded", "strongly", "agree", ".", "31.25", "%", "of", "partners", "responded", "strongly", "disagree", ",", "and", "31.25", "%", "responded", "strongly", "agree", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Understanding the Role of Technology to Support Breastfeeding ", "tokens": ["Understanding", "the", "Role", "of", "Technology", "to", "Support", "Breastfeeding"]}, "filename": "79b8dcb17db71e75b3401e6743e194cf0943b960_Image_003.jpg", "orig_filename": "79b8dcb17db71e75b3401e6743e194cf0943b960", "split": "train"}, {"article_id": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "description": {"raw": "A scatter plot showing a relationship between HIT reward and hourly wage. A black dashed line represents the model that is fit to the data using an ordinary least square regression.", "tokens": ["A", "scatter", "plot", "showing", "a", "relationship", "between", "HIT", "reward", "and", "hourly", "wage", ".", "A", "black", "dashed", "line", "represents", "the", "model", "that", "is", "fit", "to", "the", "data", "using", "an", "ordinary", "least", "square", "regression", "."]}, "caption": {"raw": "Figure 7. The scatter plot showing the relationship between the transformed reward and hourly wage. The line represents the model that we fit with ordinary linear regression.", "tokens": ["Figure", "7", ".", "The", "scatter", "plot", "showing", "the", "relationship", "between", "the", "transformed", "reward", "and", "hourly", "wage", ".", "The", "line", "represents", "the", "model", "that", "we", "fit", "with", "ordinary", "linear", "regression", "."]}, "context": {"raw": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk Figure 7. The scatter plot showing the relationship between the transformed reward and hourly wage. The line represents the model that we fit with ordinary linear regression.", "tokens": ["A", "Data-Driven", "Analysis", "of", "Workers", "'", "Earnings", "on", "Amazon", "Mechanical", "Turk", "Figure", "7", ".", "The", "scatter", "plot", "showing", "the", "relationship", "between", "the", "transformed", "reward", "and", "hourly", "wage", ".", "The", "line", "represents", "the", "model", "that", "we", "fit", "with", "ordinary", "linear", "regression", "."]}, "filename": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_007.jpg", "orig_filename": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "split": "train"}, {"article_id": "What Makes Smartphone Use Meaningful or Meaningless?", "description": {"raw": "This figure is a bar chart of the average level of  meaningfulness for 2 motivations of use. Meaningfulness is shown on the y-axis on a 1-7 point scale. These motivations are: Instrumental (3.5) and Habitual (2.5).", "tokens": ["This", "figure", "is", "a", "bar", "chart", "of", "the", "average", "level", "of", "meaningfulness", "for", "2", "motivations", "of", "use", ".", "Meaningfulness", "is", "shown", "on", "the", "y-axis", "on", "a", "1-7", "point", "scale", ".", "These", "motivations", "are", ":", "Instrumental", "(", "3.5", ")", "and", "Habitual", "(", "2.5", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "What Makes Smartphone Use Meaningful or Meaningless? ", "tokens": ["What", "Makes", "Smartphone", "Use", "Meaningful", "or", "Meaningless", "?"]}, "filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_006.jpg", "orig_filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "split": "train"}, {"article_id": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "description": {"raw": "This figure shows a graph of a segment of the sensor data from both the left and right sensors on the shoes. On the x-axis is time in seconds and on the y-axis is acceleration in metres per second squared. For every stride there is a large peak followed by a trough, which represents the toe-off and heel-strike events of the step respectively. These peaks and trough pairs alternate between the left and right strides.", "tokens": ["This", "figure", "shows", "a", "graph", "of", "a", "segment", "of", "the", "sensor", "data", "from", "both", "the", "left", "and", "right", "sensors", "on", "the", "shoes", ".", "On", "the", "x-axis", "is", "time", "in", "seconds", "and", "on", "the", "y-axis", "is", "acceleration", "in", "metres", "per", "second", "squared", ".", "For", "every", "stride", "there", "is", "a", "large", "peak", "followed", "by", "a", "trough", ",", "which", "represents", "the", "toe-off", "and", "heel-strike", "events", "of", "the", "step", "respectively", ".", "These", "peaks", "and", "trough", "pairs", "alternate", "between", "the", "left", "and", "right", "strides", "."]}, "caption": {"raw": "Figure 8. Annotated gait segment.", "tokens": ["Figure", "8", ".", "Annotated", "gait", "segment", "."]}, "context": {"raw": "MANA: Designing and Validating a User-Centered Mobility Analysis System Figure 8. Annotated gait segment.", "tokens": ["MANA", ":", "Designing", "and", "Validating", "a", "User-Centered", "Mobility", "Analysis", "System", "Figure", "8", ".", "Annotated", "gait", "segment", "."]}, "filename": "2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_008.jpg", "orig_filename": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "split": "train"}, {"article_id": "BrowseWithMe: An Online Clothes Shopping Assistant for People with Visual Impairments", "description": {"raw": "Box plot results show the frequency with which humans could identify the correct image from a given description, both for BrowseWithMe generated descriptions and Alt text.", "tokens": ["Box", "plot", "results", "show", "the", "frequency", "with", "which", "humans", "could", "identify", "the", "correct", "image", "from", "a", "given", "description", ",", "both", "for", "BrowseWithMe", "generated", "descriptions", "and", "Alt", "text", "."]}, "caption": {"raw": "Figure 5. Human voting accuracy in choosing the correct image from ﬁve options, when given an image description from BrowseWithMe and Alt text. Shown are results for (a) 30 voting tasks which consist of (b-d) 10 voting tasks per clothing item for “tops\", “pants\", and “skirts\" respectively. Each score represents the fraction of ten people who chose the correct image from ﬁve options. The central marks of the boxes denote the median values, box edges denote the 25th and 75th percentiles values, whiskers denote the adjacent value to the data point that is greater than one and a half times the size of the inter-quartile range, and black cross-hairs denote outliers. Also shown below the plots are the mean values. Alt text leads to accurate recognition only slightly more than half the time. BrowseWithMe yields a great improvement over Alt text; e.g., 20 percentage point increase in the median score.", "tokens": ["Figure", "5", ".", "Human", "voting", "accuracy", "in", "choosing", "the", "correct", "image", "from", "ﬁve", "options", ",", "when", "given", "an", "image", "description", "from", "BrowseWithMe", "and", "Alt", "text", ".", "Shown", "are", "results", "for", "(", "a", ")", "30", "voting", "tasks", "which", "consist", "of", "(", "b-d", ")", "10", "voting", "tasks", "per", "clothing", "item", "for", "“", "tops", "''", ",", "“", "pants", "''", ",", "and", "“", "skirts", "''", "respectively", ".", "Each", "score", "represents", "the", "fraction", "of", "ten", "people", "who", "chose", "the", "correct", "image", "from", "ﬁve", "options", ".", "The", "central", "marks", "of", "the", "boxes", "denote", "the", "median", "values", ",", "box", "edges", "denote", "the", "25th", "and", "75th", "percentiles", "values", ",", "whiskers", "denote", "the", "adjacent", "value", "to", "the", "data", "point", "that", "is", "greater", "than", "one", "and", "a", "half", "times", "the", "size", "of", "the", "inter-quartile", "range", ",", "and", "black", "cross-hairs", "denote", "outliers", ".", "Also", "shown", "below", "the", "plots", "are", "the", "mean", "values", ".", "Alt", "text", "leads", "to", "accurate", "recognition", "only", "slightly", "more", "than", "half", "the", "time", ".", "BrowseWithMe", "yields", "a", "great", "improvement", "over", "Alt", "text", ";", "e.g.", ",", "20", "percentage", "point", "increase", "in", "the", "median", "score", "."]}, "context": {"raw": "BrowseWithMe: An Online Clothes Shopping Assistant for People with Visual Impairments Figure 5. Human voting accuracy in choosing the correct image from ﬁve options, when given an image description from BrowseWithMe and Alt text. Shown are results for (a) 30 voting tasks which consist of (b-d) 10 voting tasks per clothing item for “tops\", “pants\", and “skirts\" respectively. Each score represents the fraction of ten people who chose the correct image from ﬁve options. The central marks of the boxes denote the median values, box edges denote the 25th and 75th percentiles values, whiskers denote the adjacent value to the data point that is greater than one and a half times the size of the inter-quartile range, and black cross-hairs denote outliers. Also shown below the plots are the mean values. Alt text leads to accurate recognition only slightly more than half the time. BrowseWithMe yields a great improvement over Alt text; e.g., 20 percentage point increase in the median score.", "tokens": ["BrowseWithMe", ":", "An", "Online", "Clothes", "Shopping", "Assistant", "for", "People", "with", "Visual", "Impairments", "Figure", "5", ".", "Human", "voting", "accuracy", "in", "choosing", "the", "correct", "image", "from", "ﬁve", "options", ",", "when", "given", "an", "image", "description", "from", "BrowseWithMe", "and", "Alt", "text", ".", "Shown", "are", "results", "for", "(", "a", ")", "30", "voting", "tasks", "which", "consist", "of", "(", "b-d", ")", "10", "voting", "tasks", "per", "clothing", "item", "for", "“", "tops", "''", ",", "“", "pants", "''", ",", "and", "“", "skirts", "''", "respectively", ".", "Each", "score", "represents", "the", "fraction", "of", "ten", "people", "who", "chose", "the", "correct", "image", "from", "ﬁve", "options", ".", "The", "central", "marks", "of", "the", "boxes", "denote", "the", "median", "values", ",", "box", "edges", "denote", "the", "25th", "and", "75th", "percentiles", "values", ",", "whiskers", "denote", "the", "adjacent", "value", "to", "the", "data", "point", "that", "is", "greater", "than", "one", "and", "a", "half", "times", "the", "size", "of", "the", "inter-quartile", "range", ",", "and", "black", "cross-hairs", "denote", "outliers", ".", "Also", "shown", "below", "the", "plots", "are", "the", "mean", "values", ".", "Alt", "text", "leads", "to", "accurate", "recognition", "only", "slightly", "more", "than", "half", "the", "time", ".", "BrowseWithMe", "yields", "a", "great", "improvement", "over", "Alt", "text", ";", "e.g.", ",", "20", "percentage", "point", "increase", "in", "the", "median", "score", "."]}, "filename": "4715303bdb871e57cbb597b7e936a6ef4aa2f71a_Image_005.png", "orig_filename": "4715303bdb871e57cbb597b7e936a6ef4aa2f71a", "split": "train"}, {"article_id": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "description": {"raw": "Figure 1: Text stating \"I'd like to visualize 'Origin', 'Miles_per_Gallon', and 'Displacement'\". Two visualizations are shown. The first, a colored tick plot with 'Miles_per_Gallon' on x, 'Origin' on y, and 'Displacement' on color. Next, a colored scatterplot, with 'Miles_per_Gallon' on x, 'Displacement' on y, and 'Origin' on color. There is a question mark between them.", "tokens": ["Figure", "1", ":", "Text", "stating", "``", "I", "'d", "like", "to", "visualize", "'Origin", "'", ",", "'Miles_per_Gallon", "'", ",", "and", "'Displacement", "'", "''", ".", "Two", "visualizations", "are", "shown", ".", "The", "first", ",", "a", "colored", "tick", "plot", "with", "'Miles_per_Gallon", "'", "on", "x", ",", "'Origin", "'", "on", "y", ",", "and", "'Displacement", "'", "on", "color", ".", "Next", ",", "a", "colored", "scatterplot", ",", "with", "'Miles_per_Gallon", "'", "on", "x", ",", "'Displacement", "'", "on", "y", ",", "and", "'Origin", "'", "on", "color", ".", "There", "is", "a", "question", "mark", "between", "them", "."]}, "caption": {"raw": "Figure 1. Which chart should a recommender suggest? Recommender systems are often forced to make decisions in the face of ambiguous user intent. Sometimes, these decisions will hamper exploration.", "tokens": ["Figure", "1", ".", "Which", "chart", "should", "a", "recommender", "suggest", "?", "Recommender", "systems", "are", "often", "forced", "to", "make", "decisions", "in", "the", "face", "of", "ambiguous", "user", "intent", ".", "Sometimes", ",", "these", "decisions", "will", "hamper", "exploration", "."]}, "context": {"raw": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations Figure 1. Which chart should a recommender suggest? Recommender systems are often forced to make decisions in the face of ambiguous user intent. Sometimes, these decisions will hamper exploration.", "tokens": ["Dziban", ":", "Balancing", "Agency", "&", "Automation", "in", "Visualization", "Design", "via", "Anchored", "Recommendations", "Figure", "1", ".", "Which", "chart", "should", "a", "recommender", "suggest", "?", "Recommender", "systems", "are", "often", "forced", "to", "make", "decisions", "in", "the", "face", "of", "ambiguous", "user", "intent", ".", "Sometimes", ",", "these", "decisions", "will", "hamper", "exploration", "."]}, "filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_002.png", "orig_filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "split": "train"}, {"article_id": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "description": {"raw": "Figure 6 - Line graph with iteration step on the X axis and cosine similarity on the Y axis, with one line for each of three intelligibility levels (30, 40, 50). Worker transcriptions tended to converge resulting in overall increasing cosine similarity with each iteration step.    Figure 7 - Line graph showing word error rate of worker transcriptions in a 10-step iterative workflow for the Alexa commands dataset. ASR transcription word error rate is constant across all 10 steps (at about 0.84) and average individual transcription word error rate is constant across all 10 steps (at about 0.4). Both are represented by horizontal lines. The iterative approach produced significantly lower word error rates than both other approaches, with a downwards trending line as the number of iteration steps increased.", "tokens": ["Figure", "6", "-", "Line", "graph", "with", "iteration", "step", "on", "the", "X", "axis", "and", "cosine", "similarity", "on", "the", "Y", "axis", ",", "with", "one", "line", "for", "each", "of", "three", "intelligibility", "levels", "(", "30", ",", "40", ",", "50", ")", ".", "Worker", "transcriptions", "tended", "to", "converge", "resulting", "in", "overall", "increasing", "cosine", "similarity", "with", "each", "iteration", "step", ".", "Figure", "7", "-", "Line", "graph", "showing", "word", "error", "rate", "of", "worker", "transcriptions", "in", "a", "10-step", "iterative", "workflow", "for", "the", "Alexa", "commands", "dataset", ".", "ASR", "transcription", "word", "error", "rate", "is", "constant", "across", "all", "10", "steps", "(", "at", "about", "0.84", ")", "and", "average", "individual", "transcription", "word", "error", "rate", "is", "constant", "across", "all", "10", "steps", "(", "at", "about", "0.4", ")", ".", "Both", "are", "represented", "by", "horizontal", "lines", ".", "The", "iterative", "approach", "produced", "significantly", "lower", "word", "error", "rates", "than", "both", "other", "approaches", ",", "with", "a", "downwards", "trending", "line", "as", "the", "number", "of", "iteration", "steps", "increased", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users ", "tokens": ["Towards", "More", "Robust", "Speech", "Interactions", "for", "Deaf", "and", "Hard", "of", "Hearing", "Users"]}, "filename": "8bba1845a85370618cd5c400ec8be42208554549_Image_009.jpg", "orig_filename": "8bba1845a85370618cd5c400ec8be42208554549", "split": "train"}, {"article_id": "Designing an Animated Character System for American Sign Language", "description": {"raw": "Figure 5: Barriers to using ASL character systems reported by participants. This figure presents a bar chart, with separate bars for DHH (light blue) and hearing (dark blue) populations. Y-axis is % participants, ranging from 0-55. X-axis is Barrier to Adoption, sorted by DHH popularity (most popular first): Few printed material, Few online material, Hard to learn, Prefer English, Do not resemble SL, Dislike of reading, Other, None. Barriers with asterisks are potentially addressed by introducing animation to character systems. Barriers that have asterisks at the top of their bars: Hard to learn, and Do not resemble ASL.", "tokens": ["Figure", "5", ":", "Barriers", "to", "using", "ASL", "character", "systems", "reported", "by", "participants", ".", "This", "figure", "presents", "a", "bar", "chart", ",", "with", "separate", "bars", "for", "DHH", "(", "light", "blue", ")", "and", "hearing", "(", "dark", "blue", ")", "populations", ".", "Y-axis", "is", "%", "participants", ",", "ranging", "from", "0-55", ".", "X-axis", "is", "Barrier", "to", "Adoption", ",", "sorted", "by", "DHH", "popularity", "(", "most", "popular", "first", ")", ":", "Few", "printed", "material", ",", "Few", "online", "material", ",", "Hard", "to", "learn", ",", "Prefer", "English", ",", "Do", "not", "resemble", "SL", ",", "Dislike", "of", "reading", ",", "Other", ",", "None", ".", "Barriers", "with", "asterisks", "are", "potentially", "addressed", "by", "introducing", "animation", "to", "character", "systems", ".", "Barriers", "that", "have", "asterisks", "at", "the", "top", "of", "their", "bars", ":", "Hard", "to", "learn", ",", "and", "Do", "not", "resemble", "ASL", "."]}, "caption": {"raw": "Figure 5: Barriers to using ASL character systems reported by participants. Barriers with asterisks are potentially addressed by introducing animation to character systems.", "tokens": ["Figure", "5", ":", "Barriers", "to", "using", "ASL", "character", "systems", "reported", "by", "participants", ".", "Barriers", "with", "asterisks", "are", "potentially", "addressed", "by", "introducing", "animation", "to", "character", "systems", "."]}, "context": {"raw": "Designing an Animated Character System for American Sign Language Figure 5: Barriers to using ASL character systems reported by participants. Barriers with asterisks are potentially addressed by introducing animation to character systems.", "tokens": ["Designing", "an", "Animated", "Character", "System", "for", "American", "Sign", "Language", "Figure", "5", ":", "Barriers", "to", "using", "ASL", "character", "systems", "reported", "by", "participants", ".", "Barriers", "with", "asterisks", "are", "potentially", "addressed", "by", "introducing", "animation", "to", "character", "systems", "."]}, "filename": "8209b931c94fea5d107e6ef2461b64e00fd52249_Image_008.jpg", "orig_filename": "8209b931c94fea5d107e6ef2461b64e00fd52249", "split": "train"}, {"article_id": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera", "description": {"raw": "This figure is a line graph with frame index number on the horizontal axis and foot position in pixels on the vertical axis. It is split into two panels, the top half shows the absolute distance of left and right feet, with an expected step function like curve. The lower panel has the first order difference of the absolute difference, and shows the expected left and right foot oscillations.", "tokens": ["This", "figure", "is", "a", "line", "graph", "with", "frame", "index", "number", "on", "the", "horizontal", "axis", "and", "foot", "position", "in", "pixels", "on", "the", "vertical", "axis", ".", "It", "is", "split", "into", "two", "panels", ",", "the", "top", "half", "shows", "the", "absolute", "distance", "of", "left", "and", "right", "feet", ",", "with", "an", "expected", "step", "function", "like", "curve", ".", "The", "lower", "panel", "has", "the", "first", "order", "difference", "of", "the", "absolute", "difference", ",", "and", "shows", "the", "expected", "left", "and", "right", "foot", "oscillations", "."]}, "caption": {"raw": "Figure 8: Front point position on foot contour", "tokens": ["Figure", "8", ":", "Front", "point", "position", "on", "foot", "contour"]}, "context": {"raw": "A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera Figure 8: Front point position on foot contour", "tokens": ["A", "Computer", "Vision-Based", "System", "for", "Stride", "Length", "Estimation", "using", "a", "Mobile", "Phone", "Camera", "Figure", "8", ":", "Front", "point", "position", "on", "foot", "contour"]}, "filename": "84e867b0e07ef33492478682a68899d99ea211d8_Image_021.jpg", "orig_filename": "84e867b0e07ef33492478682a68899d99ea211d8", "split": "train"}, {"article_id": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation", "description": {"raw": "Left and Right: Average user-technology engagement difficulty at three levels of perception, cognition, and action in the presence and absence of a stressor, with effect sizes displayed. Left: Synchronization with vibrations is more difficult than both noticing and differentiating vibrations. This is true both without a stressor (at V-Breathing Practice) and with a stressor (at Post-stressor 2). Right: When a stressor is introduced, synchronizing becomes significantly more difficult than both noticing and differentiating. Note that there is data for both treatment and control groups in the absent stressor condition (at V-Breathing Practice), but only for the treatment group in the stressor condition (at Post-stressor 2).", "tokens": ["Left", "and", "Right", ":", "Average", "user-technology", "engagement", "difficulty", "at", "three", "levels", "of", "perception", ",", "cognition", ",", "and", "action", "in", "the", "presence", "and", "absence", "of", "a", "stressor", ",", "with", "effect", "sizes", "displayed", ".", "Left", ":", "Synchronization", "with", "vibrations", "is", "more", "difficult", "than", "both", "noticing", "and", "differentiating", "vibrations", ".", "This", "is", "true", "both", "without", "a", "stressor", "(", "at", "V-Breathing", "Practice", ")", "and", "with", "a", "stressor", "(", "at", "Post-stressor", "2", ")", ".", "Right", ":", "When", "a", "stressor", "is", "introduced", ",", "synchronizing", "becomes", "significantly", "more", "difficult", "than", "both", "noticing", "and", "differentiating", ".", "Note", "that", "there", "is", "data", "for", "both", "treatment", "and", "control", "groups", "in", "the", "absent", "stressor", "condition", "(", "at", "V-Breathing", "Practice", ")", ",", "but", "only", "for", "the", "treatment", "group", "in", "the", "stressor", "condition", "(", "at", "Post-stressor", "2", ")", "."]}, "caption": {"raw": "Figure 5. Left and Right: Average user-technology engagement difﬁculty at three levels of perception, cognition, and action in the presence and absence of a stressor, with effect sizes displayed. Left: Synchronization with vibrations is more difﬁcult than both noticing and differentiating vibrations. This is true both without a stressor (at V-Breathing Practice) and with a stressor (at Post-stressor 2). Right: When a stressor is introduced, synchronizing becomes signiﬁcantly more difﬁcult than both noticing and differentiating. Note that there is data for both treatment and control groups in the absent stressor condition (at V-Breathing Practice), but only for the treatment group in the stressor condition (at Post-stressor 2).", "tokens": ["Figure", "5", ".", "Left", "and", "Right", ":", "Average", "user-technology", "engagement", "difﬁculty", "at", "three", "levels", "of", "perception", ",", "cognition", ",", "and", "action", "in", "the", "presence", "and", "absence", "of", "a", "stressor", ",", "with", "effect", "sizes", "displayed", ".", "Left", ":", "Synchronization", "with", "vibrations", "is", "more", "difﬁcult", "than", "both", "noticing", "and", "differentiating", "vibrations", ".", "This", "is", "true", "both", "without", "a", "stressor", "(", "at", "V-Breathing", "Practice", ")", "and", "with", "a", "stressor", "(", "at", "Post-stressor", "2", ")", ".", "Right", ":", "When", "a", "stressor", "is", "introduced", ",", "synchronizing", "becomes", "signiﬁcantly", "more", "difﬁcult", "than", "both", "noticing", "and", "differentiating", ".", "Note", "that", "there", "is", "data", "for", "both", "treatment", "and", "control", "groups", "in", "the", "absent", "stressor", "condition", "(", "at", "V-Breathing", "Practice", ")", ",", "but", "only", "for", "the", "treatment", "group", "in", "the", "stressor", "condition", "(", "at", "Post-stressor", "2", ")", "."]}, "context": {"raw": "Evaluating a Personalizable, Inconspicuous Vibrotactile(PIV) Breathing Pacer for In-the-Moment Affect Regulation Figure 5. Left and Right: Average user-technology engagement difﬁculty at three levels of perception, cognition, and action in the presence and absence of a stressor, with effect sizes displayed. Left: Synchronization with vibrations is more difﬁcult than both noticing and differentiating vibrations. This is true both without a stressor (at V-Breathing Practice) and with a stressor (at Post-stressor 2). Right: When a stressor is introduced, synchronizing becomes signiﬁcantly more difﬁcult than both noticing and differentiating. Note that there is data for both treatment and control groups in the absent stressor condition (at V-Breathing Practice), but only for the treatment group in the stressor condition (at Post-stressor 2).", "tokens": ["Evaluating", "a", "Personalizable", ",", "Inconspicuous", "Vibrotactile", "(", "PIV", ")", "Breathing", "Pacer", "for", "In-the-Moment", "Affect", "Regulation", "Figure", "5", ".", "Left", "and", "Right", ":", "Average", "user-technology", "engagement", "difﬁculty", "at", "three", "levels", "of", "perception", ",", "cognition", ",", "and", "action", "in", "the", "presence", "and", "absence", "of", "a", "stressor", ",", "with", "effect", "sizes", "displayed", ".", "Left", ":", "Synchronization", "with", "vibrations", "is", "more", "difﬁcult", "than", "both", "noticing", "and", "differentiating", "vibrations", ".", "This", "is", "true", "both", "without", "a", "stressor", "(", "at", "V-Breathing", "Practice", ")", "and", "with", "a", "stressor", "(", "at", "Post-stressor", "2", ")", ".", "Right", ":", "When", "a", "stressor", "is", "introduced", ",", "synchronizing", "becomes", "signiﬁcantly", "more", "difﬁcult", "than", "both", "noticing", "and", "differentiating", ".", "Note", "that", "there", "is", "data", "for", "both", "treatment", "and", "control", "groups", "in", "the", "absent", "stressor", "condition", "(", "at", "V-Breathing", "Practice", ")", ",", "but", "only", "for", "the", "treatment", "group", "in", "the", "stressor", "condition", "(", "at", "Post-stressor", "2", ")", "."]}, "filename": "031c5426862746aeef25ca784afa9191fcdb0eff_Image_011.jpg", "orig_filename": "031c5426862746aeef25ca784afa9191fcdb0eff", "split": "train"}, {"article_id": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces", "description": {"raw": "The bar graph for the completion time of each animation. Total 6 bars are present (1st demo and the 2nd demo per condition (C1, C2, and C3)). The values are as follows: C1- 1st Demo:39.4s, C1-2nd Demo:35.6s, C1-2nd Demo(Green portion):22.5s, C2-1st Demo:78.3s, C2-2nd Demo:19.1s, C2-2nd Demo (Green portion):4.6s, C3-1st Demo:174.5s, C2-2nd Demo:12.4s, C2-2nd Demo (Green portion):1.1s", "tokens": ["The", "bar", "graph", "for", "the", "completion", "time", "of", "each", "animation", ".", "Total", "6", "bars", "are", "present", "(", "1st", "demo", "and", "the", "2nd", "demo", "per", "condition", "(", "C1", ",", "C2", ",", "and", "C3", ")", ")", ".", "The", "values", "are", "as", "follows", ":", "C1-", "1st", "Demo:39.4s", ",", "C1-2nd", "Demo:35.6s", ",", "C1-2nd", "Demo", "(", "Green", "portion", ")", ":22.5s", ",", "C2-1st", "Demo:78.3s", ",", "C2-2nd", "Demo:19.1s", ",", "C2-2nd", "Demo", "(", "Green", "portion", ")", ":4.6s", ",", "C3-1st", "Demo:174.5s", ",", "C2-2nd", "Demo:12.4s", ",", "C2-2nd", "Demo", "(", "Green", "portion", ")", ":1.1s"]}, "caption": {"raw": "Figure 6. Latency of the 1st demo(blue) and the 2nd demo(yellow); The time it takes to demo-remix-replay an animation is longer than the other two conditions, but once an animation is created (the 2nd demo), a worker can respond quickly by replaying the animation. This is because the amount of time needed to respond to the demonstration request is the time it takes to restore the initial state needed to reproduce the requester behavior (the portion of the green bar in the yellow one).", "tokens": ["Figure", "6", ".", "Latency", "of", "the", "1st", "demo", "(", "blue", ")", "and", "the", "2nd", "demo", "(", "yellow", ")", ";", "The", "time", "it", "takes", "to", "demo-remix-replay", "an", "animation", "is", "longer", "than", "the", "other", "two", "conditions", ",", "but", "once", "an", "animation", "is", "created", "(", "the", "2nd", "demo", ")", ",", "a", "worker", "can", "respond", "quickly", "by", "replaying", "the", "animation", ".", "This", "is", "because", "the", "amount", "of", "time", "needed", "to", "respond", "to", "the", "demonstration", "request", "is", "the", "time", "it", "takes", "to", "restore", "the", "initial", "state", "needed", "to", "reproduce", "the", "requester", "behavior", "(", "the", "portion", "of", "the", "green", "bar", "in", "the", "yellow", "one", ")", "."]}, "context": {"raw": "SketchExpress: Remixing Animations for More Effective Crowd-Powered Prototyping of Interactive Interfaces Figure 6. Latency of the 1st demo(blue) and the 2nd demo(yellow); The time it takes to demo-remix-replay an animation is longer than the other two conditions, but once an animation is created (the 2nd demo), a worker can respond quickly by replaying the animation. This is because the amount of time needed to respond to the demonstration request is the time it takes to restore the initial state needed to reproduce the requester behavior (the portion of the green bar in the yellow one).", "tokens": ["SketchExpress", ":", "Remixing", "Animations", "for", "More", "Effective", "Crowd-Powered", "Prototyping", "of", "Interactive", "Interfaces", "Figure", "6", ".", "Latency", "of", "the", "1st", "demo", "(", "blue", ")", "and", "the", "2nd", "demo", "(", "yellow", ")", ";", "The", "time", "it", "takes", "to", "demo-remix-replay", "an", "animation", "is", "longer", "than", "the", "other", "two", "conditions", ",", "but", "once", "an", "animation", "is", "created", "(", "the", "2nd", "demo", ")", ",", "a", "worker", "can", "respond", "quickly", "by", "replaying", "the", "animation", ".", "This", "is", "because", "the", "amount", "of", "time", "needed", "to", "respond", "to", "the", "demonstration", "request", "is", "the", "time", "it", "takes", "to", "restore", "the", "initial", "state", "needed", "to", "reproduce", "the", "requester", "behavior", "(", "the", "portion", "of", "the", "green", "bar", "in", "the", "yellow", "one", ")", "."]}, "filename": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c_Image_008.jpg", "orig_filename": "0127182e5320bcbdcdad87813a58bc78eb2d2e9c", "split": "train"}, {"article_id": "Improving Crowd-Supported GUI Testing with Structural Guidance", "description": {"raw": "The left hand side is a set of screenshots of three different sub-pages of a GUI. The right hand side is a event-flow graph to represent all possible event-flows that a tester can find.", "tokens": ["The", "left", "hand", "side", "is", "a", "set", "of", "screenshots", "of", "three", "different", "sub-pages", "of", "a", "GUI", ".", "The", "right", "hand", "side", "is", "a", "event-flow", "graph", "to", "represent", "all", "possible", "event-flows", "that", "a", "tester", "can", "find", "."]}, "caption": {"raw": "these approaches can be overwhelming for testers to evaluate because the number of possible permutations of low-level events and targets are too large to test, especially when the context of the path is missing. So developers typically rely on manually crafting a small number of event sequences, which is not scalable.", "tokens": ["these", "approaches", "can", "be", "overwhelming", "for", "testers", "to", "evaluate", "because", "the", "number", "of", "possible", "permutations", "of", "low-level", "events", "and", "targets", "are", "too", "large", "to", "test", ",", "especially", "when", "the", "context", "of", "the", "path", "is", "missing", ".", "So", "developers", "typically", "rely", "on", "manually", "crafting", "a", "small", "number", "of", "event", "sequences", ",", "which", "is", "not", "scalable", "."]}, "context": {"raw": "Improving Crowd-Supported GUI Testing with Structural Guidance these approaches can be overwhelming for testers to evaluate because the number of possible permutations of low-level events and targets are too large to test, especially when the context of the path is missing. So developers typically rely on manually crafting a small number of event sequences, which is not scalable.", "tokens": ["Improving", "Crowd-Supported", "GUI", "Testing", "with", "Structural", "Guidance", "these", "approaches", "can", "be", "overwhelming", "for", "testers", "to", "evaluate", "because", "the", "number", "of", "possible", "permutations", "of", "low-level", "events", "and", "targets", "are", "too", "large", "to", "test", ",", "especially", "when", "the", "context", "of", "the", "path", "is", "missing", ".", "So", "developers", "typically", "rely", "on", "manually", "crafting", "a", "small", "number", "of", "event", "sequences", ",", "which", "is", "not", "scalable", "."]}, "filename": "5052e22cf635805f4a6503ab3cf528ed80fc0fef_Image_003.jpg", "orig_filename": "5052e22cf635805f4a6503ab3cf528ed80fc0fef", "split": "train"}, {"article_id": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults", "description": {"raw": "This figure shows the participants’ mean responses to game experience statements S1 - S3 following each condition. The chart serves to provide a clear visual impression of the fact that the abstract version received the worst responses for each item. The means and standard deviations are listed in table 3.", "tokens": ["This", "figure", "shows", "the", "participants", "’", "mean", "responses", "to", "game", "experience", "statements", "S1", "-", "S3", "following", "each", "condition", ".", "The", "chart", "serves", "to", "provide", "a", "clear", "visual", "impression", "of", "the", "fact", "that", "the", "abstract", "version", "received", "the", "worst", "responses", "for", "each", "item", ".", "The", "means", "and", "standard", "deviations", "are", "listed", "in", "table", "3", "."]}, "caption": {"raw": "Figure 4: Participants’ mean responses to game experience statements S1 - S3 following each condition.", "tokens": ["Figure", "4", ":", "Participants", "’", "mean", "responses", "to", "game", "experience", "statements", "S1", "-", "S3", "following", "each", "condition", "."]}, "context": {"raw": "Visual complexity, player experience, performance and physical exertion in motion-based games for older adults Figure 4: Participants’ mean responses to game experience statements S1 - S3 following each condition.", "tokens": ["Visual", "complexity", ",", "player", "experience", ",", "performance", "and", "physical", "exertion", "in", "motion-based", "games", "for", "older", "adults", "Figure", "4", ":", "Participants", "’", "mean", "responses", "to", "game", "experience", "statements", "S1", "-", "S3", "following", "each", "condition", "."]}, "filename": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e_Image_006.gif", "orig_filename": "a24780a906824f5e84777f8a1a4ebb7f65ff4f7e", "split": "train"}, {"article_id": "A readability evaluation of real-time crowd captions in the classroom", "description": {"raw": "Figure 1a: A stenograph keyboard that shows its phonetic-based keys.  Figure 1b: A graph of a stenographer's typical Words Per Minute (WPM) limit and range.", "tokens": ["Figure", "1a", ":", "A", "stenograph", "keyboard", "that", "shows", "its", "phonetic-based", "keys", ".", "Figure", "1b", ":", "A", "graph", "of", "a", "stenographer", "'s", "typical", "Words", "Per", "Minute", "(", "WPM", ")", "limit", "and", "range", "."]}, "caption": {"raw": "A stenographer’s typical Words Per Minute (WPM) limit and range.Figure 1: Professional Real-Time Captioning using a stenographrecognition (ASR). Both professional captioning and ASR provide a real-time word-for-word display of what is said in class, as well as options for saving the text after class for study. We discuss the readability of these approaches and a new approach, which utilizes crowd sourcing to generate real-time captions.Professional CaptioningThe most widely used approach, Communications Access Real Time (CART), is generated by professional captionists who use shorthand software to generate captions can keep up with natural speaking rates. Although popular, professional captioners undergo years of training, which results in professional captioning services being expensive. Furthermore, captionists usually have inadequate content knowledge and dictionaries to handle higher education lectures in speciﬁc ﬁelds. is the most reliable transcription service, but is also the most expensive one. Trained stenographers type in shorthand on a stenographic (short hand writing system) keyboard as shown in Figure 1. This keyboard maps multiple key presses to phonemes that are expanded to verbatim full text. Stenography requires 2-3 years of training to achieve at least 225 words per minute (WPM) and up to 300 WPM that is needed to consistently transcribe all real- time speech, which helps to explain the current cost of more than $100 an hour. CART stenographers need only to recognize and type in the phonemes to create the transcript, which enables them to type fast enough to keep up with the natural speaking rate. But the software translation of phonemes to words requires a dictionary that already contains the words used in the lecture; typing in new words into the dictionary slows down the transcription speed considerably. The stenographer can transcribe speech even if the words or phonemes do not make sense to them, e.g., ifthe speech words appear to violate rules of grammar, pronunciation, or logic. If the captioner cannot understand the phoneme or word at all, then they cannot transcribe it.In response to the high costs of CART, computer-based macro expansion services like C-Print were developed and introduced. C-Print is a type of nearly-realtime transcription that was developed at the National Technical Institute for the Deaf. The captionist balances the tradeoﬀ between typing speed and summarization, by including as much information as possible, generally providing a meaning-for-meaning but not verbatim translation of the spoken English content. This system enables operators who are trained in academic situations to consolidate and better organize the text with the goal of creating an end result more like class notes that may be more conducive to for learning. C-Print captionists need less training, and generally charge around $60 an hour. As the captionist normally cannot type as fast as the natural speaking rate, they are not able to produce a verbatim real- time transcript. Also, the captionist can only eﬀectively convey classroom content if they understand that content themselves. The advantage is that the C-Print transcript accuracy and readability is high [21], but the disadvantage of this approach is that the transcript shows the summary that is based on the captionist’s understanding of the material, which may be diﬀerent from the speaker or reader’s understanding of the material.There are several captioning challenges in higher education. The ﬁrst challenge is content knowledge - lecture information is dense and contains specialized vocabulary. This makes it hard to identify and schedule captionists who are both skilled in typing and have the appropriate content knowledge. Another captioning issue involves transcription delay, which occurs when captionists have to understand the phonemes or words and then type in what they have recognized. As a result, captionists tend to type the material to students with a delay of several seconds. This prevents students from eﬀectively participating in an interactive classroom. Another challenge is speaker identiﬁcation, in which captionist are unfamiliar with participants and are challenged to properly identify the current speaker. They can simplify this by recognizing the speaker by name, or asking the speaker to pause before beginning until the captionist has caught up and had an opportunity to identify the new speaker. In terms of availability, captionists typically are not available to transcribe live speech or dialogue for short periods or on-demand. Professional captionists usually need at least a few hours advance notice, and prefer to work in 1-hour increments so as to account for their commute times. As a result, students cannot easily decide at the last minute to attend a lecture or after class interactions with peers and teacher. Captionists used to need to be physically present at the event they were transcribing, but captioning services are increasingly being oﬀered remotely [12, 1]. Captionists often are simply not available for many technical ﬁelds [21, 8]. Remote captioning oﬀers the potential to recruit captionists familiar with a particular subject (e.g., organic chemistry) even if the captionist is located far away from an event. Selecting for expertise further reduces the pool of captionists. A ﬁnal challenge is their cost - professional captionists are highly trained to keep up with speech with low errors rates, and so are highly paid. Experienced verbatim captionists’ pay can exceed $200 an hour, and newly trained summarization captionists can go as low as $60 an hour [21].Automatic Speech RecognitionASR platforms typically use probabilistic approaches to translate speech to text. These platforms face challenges in accurately capturing modern classroom lectures that can have one or more of the following challenges: extensive technical vocabulary, poor acoustic quality, multiple information sources, speaker accents, or other problems. They also impose a processing delay of several seconds and the delay lengthens as the amount of data to be analyzed gets bigger. In other words, ASR works well under ideal situations, but degrades quickly in many real settings. Kheir et al. [12] found that untrained ASR software had 75% accuracy rate, but with training, could go to 90% under ideal single speaker, but this accuracy rate was still too low for use by deaf students. In the best possible case,  in which the speaker has trained the ASR and wears a high-quality, noise-canceling microphone, the accuracy can be above 90%. When recording a speaker using a standard microphone on ASR not trained for the speaker, accuracy rates plummet to far below 50%. Additionally, the errors made by ASR often change the meaning of the text, whereas we have found non- expert captionists are much more likely to simply omit words or make spelling errors. In Figure 2 for instance, the ASR changes ‘two fold axis’ to ‘twenty four lexus’, whereas the c typists typically omit words they do not understand or make spelling errors. Current ASR is speaker-dependent, has diﬃculty recognizing domain-speciﬁc jargon, and adapts poorly to vocal changes, such as when the speaker is sick [6, 7]. ASR systems generally need substantial computing power and high-quality audio to work well, which means systems can be diﬃcult to transport. They are also ill-equipped to recognize and convey tone, attitudes, interest and emphasis, and to refer to visual information such as slides or demonstrations. ASR services charge about $15-20 an hour. However, these systems are more easily integrated with other functions such as multimedia indexing.Crowd Captions in the ClassroomDeaf and hard of hearing students have had a long history of enhancing their classroom accessibility by collaborating with classmates. For example, they often arrange to copy notes from a classmate and share it with their study group. Crowdsourcing has been applied to oﬄine transcription with great success [2], but has just recently been used for real- time transcription [15]. Applying a collaborative captioning approach among classmates enables real-time transcription from multiple non-experts, and crowd agreement mechanisms can be utilized to vet transcript quality [14].We imagine a deaf or hard of hearing person eventually being able to capture aural speech with her cellphone anywhere and have captions returned to her with a few seconds latency. She may use this to follow along in a lecture for which a professional captionist was not requested, to participate in informal conversation with peers after class, or enjoy a movie or other live event that lacks closed captioning. These use cases currently beyond the scope of ASR, and their serendipitous nature precludes pre-arranging a professional captionist. Lasecki et al. have demonstrated that a modest number of people can provide reasonably high coverage over the caption stream, and introduces an algorithm that uses overlapping portions of the sequences to align and merge them using the Legion:Scribe system [15]. Scribe is based on the Legion [13] framework, which uses crowds of…………………………….that has a two fold axis…….………….have a crystal that…………………………….....we have a crystal……………………………………….....we have a crystal that has a two fold axis…..Figure 2: The crowd captioning interface. The interface provides a text input box at the bottom, and shifts text up as users type (either when the text hits the end of the box, or when the user presses the enter key). To encourage users to continue typing even when making mistakes, editing of text is disabled word by word. Partial captions are forwarded to the server in real-time, which uses overlapping segments and the order in segments are received to align and merge them.workers to accomplish tasks in real-time. Unlike Legion, Scribe merges responses to create a single, better, response instead of selecting from inputs to select the best sequence. This merger is done using an online multiple sequence alignment algorithm that aligns worker input to both reconstruct the ﬁnal stream and correct errors (such as spelling mistakes) made by individual workers.Crowd captioning oﬀers several potential beneﬁts over existing approaches. First, it is potentially much cheaper than hiring a professional captionist because non-expert captionists do not need extensive training to acquire a speciﬁc skill set, and thus may be drawn from a variety of sources, e.g. classmates, audience members, microtask marketplaces, volunteers, or aﬀordable and readily available employees. Our workforce can be very large because, for people who can hear, speech recognition is relatively easy and most people can type accurately. The problem is that individually they cannot type quickly enough to keep up with natural speaking rates, and crowd captioning nicely remedies this problem.  Recent work has demonstrated that small crowds can be recruited quickly on-demand (in less than 2 seconds) from such sources[4, 3]. Scribe4Me enabled DHH users toreceive a transcript of a short sound sequence in a few minutes, but is not able to produce verbatim captions over long periods of time [17].In previous work, we developed a crowd captioning system that accepts realtime transcription from multiple non- experts as shown in Figure 2. While non-experts cannot type as quickly as the natural speaking rate, we have found that they can provide accurate partial captions. Our system recruits fellow students with no training and compensates for slower typing speed and lower accuracy by combining the efforts of multiple captionists simultaneously and merges these partial captions in real-time. We have shown that groups of non-experts can achieve more timely captions than a professional captionist, that we can encourage them to focus on speciﬁc portions of the speech to improve global coverage, and that it is possible to recombine partial captions and eﬀectively tradeoﬀ coverage and precision [15].Real-time text reading versus listeningMost people only see real-time text on TV at the bar or gym in the form of closed captions, which tend to have noticeable errors. However, those programs are captioned by live captionists or stenographers. To reduce errors, these real-time transcripts are often corrected and made into a permanent part of the video ﬁle by oﬀ-line captionists who prepare captions from pre-recorded videotapes and thoroughly review the work for errors before airing.The translation of speech to text is not direct, but rather is interpreted and changed in the course of each utterance. Markers like accent, tone, and timbre are stripped out and represented by standardized written words and symbols. Then the reader interprets these words and ﬂow to make meanings for themselves. Captionists tend not to include all spoken information so that readers can keep up with the transcript. Captionists are encouraged to alter the original transcription to provide time for the readers to completely read the caption and to synchronize with the audio. This is needed because, for a non-orthographic language like English, the length of a spoken utterance is not necessarily proportional to the length of a spelled word. In other words, reading speed is not the same as listening speed, especially for real- time scrolling text, as opposed to static pre-prepared text. For static text, reading speed has been measured at 291 wpm [19]. By contrast the average caption rate for TV programs is 141 wpm [11], while the most comfortable reading rate for hearing, hard-of-hearing, and deaf adults is around 145 wpm [10]. The reason is that the task of viewing real-time captions involved diﬀerent processing demands in visual location and tracking of moving text on a dynamic background. English literacy rates among deaf and hard of hearing people who is low compared to hearing peers. Captioning research has shown that both rate and text reduction and viewer reading ability are important factors, and that captions need to be provided within 5 seconds so that the reader can participate [20].The number of spoken words and their complexity can also inﬂuence the captioning decision on the amount of words to transcribe and degree of summarization to include so as to reduce the reader’s total cognitive load. Jensema et al.[10] analyzed a large sample of captioned TV programs andlecture transcripts have a very diﬀerent proﬁle. For comparison purposes, we selected a 50 minute long clip from the MIT Open CourseWare (OCW) website1. The audio sample was picked from a lecture segment in which the speech was relatively clear.We chose this lecture because it combined both technical and non-technical components. We found that the lecture had 9137 words, of which 1428 were unique, at 182.7 wpm. Furthermore, over two thirds of the transcript consisted of around 500 words, which is double the size of the captioned TV word set.", "tokens": ["A", "stenographer", "’", "s", "typical", "Words", "Per", "Minute", "(", "WPM", ")", "limit", "and", "range.Figure", "1", ":", "Professional", "Real-Time", "Captioning", "using", "a", "stenographrecognition", "(", "ASR", ")", ".", "Both", "professional", "captioning", "and", "ASR", "provide", "a", "real-time", "word-for-word", "display", "of", "what", "is", "said", "in", "class", ",", "as", "well", "as", "options", "for", "saving", "the", "text", "after", "class", "for", "study", ".", "We", "discuss", "the", "readability", "of", "these", "approaches", "and", "a", "new", "approach", ",", "which", "utilizes", "crowd", "sourcing", "to", "generate", "real-time", "captions.Professional", "CaptioningThe", "most", "widely", "used", "approach", ",", "Communications", "Access", "Real", "Time", "(", "CART", ")", ",", "is", "generated", "by", "professional", "captionists", "who", "use", "shorthand", "software", "to", "generate", "captions", "can", "keep", "up", "with", "natural", "speaking", "rates", ".", "Although", "popular", ",", "professional", "captioners", "undergo", "years", "of", "training", ",", "which", "results", "in", "professional", "captioning", "services", "being", "expensive", ".", "Furthermore", ",", "captionists", "usually", "have", "inadequate", "content", "knowledge", "and", "dictionaries", "to", "handle", "higher", "education", "lectures", "in", "speciﬁc", "ﬁelds", ".", "is", "the", "most", "reliable", "transcription", "service", ",", "but", "is", "also", "the", "most", "expensive", "one", ".", "Trained", "stenographers", "type", "in", "shorthand", "on", "a", "stenographic", "(", "short", "hand", "writing", "system", ")", "keyboard", "as", "shown", "in", "Figure", "1", ".", "This", "keyboard", "maps", "multiple", "key", "presses", "to", "phonemes", "that", "are", "expanded", "to", "verbatim", "full", "text", ".", "Stenography", "requires", "2-3", "years", "of", "training", "to", "achieve", "at", "least", "225", "words", "per", "minute", "(", "WPM", ")", "and", "up", "to", "300", "WPM", "that", "is", "needed", "to", "consistently", "transcribe", "all", "real-", "time", "speech", ",", "which", "helps", "to", "explain", "the", "current", "cost", "of", "more", "than", "$", "100", "an", "hour", ".", "CART", "stenographers", "need", "only", "to", "recognize", "and", "type", "in", "the", "phonemes", "to", "create", "the", "transcript", ",", "which", "enables", "them", "to", "type", "fast", "enough", "to", "keep", "up", "with", "the", "natural", "speaking", "rate", ".", "But", "the", "software", "translation", "of", "phonemes", "to", "words", "requires", "a", "dictionary", "that", "already", "contains", "the", "words", "used", "in", "the", "lecture", ";", "typing", "in", "new", "words", "into", "the", "dictionary", "slows", "down", "the", "transcription", "speed", "considerably", ".", "The", "stenographer", "can", "transcribe", "speech", "even", "if", "the", "words", "or", "phonemes", "do", "not", "make", "sense", "to", "them", ",", "e.g.", ",", "ifthe", "speech", "words", "appear", "to", "violate", "rules", "of", "grammar", ",", "pronunciation", ",", "or", "logic", ".", "If", "the", "captioner", "can", "not", "understand", "the", "phoneme", "or", "word", "at", "all", ",", "then", "they", "can", "not", "transcribe", "it.In", "response", "to", "the", "high", "costs", "of", "CART", ",", "computer-based", "macro", "expansion", "services", "like", "C-Print", "were", "developed", "and", "introduced", ".", "C-Print", "is", "a", "type", "of", "nearly-realtime", "transcription", "that", "was", "developed", "at", "the", "National", "Technical", "Institute", "for", "the", "Deaf", ".", "The", "captionist", "balances", "the", "tradeoﬀ", "between", "typing", "speed", "and", "summarization", ",", "by", "including", "as", "much", "information", "as", "possible", ",", "generally", "providing", "a", "meaning-for-meaning", "but", "not", "verbatim", "translation", "of", "the", "spoken", "English", "content", ".", "This", "system", "enables", "operators", "who", "are", "trained", "in", "academic", "situations", "to", "consolidate", "and", "better", "organize", "the", "text", "with", "the", "goal", "of", "creating", "an", "end", "result", "more", "like", "class", "notes", "that", "may", "be", "more", "conducive", "to", "for", "learning", ".", "C-Print", "captionists", "need", "less", "training", ",", "and", "generally", "charge", "around", "$", "60", "an", "hour", ".", "As", "the", "captionist", "normally", "can", "not", "type", "as", "fast", "as", "the", "natural", "speaking", "rate", ",", "they", "are", "not", "able", "to", "produce", "a", "verbatim", "real-", "time", "transcript", ".", "Also", ",", "the", "captionist", "can", "only", "eﬀectively", "convey", "classroom", "content", "if", "they", "understand", "that", "content", "themselves", ".", "The", "advantage", "is", "that", "the", "C-Print", "transcript", "accuracy", "and", "readability", "is", "high", "[", "21", "]", ",", "but", "the", "disadvantage", "of", "this", "approach", "is", "that", "the", "transcript", "shows", "the", "summary", "that", "is", "based", "on", "the", "captionist", "’", "s", "understanding", "of", "the", "material", ",", "which", "may", "be", "diﬀerent", "from", "the", "speaker", "or", "reader", "’", "s", "understanding", "of", "the", "material.There", "are", "several", "captioning", "challenges", "in", "higher", "education", ".", "The", "ﬁrst", "challenge", "is", "content", "knowledge", "-", "lecture", "information", "is", "dense", "and", "contains", "specialized", "vocabulary", ".", "This", "makes", "it", "hard", "to", "identify", "and", "schedule", "captionists", "who", "are", "both", "skilled", "in", "typing", "and", "have", "the", "appropriate", "content", "knowledge", ".", "Another", "captioning", "issue", "involves", "transcription", "delay", ",", "which", "occurs", "when", "captionists", "have", "to", "understand", "the", "phonemes", "or", "words", "and", "then", "type", "in", "what", "they", "have", "recognized", ".", "As", "a", "result", ",", "captionists", "tend", "to", "type", "the", "material", "to", "students", "with", "a", "delay", "of", "several", "seconds", ".", "This", "prevents", "students", "from", "eﬀectively", "participating", "in", "an", "interactive", "classroom", ".", "Another", "challenge", "is", "speaker", "identiﬁcation", ",", "in", "which", "captionist", "are", "unfamiliar", "with", "participants", "and", "are", "challenged", "to", "properly", "identify", "the", "current", "speaker", ".", "They", "can", "simplify", "this", "by", "recognizing", "the", "speaker", "by", "name", ",", "or", "asking", "the", "speaker", "to", "pause", "before", "beginning", "until", "the", "captionist", "has", "caught", "up", "and", "had", "an", "opportunity", "to", "identify", "the", "new", "speaker", ".", "In", "terms", "of", "availability", ",", "captionists", "typically", "are", "not", "available", "to", "transcribe", "live", "speech", "or", "dialogue", "for", "short", "periods", "or", "on-demand", ".", "Professional", "captionists", "usually", "need", "at", "least", "a", "few", "hours", "advance", "notice", ",", "and", "prefer", "to", "work", "in", "1-hour", "increments", "so", "as", "to", "account", "for", "their", "commute", "times", ".", "As", "a", "result", ",", "students", "can", "not", "easily", "decide", "at", "the", "last", "minute", "to", "attend", "a", "lecture", "or", "after", "class", "interactions", "with", "peers", "and", "teacher", ".", "Captionists", "used", "to", "need", "to", "be", "physically", "present", "at", "the", "event", "they", "were", "transcribing", ",", "but", "captioning", "services", "are", "increasingly", "being", "oﬀered", "remotely", "[", "12", ",", "1", "]", ".", "Captionists", "often", "are", "simply", "not", "available", "for", "many", "technical", "ﬁelds", "[", "21", ",", "8", "]", ".", "Remote", "captioning", "oﬀers", "the", "potential", "to", "recruit", "captionists", "familiar", "with", "a", "particular", "subject", "(", "e.g.", ",", "organic", "chemistry", ")", "even", "if", "the", "captionist", "is", "located", "far", "away", "from", "an", "event", ".", "Selecting", "for", "expertise", "further", "reduces", "the", "pool", "of", "captionists", ".", "A", "ﬁnal", "challenge", "is", "their", "cost", "-", "professional", "captionists", "are", "highly", "trained", "to", "keep", "up", "with", "speech", "with", "low", "errors", "rates", ",", "and", "so", "are", "highly", "paid", ".", "Experienced", "verbatim", "captionists", "’", "pay", "can", "exceed", "$", "200", "an", "hour", ",", "and", "newly", "trained", "summarization", "captionists", "can", "go", "as", "low", "as", "$", "60", "an", "hour", "[", "21", "]", ".Automatic", "Speech", "RecognitionASR", "platforms", "typically", "use", "probabilistic", "approaches", "to", "translate", "speech", "to", "text", ".", "These", "platforms", "face", "challenges", "in", "accurately", "capturing", "modern", "classroom", "lectures", "that", "can", "have", "one", "or", "more", "of", "the", "following", "challenges", ":", "extensive", "technical", "vocabulary", ",", "poor", "acoustic", "quality", ",", "multiple", "information", "sources", ",", "speaker", "accents", ",", "or", "other", "problems", ".", "They", "also", "impose", "a", "processing", "delay", "of", "several", "seconds", "and", "the", "delay", "lengthens", "as", "the", "amount", "of", "data", "to", "be", "analyzed", "gets", "bigger", ".", "In", "other", "words", ",", "ASR", "works", "well", "under", "ideal", "situations", ",", "but", "degrades", "quickly", "in", "many", "real", "settings", ".", "Kheir", "et", "al", ".", "[", "12", "]", "found", "that", "untrained", "ASR", "software", "had", "75", "%", "accuracy", "rate", ",", "but", "with", "training", ",", "could", "go", "to", "90", "%", "under", "ideal", "single", "speaker", ",", "but", "this", "accuracy", "rate", "was", "still", "too", "low", "for", "use", "by", "deaf", "students", ".", "In", "the", "best", "possible", "case", ",", "in", "which", "the", "speaker", "has", "trained", "the", "ASR", "and", "wears", "a", "high-quality", ",", "noise-canceling", "microphone", ",", "the", "accuracy", "can", "be", "above", "90", "%", ".", "When", "recording", "a", "speaker", "using", "a", "standard", "microphone", "on", "ASR", "not", "trained", "for", "the", "speaker", ",", "accuracy", "rates", "plummet", "to", "far", "below", "50", "%", ".", "Additionally", ",", "the", "errors", "made", "by", "ASR", "often", "change", "the", "meaning", "of", "the", "text", ",", "whereas", "we", "have", "found", "non-", "expert", "captionists", "are", "much", "more", "likely", "to", "simply", "omit", "words", "or", "make", "spelling", "errors", ".", "In", "Figure", "2", "for", "instance", ",", "the", "ASR", "changes", "‘", "two", "fold", "axis", "’", "to", "‘", "twenty", "four", "lexus", "’", ",", "whereas", "the", "c", "typists", "typically", "omit", "words", "they", "do", "not", "understand", "or", "make", "spelling", "errors", ".", "Current", "ASR", "is", "speaker-dependent", ",", "has", "diﬃculty", "recognizing", "domain-speciﬁc", "jargon", ",", "and", "adapts", "poorly", "to", "vocal", "changes", ",", "such", "as", "when", "the", "speaker", "is", "sick", "[", "6", ",", "7", "]", ".", "ASR", "systems", "generally", "need", "substantial", "computing", "power", "and", "high-quality", "audio", "to", "work", "well", ",", "which", "means", "systems", "can", "be", "diﬃcult", "to", "transport", ".", "They", "are", "also", "ill-equipped", "to", "recognize", "and", "convey", "tone", ",", "attitudes", ",", "interest", "and", "emphasis", ",", "and", "to", "refer", "to", "visual", "information", "such", "as", "slides", "or", "demonstrations", ".", "ASR", "services", "charge", "about", "$", "15-20", "an", "hour", ".", "However", ",", "these", "systems", "are", "more", "easily", "integrated", "with", "other", "functions", "such", "as", "multimedia", "indexing.Crowd", "Captions", "in", "the", "ClassroomDeaf", "and", "hard", "of", "hearing", "students", "have", "had", "a", "long", "history", "of", "enhancing", "their", "classroom", "accessibility", "by", "collaborating", "with", "classmates", ".", "For", "example", ",", "they", "often", "arrange", "to", "copy", "notes", "from", "a", "classmate", "and", "share", "it", "with", "their", "study", "group", ".", "Crowdsourcing", "has", "been", "applied", "to", "oﬄine", "transcription", "with", "great", "success", "[", "2", "]", ",", "but", "has", "just", "recently", "been", "used", "for", "real-", "time", "transcription", "[", "15", "]", ".", "Applying", "a", "collaborative", "captioning", "approach", "among", "classmates", "enables", "real-time", "transcription", "from", "multiple", "non-experts", ",", "and", "crowd", "agreement", "mechanisms", "can", "be", "utilized", "to", "vet", "transcript", "quality", "[", "14", "]", ".We", "imagine", "a", "deaf", "or", "hard", "of", "hearing", "person", "eventually", "being", "able", "to", "capture", "aural", "speech", "with", "her", "cellphone", "anywhere", "and", "have", "captions", "returned", "to", "her", "with", "a", "few", "seconds", "latency", ".", "She", "may", "use", "this", "to", "follow", "along", "in", "a", "lecture", "for", "which", "a", "professional", "captionist", "was", "not", "requested", ",", "to", "participate", "in", "informal", "conversation", "with", "peers", "after", "class", ",", "or", "enjoy", "a", "movie", "or", "other", "live", "event", "that", "lacks", "closed", "captioning", ".", "These", "use", "cases", "currently", "beyond", "the", "scope", "of", "ASR", ",", "and", "their", "serendipitous", "nature", "precludes", "pre-arranging", "a", "professional", "captionist", ".", "Lasecki", "et", "al", ".", "have", "demonstrated", "that", "a", "modest", "number", "of", "people", "can", "provide", "reasonably", "high", "coverage", "over", "the", "caption", "stream", ",", "and", "introduces", "an", "algorithm", "that", "uses", "overlapping", "portions", "of", "the", "sequences", "to", "align", "and", "merge", "them", "using", "the", "Legion", ":", "Scribe", "system", "[", "15", "]", ".", "Scribe", "is", "based", "on", "the", "Legion", "[", "13", "]", "framework", ",", "which", "uses", "crowds", "of…………………………….that", "has", "a", "two", "fold", "axis…….………….have", "a", "crystal", "that……………………………", ".....", "we", "have", "a", "crystal………………………………………", ".....", "we", "have", "a", "crystal", "that", "has", "a", "two", "fold", "axis…", "..", "Figure", "2", ":", "The", "crowd", "captioning", "interface", ".", "The", "interface", "provides", "a", "text", "input", "box", "at", "the", "bottom", ",", "and", "shifts", "text", "up", "as", "users", "type", "(", "either", "when", "the", "text", "hits", "the", "end", "of", "the", "box", ",", "or", "when", "the", "user", "presses", "the", "enter", "key", ")", ".", "To", "encourage", "users", "to", "continue", "typing", "even", "when", "making", "mistakes", ",", "editing", "of", "text", "is", "disabled", "word", "by", "word", ".", "Partial", "captions", "are", "forwarded", "to", "the", "server", "in", "real-time", ",", "which", "uses", "overlapping", "segments", "and", "the", "order", "in", "segments", "are", "received", "to", "align", "and", "merge", "them.workers", "to", "accomplish", "tasks", "in", "real-time", ".", "Unlike", "Legion", ",", "Scribe", "merges", "responses", "to", "create", "a", "single", ",", "better", ",", "response", "instead", "of", "selecting", "from", "inputs", "to", "select", "the", "best", "sequence", ".", "This", "merger", "is", "done", "using", "an", "online", "multiple", "sequence", "alignment", "algorithm", "that", "aligns", "worker", "input", "to", "both", "reconstruct", "the", "ﬁnal", "stream", "and", "correct", "errors", "(", "such", "as", "spelling", "mistakes", ")", "made", "by", "individual", "workers.Crowd", "captioning", "oﬀers", "several", "potential", "beneﬁts", "over", "existing", "approaches", ".", "First", ",", "it", "is", "potentially", "much", "cheaper", "than", "hiring", "a", "professional", "captionist", "because", "non-expert", "captionists", "do", "not", "need", "extensive", "training", "to", "acquire", "a", "speciﬁc", "skill", "set", ",", "and", "thus", "may", "be", "drawn", "from", "a", "variety", "of", "sources", ",", "e.g", ".", "classmates", ",", "audience", "members", ",", "microtask", "marketplaces", ",", "volunteers", ",", "or", "aﬀordable", "and", "readily", "available", "employees", ".", "Our", "workforce", "can", "be", "very", "large", "because", ",", "for", "people", "who", "can", "hear", ",", "speech", "recognition", "is", "relatively", "easy", "and", "most", "people", "can", "type", "accurately", ".", "The", "problem", "is", "that", "individually", "they", "can", "not", "type", "quickly", "enough", "to", "keep", "up", "with", "natural", "speaking", "rates", ",", "and", "crowd", "captioning", "nicely", "remedies", "this", "problem", ".", "Recent", "work", "has", "demonstrated", "that", "small", "crowds", "can", "be", "recruited", "quickly", "on-demand", "(", "in", "less", "than", "2", "seconds", ")", "from", "such", "sources", "[", "4", ",", "3", "]", ".", "Scribe4Me", "enabled", "DHH", "users", "toreceive", "a", "transcript", "of", "a", "short", "sound", "sequence", "in", "a", "few", "minutes", ",", "but", "is", "not", "able", "to", "produce", "verbatim", "captions", "over", "long", "periods", "of", "time", "[", "17", "]", ".In", "previous", "work", ",", "we", "developed", "a", "crowd", "captioning", "system", "that", "accepts", "realtime", "transcription", "from", "multiple", "non-", "experts", "as", "shown", "in", "Figure", "2", ".", "While", "non-experts", "can", "not", "type", "as", "quickly", "as", "the", "natural", "speaking", "rate", ",", "we", "have", "found", "that", "they", "can", "provide", "accurate", "partial", "captions", ".", "Our", "system", "recruits", "fellow", "students", "with", "no", "training", "and", "compensates", "for", "slower", "typing", "speed", "and", "lower", "accuracy", "by", "combining", "the", "efforts", "of", "multiple", "captionists", "simultaneously", "and", "merges", "these", "partial", "captions", "in", "real-time", ".", "We", "have", "shown", "that", "groups", "of", "non-experts", "can", "achieve", "more", "timely", "captions", "than", "a", "professional", "captionist", ",", "that", "we", "can", "encourage", "them", "to", "focus", "on", "speciﬁc", "portions", "of", "the", "speech", "to", "improve", "global", "coverage", ",", "and", "that", "it", "is", "possible", "to", "recombine", "partial", "captions", "and", "eﬀectively", "tradeoﬀ", "coverage", "and", "precision", "[", "15", "]", ".Real-time", "text", "reading", "versus", "listeningMost", "people", "only", "see", "real-time", "text", "on", "TV", "at", "the", "bar", "or", "gym", "in", "the", "form", "of", "closed", "captions", ",", "which", "tend", "to", "have", "noticeable", "errors", ".", "However", ",", "those", "programs", "are", "captioned", "by", "live", "captionists", "or", "stenographers", ".", "To", "reduce", "errors", ",", "these", "real-time", "transcripts", "are", "often", "corrected", "and", "made", "into", "a", "permanent", "part", "of", "the", "video", "ﬁle", "by", "oﬀ-line", "captionists", "who", "prepare", "captions", "from", "pre-recorded", "videotapes", "and", "thoroughly", "review", "the", "work", "for", "errors", "before", "airing.The", "translation", "of", "speech", "to", "text", "is", "not", "direct", ",", "but", "rather", "is", "interpreted", "and", "changed", "in", "the", "course", "of", "each", "utterance", ".", "Markers", "like", "accent", ",", "tone", ",", "and", "timbre", "are", "stripped", "out", "and", "represented", "by", "standardized", "written", "words", "and", "symbols", ".", "Then", "the", "reader", "interprets", "these", "words", "and", "ﬂow", "to", "make", "meanings", "for", "themselves", ".", "Captionists", "tend", "not", "to", "include", "all", "spoken", "information", "so", "that", "readers", "can", "keep", "up", "with", "the", "transcript", ".", "Captionists", "are", "encouraged", "to", "alter", "the", "original", "transcription", "to", "provide", "time", "for", "the", "readers", "to", "completely", "read", "the", "caption", "and", "to", "synchronize", "with", "the", "audio", ".", "This", "is", "needed", "because", ",", "for", "a", "non-orthographic", "language", "like", "English", ",", "the", "length", "of", "a", "spoken", "utterance", "is", "not", "necessarily", "proportional", "to", "the", "length", "of", "a", "spelled", "word", ".", "In", "other", "words", ",", "reading", "speed", "is", "not", "the", "same", "as", "listening", "speed", ",", "especially", "for", "real-", "time", "scrolling", "text", ",", "as", "opposed", "to", "static", "pre-prepared", "text", ".", "For", "static", "text", ",", "reading", "speed", "has", "been", "measured", "at", "291", "wpm", "[", "19", "]", ".", "By", "contrast", "the", "average", "caption", "rate", "for", "TV", "programs", "is", "141", "wpm", "[", "11", "]", ",", "while", "the", "most", "comfortable", "reading", "rate", "for", "hearing", ",", "hard-of-hearing", ",", "and", "deaf", "adults", "is", "around", "145", "wpm", "[", "10", "]", ".", "The", "reason", "is", "that", "the", "task", "of", "viewing", "real-time", "captions", "involved", "diﬀerent", "processing", "demands", "in", "visual", "location", "and", "tracking", "of", "moving", "text", "on", "a", "dynamic", "background", ".", "English", "literacy", "rates", "among", "deaf", "and", "hard", "of", "hearing", "people", "who", "is", "low", "compared", "to", "hearing", "peers", ".", "Captioning", "research", "has", "shown", "that", "both", "rate", "and", "text", "reduction", "and", "viewer", "reading", "ability", "are", "important", "factors", ",", "and", "that", "captions", "need", "to", "be", "provided", "within", "5", "seconds", "so", "that", "the", "reader", "can", "participate", "[", "20", "]", ".The", "number", "of", "spoken", "words", "and", "their", "complexity", "can", "also", "inﬂuence", "the", "captioning", "decision", "on", "the", "amount", "of", "words", "to", "transcribe", "and", "degree", "of", "summarization", "to", "include", "so", "as", "to", "reduce", "the", "reader", "’", "s", "total", "cognitive", "load", ".", "Jensema", "et", "al", ".", "[", "10", "]", "analyzed", "a", "large", "sample", "of", "captioned", "TV", "programs", "andlecture", "transcripts", "have", "a", "very", "diﬀerent", "proﬁle", ".", "For", "comparison", "purposes", ",", "we", "selected", "a", "50", "minute", "long", "clip", "from", "the", "MIT", "Open", "CourseWare", "(", "OCW", ")", "website1", ".", "The", "audio", "sample", "was", "picked", "from", "a", "lecture", "segment", "in", "which", "the", "speech", "was", "relatively", "clear.We", "chose", "this", "lecture", "because", "it", "combined", "both", "technical", "and", "non-technical", "components", ".", "We", "found", "that", "the", "lecture", "had", "9137", "words", ",", "of", "which", "1428", "were", "unique", ",", "at", "182.7", "wpm", ".", "Furthermore", ",", "over", "two", "thirds", "of", "the", "transcript", "consisted", "of", "around", "500", "words", ",", "which", "is", "double", "the", "size", "of", "the", "captioned", "TV", "word", "set", "."]}, "context": {"raw": "A readability evaluation of real-time crowd captions in the classroom A stenographer’s typical Words Per Minute (WPM) limit and range.Figure 1: Professional Real-Time Captioning using a stenographrecognition (ASR). Both professional captioning and ASR provide a real-time word-for-word display of what is said in class, as well as options for saving the text after class for study. We discuss the readability of these approaches and a new approach, which utilizes crowd sourcing to generate real-time captions.Professional CaptioningThe most widely used approach, Communications Access Real Time (CART), is generated by professional captionists who use shorthand software to generate captions can keep up with natural speaking rates. Although popular, professional captioners undergo years of training, which results in professional captioning services being expensive. Furthermore, captionists usually have inadequate content knowledge and dictionaries to handle higher education lectures in speciﬁc ﬁelds. is the most reliable transcription service, but is also the most expensive one. Trained stenographers type in shorthand on a stenographic (short hand writing system) keyboard as shown in Figure 1. This keyboard maps multiple key presses to phonemes that are expanded to verbatim full text. Stenography requires 2-3 years of training to achieve at least 225 words per minute (WPM) and up to 300 WPM that is needed to consistently transcribe all real- time speech, which helps to explain the current cost of more than $100 an hour. CART stenographers need only to recognize and type in the phonemes to create the transcript, which enables them to type fast enough to keep up with the natural speaking rate. But the software translation of phonemes to words requires a dictionary that already contains the words used in the lecture; typing in new words into the dictionary slows down the transcription speed considerably. The stenographer can transcribe speech even if the words or phonemes do not make sense to them, e.g., ifthe speech words appear to violate rules of grammar, pronunciation, or logic. If the captioner cannot understand the phoneme or word at all, then they cannot transcribe it.In response to the high costs of CART, computer-based macro expansion services like C-Print were developed and introduced. C-Print is a type of nearly-realtime transcription that was developed at the National Technical Institute for the Deaf. The captionist balances the tradeoﬀ between typing speed and summarization, by including as much information as possible, generally providing a meaning-for-meaning but not verbatim translation of the spoken English content. This system enables operators who are trained in academic situations to consolidate and better organize the text with the goal of creating an end result more like class notes that may be more conducive to for learning. C-Print captionists need less training, and generally charge around $60 an hour. As the captionist normally cannot type as fast as the natural speaking rate, they are not able to produce a verbatim real- time transcript. Also, the captionist can only eﬀectively convey classroom content if they understand that content themselves. The advantage is that the C-Print transcript accuracy and readability is high [21], but the disadvantage of this approach is that the transcript shows the summary that is based on the captionist’s understanding of the material, which may be diﬀerent from the speaker or reader’s understanding of the material.There are several captioning challenges in higher education. The ﬁrst challenge is content knowledge - lecture information is dense and contains specialized vocabulary. This makes it hard to identify and schedule captionists who are both skilled in typing and have the appropriate content knowledge. Another captioning issue involves transcription delay, which occurs when captionists have to understand the phonemes or words and then type in what they have recognized. As a result, captionists tend to type the material to students with a delay of several seconds. This prevents students from eﬀectively participating in an interactive classroom. Another challenge is speaker identiﬁcation, in which captionist are unfamiliar with participants and are challenged to properly identify the current speaker. They can simplify this by recognizing the speaker by name, or asking the speaker to pause before beginning until the captionist has caught up and had an opportunity to identify the new speaker. In terms of availability, captionists typically are not available to transcribe live speech or dialogue for short periods or on-demand. Professional captionists usually need at least a few hours advance notice, and prefer to work in 1-hour increments so as to account for their commute times. As a result, students cannot easily decide at the last minute to attend a lecture or after class interactions with peers and teacher. Captionists used to need to be physically present at the event they were transcribing, but captioning services are increasingly being oﬀered remotely [12, 1]. Captionists often are simply not available for many technical ﬁelds [21, 8]. Remote captioning oﬀers the potential to recruit captionists familiar with a particular subject (e.g., organic chemistry) even if the captionist is located far away from an event. Selecting for expertise further reduces the pool of captionists. A ﬁnal challenge is their cost - professional captionists are highly trained to keep up with speech with low errors rates, and so are highly paid. Experienced verbatim captionists’ pay can exceed $200 an hour, and newly trained summarization captionists can go as low as $60 an hour [21].Automatic Speech RecognitionASR platforms typically use probabilistic approaches to translate speech to text. These platforms face challenges in accurately capturing modern classroom lectures that can have one or more of the following challenges: extensive technical vocabulary, poor acoustic quality, multiple information sources, speaker accents, or other problems. They also impose a processing delay of several seconds and the delay lengthens as the amount of data to be analyzed gets bigger. In other words, ASR works well under ideal situations, but degrades quickly in many real settings. Kheir et al. [12] found that untrained ASR software had 75% accuracy rate, but with training, could go to 90% under ideal single speaker, but this accuracy rate was still too low for use by deaf students. In the best possible case,  in which the speaker has trained the ASR and wears a high-quality, noise-canceling microphone, the accuracy can be above 90%. When recording a speaker using a standard microphone on ASR not trained for the speaker, accuracy rates plummet to far below 50%. Additionally, the errors made by ASR often change the meaning of the text, whereas we have found non- expert captionists are much more likely to simply omit words or make spelling errors. In Figure 2 for instance, the ASR changes ‘two fold axis’ to ‘twenty four lexus’, whereas the c typists typically omit words they do not understand or make spelling errors. Current ASR is speaker-dependent, has diﬃculty recognizing domain-speciﬁc jargon, and adapts poorly to vocal changes, such as when the speaker is sick [6, 7]. ASR systems generally need substantial computing power and high-quality audio to work well, which means systems can be diﬃcult to transport. They are also ill-equipped to recognize and convey tone, attitudes, interest and emphasis, and to refer to visual information such as slides or demonstrations. ASR services charge about $15-20 an hour. However, these systems are more easily integrated with other functions such as multimedia indexing.Crowd Captions in the ClassroomDeaf and hard of hearing students have had a long history of enhancing their classroom accessibility by collaborating with classmates. For example, they often arrange to copy notes from a classmate and share it with their study group. Crowdsourcing has been applied to oﬄine transcription with great success [2], but has just recently been used for real- time transcription [15]. Applying a collaborative captioning approach among classmates enables real-time transcription from multiple non-experts, and crowd agreement mechanisms can be utilized to vet transcript quality [14].We imagine a deaf or hard of hearing person eventually being able to capture aural speech with her cellphone anywhere and have captions returned to her with a few seconds latency. She may use this to follow along in a lecture for which a professional captionist was not requested, to participate in informal conversation with peers after class, or enjoy a movie or other live event that lacks closed captioning. These use cases currently beyond the scope of ASR, and their serendipitous nature precludes pre-arranging a professional captionist. Lasecki et al. have demonstrated that a modest number of people can provide reasonably high coverage over the caption stream, and introduces an algorithm that uses overlapping portions of the sequences to align and merge them using the Legion:Scribe system [15]. Scribe is based on the Legion [13] framework, which uses crowds of…………………………….that has a two fold axis…….………….have a crystal that…………………………….....we have a crystal……………………………………….....we have a crystal that has a two fold axis…..Figure 2: The crowd captioning interface. The interface provides a text input box at the bottom, and shifts text up as users type (either when the text hits the end of the box, or when the user presses the enter key). To encourage users to continue typing even when making mistakes, editing of text is disabled word by word. Partial captions are forwarded to the server in real-time, which uses overlapping segments and the order in segments are received to align and merge them.workers to accomplish tasks in real-time. Unlike Legion, Scribe merges responses to create a single, better, response instead of selecting from inputs to select the best sequence. This merger is done using an online multiple sequence alignment algorithm that aligns worker input to both reconstruct the ﬁnal stream and correct errors (such as spelling mistakes) made by individual workers.Crowd captioning oﬀers several potential beneﬁts over existing approaches. First, it is potentially much cheaper than hiring a professional captionist because non-expert captionists do not need extensive training to acquire a speciﬁc skill set, and thus may be drawn from a variety of sources, e.g. classmates, audience members, microtask marketplaces, volunteers, or aﬀordable and readily available employees. Our workforce can be very large because, for people who can hear, speech recognition is relatively easy and most people can type accurately. The problem is that individually they cannot type quickly enough to keep up with natural speaking rates, and crowd captioning nicely remedies this problem.  Recent work has demonstrated that small crowds can be recruited quickly on-demand (in less than 2 seconds) from such sources[4, 3]. Scribe4Me enabled DHH users toreceive a transcript of a short sound sequence in a few minutes, but is not able to produce verbatim captions over long periods of time [17].In previous work, we developed a crowd captioning system that accepts realtime transcription from multiple non- experts as shown in Figure 2. While non-experts cannot type as quickly as the natural speaking rate, we have found that they can provide accurate partial captions. Our system recruits fellow students with no training and compensates for slower typing speed and lower accuracy by combining the efforts of multiple captionists simultaneously and merges these partial captions in real-time. We have shown that groups of non-experts can achieve more timely captions than a professional captionist, that we can encourage them to focus on speciﬁc portions of the speech to improve global coverage, and that it is possible to recombine partial captions and eﬀectively tradeoﬀ coverage and precision [15].Real-time text reading versus listeningMost people only see real-time text on TV at the bar or gym in the form of closed captions, which tend to have noticeable errors. However, those programs are captioned by live captionists or stenographers. To reduce errors, these real-time transcripts are often corrected and made into a permanent part of the video ﬁle by oﬀ-line captionists who prepare captions from pre-recorded videotapes and thoroughly review the work for errors before airing.The translation of speech to text is not direct, but rather is interpreted and changed in the course of each utterance. Markers like accent, tone, and timbre are stripped out and represented by standardized written words and symbols. Then the reader interprets these words and ﬂow to make meanings for themselves. Captionists tend not to include all spoken information so that readers can keep up with the transcript. Captionists are encouraged to alter the original transcription to provide time for the readers to completely read the caption and to synchronize with the audio. This is needed because, for a non-orthographic language like English, the length of a spoken utterance is not necessarily proportional to the length of a spelled word. In other words, reading speed is not the same as listening speed, especially for real- time scrolling text, as opposed to static pre-prepared text. For static text, reading speed has been measured at 291 wpm [19]. By contrast the average caption rate for TV programs is 141 wpm [11], while the most comfortable reading rate for hearing, hard-of-hearing, and deaf adults is around 145 wpm [10]. The reason is that the task of viewing real-time captions involved diﬀerent processing demands in visual location and tracking of moving text on a dynamic background. English literacy rates among deaf and hard of hearing people who is low compared to hearing peers. Captioning research has shown that both rate and text reduction and viewer reading ability are important factors, and that captions need to be provided within 5 seconds so that the reader can participate [20].The number of spoken words and their complexity can also inﬂuence the captioning decision on the amount of words to transcribe and degree of summarization to include so as to reduce the reader’s total cognitive load. Jensema et al.[10] analyzed a large sample of captioned TV programs andlecture transcripts have a very diﬀerent proﬁle. For comparison purposes, we selected a 50 minute long clip from the MIT Open CourseWare (OCW) website1. The audio sample was picked from a lecture segment in which the speech was relatively clear.We chose this lecture because it combined both technical and non-technical components. We found that the lecture had 9137 words, of which 1428 were unique, at 182.7 wpm. Furthermore, over two thirds of the transcript consisted of around 500 words, which is double the size of the captioned TV word set.", "tokens": ["A", "readability", "evaluation", "of", "real-time", "crowd", "captions", "in", "the", "classroom", "A", "stenographer", "’", "s", "typical", "Words", "Per", "Minute", "(", "WPM", ")", "limit", "and", "range.Figure", "1", ":", "Professional", "Real-Time", "Captioning", "using", "a", "stenographrecognition", "(", "ASR", ")", ".", "Both", "professional", "captioning", "and", "ASR", "provide", "a", "real-time", "word-for-word", "display", "of", "what", "is", "said", "in", "class", ",", "as", "well", "as", "options", "for", "saving", "the", "text", "after", "class", "for", "study", ".", "We", "discuss", "the", "readability", "of", "these", "approaches", "and", "a", "new", "approach", ",", "which", "utilizes", "crowd", "sourcing", "to", "generate", "real-time", "captions.Professional", "CaptioningThe", "most", "widely", "used", "approach", ",", "Communications", "Access", "Real", "Time", "(", "CART", ")", ",", "is", "generated", "by", "professional", "captionists", "who", "use", "shorthand", "software", "to", "generate", "captions", "can", "keep", "up", "with", "natural", "speaking", "rates", ".", "Although", "popular", ",", "professional", "captioners", "undergo", "years", "of", "training", ",", "which", "results", "in", "professional", "captioning", "services", "being", "expensive", ".", "Furthermore", ",", "captionists", "usually", "have", "inadequate", "content", "knowledge", "and", "dictionaries", "to", "handle", "higher", "education", "lectures", "in", "speciﬁc", "ﬁelds", ".", "is", "the", "most", "reliable", "transcription", "service", ",", "but", "is", "also", "the", "most", "expensive", "one", ".", "Trained", "stenographers", "type", "in", "shorthand", "on", "a", "stenographic", "(", "short", "hand", "writing", "system", ")", "keyboard", "as", "shown", "in", "Figure", "1", ".", "This", "keyboard", "maps", "multiple", "key", "presses", "to", "phonemes", "that", "are", "expanded", "to", "verbatim", "full", "text", ".", "Stenography", "requires", "2-3", "years", "of", "training", "to", "achieve", "at", "least", "225", "words", "per", "minute", "(", "WPM", ")", "and", "up", "to", "300", "WPM", "that", "is", "needed", "to", "consistently", "transcribe", "all", "real-", "time", "speech", ",", "which", "helps", "to", "explain", "the", "current", "cost", "of", "more", "than", "$", "100", "an", "hour", ".", "CART", "stenographers", "need", "only", "to", "recognize", "and", "type", "in", "the", "phonemes", "to", "create", "the", "transcript", ",", "which", "enables", "them", "to", "type", "fast", "enough", "to", "keep", "up", "with", "the", "natural", "speaking", "rate", ".", "But", "the", "software", "translation", "of", "phonemes", "to", "words", "requires", "a", "dictionary", "that", "already", "contains", "the", "words", "used", "in", "the", "lecture", ";", "typing", "in", "new", "words", "into", "the", "dictionary", "slows", "down", "the", "transcription", "speed", "considerably", ".", "The", "stenographer", "can", "transcribe", "speech", "even", "if", "the", "words", "or", "phonemes", "do", "not", "make", "sense", "to", "them", ",", "e.g.", ",", "ifthe", "speech", "words", "appear", "to", "violate", "rules", "of", "grammar", ",", "pronunciation", ",", "or", "logic", ".", "If", "the", "captioner", "can", "not", "understand", "the", "phoneme", "or", "word", "at", "all", ",", "then", "they", "can", "not", "transcribe", "it.In", "response", "to", "the", "high", "costs", "of", "CART", ",", "computer-based", "macro", "expansion", "services", "like", "C-Print", "were", "developed", "and", "introduced", ".", "C-Print", "is", "a", "type", "of", "nearly-realtime", "transcription", "that", "was", "developed", "at", "the", "National", "Technical", "Institute", "for", "the", "Deaf", ".", "The", "captionist", "balances", "the", "tradeoﬀ", "between", "typing", "speed", "and", "summarization", ",", "by", "including", "as", "much", "information", "as", "possible", ",", "generally", "providing", "a", "meaning-for-meaning", "but", "not", "verbatim", "translation", "of", "the", "spoken", "English", "content", ".", "This", "system", "enables", "operators", "who", "are", "trained", "in", "academic", "situations", "to", "consolidate", "and", "better", "organize", "the", "text", "with", "the", "goal", "of", "creating", "an", "end", "result", "more", "like", "class", "notes", "that", "may", "be", "more", "conducive", "to", "for", "learning", ".", "C-Print", "captionists", "need", "less", "training", ",", "and", "generally", "charge", "around", "$", "60", "an", "hour", ".", "As", "the", "captionist", "normally", "can", "not", "type", "as", "fast", "as", "the", "natural", "speaking", "rate", ",", "they", "are", "not", "able", "to", "produce", "a", "verbatim", "real-", "time", "transcript", ".", "Also", ",", "the", "captionist", "can", "only", "eﬀectively", "convey", "classroom", "content", "if", "they", "understand", "that", "content", "themselves", ".", "The", "advantage", "is", "that", "the", "C-Print", "transcript", "accuracy", "and", "readability", "is", "high", "[", "21", "]", ",", "but", "the", "disadvantage", "of", "this", "approach", "is", "that", "the", "transcript", "shows", "the", "summary", "that", "is", "based", "on", "the", "captionist", "’", "s", "understanding", "of", "the", "material", ",", "which", "may", "be", "diﬀerent", "from", "the", "speaker", "or", "reader", "’", "s", "understanding", "of", "the", "material.There", "are", "several", "captioning", "challenges", "in", "higher", "education", ".", "The", "ﬁrst", "challenge", "is", "content", "knowledge", "-", "lecture", "information", "is", "dense", "and", "contains", "specialized", "vocabulary", ".", "This", "makes", "it", "hard", "to", "identify", "and", "schedule", "captionists", "who", "are", "both", "skilled", "in", "typing", "and", "have", "the", "appropriate", "content", "knowledge", ".", "Another", "captioning", "issue", "involves", "transcription", "delay", ",", "which", "occurs", "when", "captionists", "have", "to", "understand", "the", "phonemes", "or", "words", "and", "then", "type", "in", "what", "they", "have", "recognized", ".", "As", "a", "result", ",", "captionists", "tend", "to", "type", "the", "material", "to", "students", "with", "a", "delay", "of", "several", "seconds", ".", "This", "prevents", "students", "from", "eﬀectively", "participating", "in", "an", "interactive", "classroom", ".", "Another", "challenge", "is", "speaker", "identiﬁcation", ",", "in", "which", "captionist", "are", "unfamiliar", "with", "participants", "and", "are", "challenged", "to", "properly", "identify", "the", "current", "speaker", ".", "They", "can", "simplify", "this", "by", "recognizing", "the", "speaker", "by", "name", ",", "or", "asking", "the", "speaker", "to", "pause", "before", "beginning", "until", "the", "captionist", "has", "caught", "up", "and", "had", "an", "opportunity", "to", "identify", "the", "new", "speaker", ".", "In", "terms", "of", "availability", ",", "captionists", "typically", "are", "not", "available", "to", "transcribe", "live", "speech", "or", "dialogue", "for", "short", "periods", "or", "on-demand", ".", "Professional", "captionists", "usually", "need", "at", "least", "a", "few", "hours", "advance", "notice", ",", "and", "prefer", "to", "work", "in", "1-hour", "increments", "so", "as", "to", "account", "for", "their", "commute", "times", ".", "As", "a", "result", ",", "students", "can", "not", "easily", "decide", "at", "the", "last", "minute", "to", "attend", "a", "lecture", "or", "after", "class", "interactions", "with", "peers", "and", "teacher", ".", "Captionists", "used", "to", "need", "to", "be", "physically", "present", "at", "the", "event", "they", "were", "transcribing", ",", "but", "captioning", "services", "are", "increasingly", "being", "oﬀered", "remotely", "[", "12", ",", "1", "]", ".", "Captionists", "often", "are", "simply", "not", "available", "for", "many", "technical", "ﬁelds", "[", "21", ",", "8", "]", ".", "Remote", "captioning", "oﬀers", "the", "potential", "to", "recruit", "captionists", "familiar", "with", "a", "particular", "subject", "(", "e.g.", ",", "organic", "chemistry", ")", "even", "if", "the", "captionist", "is", "located", "far", "away", "from", "an", "event", ".", "Selecting", "for", "expertise", "further", "reduces", "the", "pool", "of", "captionists", ".", "A", "ﬁnal", "challenge", "is", "their", "cost", "-", "professional", "captionists", "are", "highly", "trained", "to", "keep", "up", "with", "speech", "with", "low", "errors", "rates", ",", "and", "so", "are", "highly", "paid", ".", "Experienced", "verbatim", "captionists", "’", "pay", "can", "exceed", "$", "200", "an", "hour", ",", "and", "newly", "trained", "summarization", "captionists", "can", "go", "as", "low", "as", "$", "60", "an", "hour", "[", "21", "]", ".Automatic", "Speech", "RecognitionASR", "platforms", "typically", "use", "probabilistic", "approaches", "to", "translate", "speech", "to", "text", ".", "These", "platforms", "face", "challenges", "in", "accurately", "capturing", "modern", "classroom", "lectures", "that", "can", "have", "one", "or", "more", "of", "the", "following", "challenges", ":", "extensive", "technical", "vocabulary", ",", "poor", "acoustic", "quality", ",", "multiple", "information", "sources", ",", "speaker", "accents", ",", "or", "other", "problems", ".", "They", "also", "impose", "a", "processing", "delay", "of", "several", "seconds", "and", "the", "delay", "lengthens", "as", "the", "amount", "of", "data", "to", "be", "analyzed", "gets", "bigger", ".", "In", "other", "words", ",", "ASR", "works", "well", "under", "ideal", "situations", ",", "but", "degrades", "quickly", "in", "many", "real", "settings", ".", "Kheir", "et", "al", ".", "[", "12", "]", "found", "that", "untrained", "ASR", "software", "had", "75", "%", "accuracy", "rate", ",", "but", "with", "training", ",", "could", "go", "to", "90", "%", "under", "ideal", "single", "speaker", ",", "but", "this", "accuracy", "rate", "was", "still", "too", "low", "for", "use", "by", "deaf", "students", ".", "In", "the", "best", "possible", "case", ",", "in", "which", "the", "speaker", "has", "trained", "the", "ASR", "and", "wears", "a", "high-quality", ",", "noise-canceling", "microphone", ",", "the", "accuracy", "can", "be", "above", "90", "%", ".", "When", "recording", "a", "speaker", "using", "a", "standard", "microphone", "on", "ASR", "not", "trained", "for", "the", "speaker", ",", "accuracy", "rates", "plummet", "to", "far", "below", "50", "%", ".", "Additionally", ",", "the", "errors", "made", "by", "ASR", "often", "change", "the", "meaning", "of", "the", "text", ",", "whereas", "we", "have", "found", "non-", "expert", "captionists", "are", "much", "more", "likely", "to", "simply", "omit", "words", "or", "make", "spelling", "errors", ".", "In", "Figure", "2", "for", "instance", ",", "the", "ASR", "changes", "‘", "two", "fold", "axis", "’", "to", "‘", "twenty", "four", "lexus", "’", ",", "whereas", "the", "c", "typists", "typically", "omit", "words", "they", "do", "not", "understand", "or", "make", "spelling", "errors", ".", "Current", "ASR", "is", "speaker-dependent", ",", "has", "diﬃculty", "recognizing", "domain-speciﬁc", "jargon", ",", "and", "adapts", "poorly", "to", "vocal", "changes", ",", "such", "as", "when", "the", "speaker", "is", "sick", "[", "6", ",", "7", "]", ".", "ASR", "systems", "generally", "need", "substantial", "computing", "power", "and", "high-quality", "audio", "to", "work", "well", ",", "which", "means", "systems", "can", "be", "diﬃcult", "to", "transport", ".", "They", "are", "also", "ill-equipped", "to", "recognize", "and", "convey", "tone", ",", "attitudes", ",", "interest", "and", "emphasis", ",", "and", "to", "refer", "to", "visual", "information", "such", "as", "slides", "or", "demonstrations", ".", "ASR", "services", "charge", "about", "$", "15-20", "an", "hour", ".", "However", ",", "these", "systems", "are", "more", "easily", "integrated", "with", "other", "functions", "such", "as", "multimedia", "indexing.Crowd", "Captions", "in", "the", "ClassroomDeaf", "and", "hard", "of", "hearing", "students", "have", "had", "a", "long", "history", "of", "enhancing", "their", "classroom", "accessibility", "by", "collaborating", "with", "classmates", ".", "For", "example", ",", "they", "often", "arrange", "to", "copy", "notes", "from", "a", "classmate", "and", "share", "it", "with", "their", "study", "group", ".", "Crowdsourcing", "has", "been", "applied", "to", "oﬄine", "transcription", "with", "great", "success", "[", "2", "]", ",", "but", "has", "just", "recently", "been", "used", "for", "real-", "time", "transcription", "[", "15", "]", ".", "Applying", "a", "collaborative", "captioning", "approach", "among", "classmates", "enables", "real-time", "transcription", "from", "multiple", "non-experts", ",", "and", "crowd", "agreement", "mechanisms", "can", "be", "utilized", "to", "vet", "transcript", "quality", "[", "14", "]", ".We", "imagine", "a", "deaf", "or", "hard", "of", "hearing", "person", "eventually", "being", "able", "to", "capture", "aural", "speech", "with", "her", "cellphone", "anywhere", "and", "have", "captions", "returned", "to", "her", "with", "a", "few", "seconds", "latency", ".", "She", "may", "use", "this", "to", "follow", "along", "in", "a", "lecture", "for", "which", "a", "professional", "captionist", "was", "not", "requested", ",", "to", "participate", "in", "informal", "conversation", "with", "peers", "after", "class", ",", "or", "enjoy", "a", "movie", "or", "other", "live", "event", "that", "lacks", "closed", "captioning", ".", "These", "use", "cases", "currently", "beyond", "the", "scope", "of", "ASR", ",", "and", "their", "serendipitous", "nature", "precludes", "pre-arranging", "a", "professional", "captionist", ".", "Lasecki", "et", "al", ".", "have", "demonstrated", "that", "a", "modest", "number", "of", "people", "can", "provide", "reasonably", "high", "coverage", "over", "the", "caption", "stream", ",", "and", "introduces", "an", "algorithm", "that", "uses", "overlapping", "portions", "of", "the", "sequences", "to", "align", "and", "merge", "them", "using", "the", "Legion", ":", "Scribe", "system", "[", "15", "]", ".", "Scribe", "is", "based", "on", "the", "Legion", "[", "13", "]", "framework", ",", "which", "uses", "crowds", "of…………………………….that", "has", "a", "two", "fold", "axis…….………….have", "a", "crystal", "that……………………………", ".....", "we", "have", "a", "crystal………………………………………", ".....", "we", "have", "a", "crystal", "that", "has", "a", "two", "fold", "axis…", "..", "Figure", "2", ":", "The", "crowd", "captioning", "interface", ".", "The", "interface", "provides", "a", "text", "input", "box", "at", "the", "bottom", ",", "and", "shifts", "text", "up", "as", "users", "type", "(", "either", "when", "the", "text", "hits", "the", "end", "of", "the", "box", ",", "or", "when", "the", "user", "presses", "the", "enter", "key", ")", ".", "To", "encourage", "users", "to", "continue", "typing", "even", "when", "making", "mistakes", ",", "editing", "of", "text", "is", "disabled", "word", "by", "word", ".", "Partial", "captions", "are", "forwarded", "to", "the", "server", "in", "real-time", ",", "which", "uses", "overlapping", "segments", "and", "the", "order", "in", "segments", "are", "received", "to", "align", "and", "merge", "them.workers", "to", "accomplish", "tasks", "in", "real-time", ".", "Unlike", "Legion", ",", "Scribe", "merges", "responses", "to", "create", "a", "single", ",", "better", ",", "response", "instead", "of", "selecting", "from", "inputs", "to", "select", "the", "best", "sequence", ".", "This", "merger", "is", "done", "using", "an", "online", "multiple", "sequence", "alignment", "algorithm", "that", "aligns", "worker", "input", "to", "both", "reconstruct", "the", "ﬁnal", "stream", "and", "correct", "errors", "(", "such", "as", "spelling", "mistakes", ")", "made", "by", "individual", "workers.Crowd", "captioning", "oﬀers", "several", "potential", "beneﬁts", "over", "existing", "approaches", ".", "First", ",", "it", "is", "potentially", "much", "cheaper", "than", "hiring", "a", "professional", "captionist", "because", "non-expert", "captionists", "do", "not", "need", "extensive", "training", "to", "acquire", "a", "speciﬁc", "skill", "set", ",", "and", "thus", "may", "be", "drawn", "from", "a", "variety", "of", "sources", ",", "e.g", ".", "classmates", ",", "audience", "members", ",", "microtask", "marketplaces", ",", "volunteers", ",", "or", "aﬀordable", "and", "readily", "available", "employees", ".", "Our", "workforce", "can", "be", "very", "large", "because", ",", "for", "people", "who", "can", "hear", ",", "speech", "recognition", "is", "relatively", "easy", "and", "most", "people", "can", "type", "accurately", ".", "The", "problem", "is", "that", "individually", "they", "can", "not", "type", "quickly", "enough", "to", "keep", "up", "with", "natural", "speaking", "rates", ",", "and", "crowd", "captioning", "nicely", "remedies", "this", "problem", ".", "Recent", "work", "has", "demonstrated", "that", "small", "crowds", "can", "be", "recruited", "quickly", "on-demand", "(", "in", "less", "than", "2", "seconds", ")", "from", "such", "sources", "[", "4", ",", "3", "]", ".", "Scribe4Me", "enabled", "DHH", "users", "toreceive", "a", "transcript", "of", "a", "short", "sound", "sequence", "in", "a", "few", "minutes", ",", "but", "is", "not", "able", "to", "produce", "verbatim", "captions", "over", "long", "periods", "of", "time", "[", "17", "]", ".In", "previous", "work", ",", "we", "developed", "a", "crowd", "captioning", "system", "that", "accepts", "realtime", "transcription", "from", "multiple", "non-", "experts", "as", "shown", "in", "Figure", "2", ".", "While", "non-experts", "can", "not", "type", "as", "quickly", "as", "the", "natural", "speaking", "rate", ",", "we", "have", "found", "that", "they", "can", "provide", "accurate", "partial", "captions", ".", "Our", "system", "recruits", "fellow", "students", "with", "no", "training", "and", "compensates", "for", "slower", "typing", "speed", "and", "lower", "accuracy", "by", "combining", "the", "efforts", "of", "multiple", "captionists", "simultaneously", "and", "merges", "these", "partial", "captions", "in", "real-time", ".", "We", "have", "shown", "that", "groups", "of", "non-experts", "can", "achieve", "more", "timely", "captions", "than", "a", "professional", "captionist", ",", "that", "we", "can", "encourage", "them", "to", "focus", "on", "speciﬁc", "portions", "of", "the", "speech", "to", "improve", "global", "coverage", ",", "and", "that", "it", "is", "possible", "to", "recombine", "partial", "captions", "and", "eﬀectively", "tradeoﬀ", "coverage", "and", "precision", "[", "15", "]", ".Real-time", "text", "reading", "versus", "listeningMost", "people", "only", "see", "real-time", "text", "on", "TV", "at", "the", "bar", "or", "gym", "in", "the", "form", "of", "closed", "captions", ",", "which", "tend", "to", "have", "noticeable", "errors", ".", "However", ",", "those", "programs", "are", "captioned", "by", "live", "captionists", "or", "stenographers", ".", "To", "reduce", "errors", ",", "these", "real-time", "transcripts", "are", "often", "corrected", "and", "made", "into", "a", "permanent", "part", "of", "the", "video", "ﬁle", "by", "oﬀ-line", "captionists", "who", "prepare", "captions", "from", "pre-recorded", "videotapes", "and", "thoroughly", "review", "the", "work", "for", "errors", "before", "airing.The", "translation", "of", "speech", "to", "text", "is", "not", "direct", ",", "but", "rather", "is", "interpreted", "and", "changed", "in", "the", "course", "of", "each", "utterance", ".", "Markers", "like", "accent", ",", "tone", ",", "and", "timbre", "are", "stripped", "out", "and", "represented", "by", "standardized", "written", "words", "and", "symbols", ".", "Then", "the", "reader", "interprets", "these", "words", "and", "ﬂow", "to", "make", "meanings", "for", "themselves", ".", "Captionists", "tend", "not", "to", "include", "all", "spoken", "information", "so", "that", "readers", "can", "keep", "up", "with", "the", "transcript", ".", "Captionists", "are", "encouraged", "to", "alter", "the", "original", "transcription", "to", "provide", "time", "for", "the", "readers", "to", "completely", "read", "the", "caption", "and", "to", "synchronize", "with", "the", "audio", ".", "This", "is", "needed", "because", ",", "for", "a", "non-orthographic", "language", "like", "English", ",", "the", "length", "of", "a", "spoken", "utterance", "is", "not", "necessarily", "proportional", "to", "the", "length", "of", "a", "spelled", "word", ".", "In", "other", "words", ",", "reading", "speed", "is", "not", "the", "same", "as", "listening", "speed", ",", "especially", "for", "real-", "time", "scrolling", "text", ",", "as", "opposed", "to", "static", "pre-prepared", "text", ".", "For", "static", "text", ",", "reading", "speed", "has", "been", "measured", "at", "291", "wpm", "[", "19", "]", ".", "By", "contrast", "the", "average", "caption", "rate", "for", "TV", "programs", "is", "141", "wpm", "[", "11", "]", ",", "while", "the", "most", "comfortable", "reading", "rate", "for", "hearing", ",", "hard-of-hearing", ",", "and", "deaf", "adults", "is", "around", "145", "wpm", "[", "10", "]", ".", "The", "reason", "is", "that", "the", "task", "of", "viewing", "real-time", "captions", "involved", "diﬀerent", "processing", "demands", "in", "visual", "location", "and", "tracking", "of", "moving", "text", "on", "a", "dynamic", "background", ".", "English", "literacy", "rates", "among", "deaf", "and", "hard", "of", "hearing", "people", "who", "is", "low", "compared", "to", "hearing", "peers", ".", "Captioning", "research", "has", "shown", "that", "both", "rate", "and", "text", "reduction", "and", "viewer", "reading", "ability", "are", "important", "factors", ",", "and", "that", "captions", "need", "to", "be", "provided", "within", "5", "seconds", "so", "that", "the", "reader", "can", "participate", "[", "20", "]", ".The", "number", "of", "spoken", "words", "and", "their", "complexity", "can", "also", "inﬂuence", "the", "captioning", "decision", "on", "the", "amount", "of", "words", "to", "transcribe", "and", "degree", "of", "summarization", "to", "include", "so", "as", "to", "reduce", "the", "reader", "’", "s", "total", "cognitive", "load", ".", "Jensema", "et", "al", ".", "[", "10", "]", "analyzed", "a", "large", "sample", "of", "captioned", "TV", "programs", "andlecture", "transcripts", "have", "a", "very", "diﬀerent", "proﬁle", ".", "For", "comparison", "purposes", ",", "we", "selected", "a", "50", "minute", "long", "clip", "from", "the", "MIT", "Open", "CourseWare", "(", "OCW", ")", "website1", ".", "The", "audio", "sample", "was", "picked", "from", "a", "lecture", "segment", "in", "which", "the", "speech", "was", "relatively", "clear.We", "chose", "this", "lecture", "because", "it", "combined", "both", "technical", "and", "non-technical", "components", ".", "We", "found", "that", "the", "lecture", "had", "9137", "words", ",", "of", "which", "1428", "were", "unique", ",", "at", "182.7", "wpm", ".", "Furthermore", ",", "over", "two", "thirds", "of", "the", "transcript", "consisted", "of", "around", "500", "words", ",", "which", "is", "double", "the", "size", "of", "the", "captioned", "TV", "word", "set", "."]}, "filename": "5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc_Image_002.png", "orig_filename": "5fe63a4c521dd10ca9ec9b4e7dc07f6194c564cc", "split": "train"}, {"article_id": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "description": {"raw": "On the far left is a bar graph of the Whack-A-Mole scores for each participant. The highest score was about 90 points (P10) while the lowest was about 60 points (P7). On the center left is a bar graph of the number of planes destroyed and number of collisions in Shoot 'em Up. All participants destroyed more than 13 enemy planes. On the center is a bar graph of the number of rallies in Multi-player Pong for each participant. P7 and P8 performed around 9 rallies on average, while P9 performed less than 2 rallies on average. On the center right is a bar graph of the Rhythm game scores for each participant. On the far right is a scatter plot of Rhythm game score against moving target selection error. The Rhythm game scores (excluding that of P1) were analyzed to show that participants with lower time error in the moving target selection task (study 2) scored higher in Rhythm game.", "tokens": ["On", "the", "far", "left", "is", "a", "bar", "graph", "of", "the", "Whack-A-Mole", "scores", "for", "each", "participant", ".", "The", "highest", "score", "was", "about", "90", "points", "(", "P10", ")", "while", "the", "lowest", "was", "about", "60", "points", "(", "P7", ")", ".", "On", "the", "center", "left", "is", "a", "bar", "graph", "of", "the", "number", "of", "planes", "destroyed", "and", "number", "of", "collisions", "in", "Shoot", "'em", "Up", ".", "All", "participants", "destroyed", "more", "than", "13", "enemy", "planes", ".", "On", "the", "center", "is", "a", "bar", "graph", "of", "the", "number", "of", "rallies", "in", "Multi-player", "Pong", "for", "each", "participant", ".", "P7", "and", "P8", "performed", "around", "9", "rallies", "on", "average", ",", "while", "P9", "performed", "less", "than", "2", "rallies", "on", "average", ".", "On", "the", "center", "right", "is", "a", "bar", "graph", "of", "the", "Rhythm", "game", "scores", "for", "each", "participant", ".", "On", "the", "far", "right", "is", "a", "scatter", "plot", "of", "Rhythm", "game", "score", "against", "moving", "target", "selection", "error", ".", "The", "Rhythm", "game", "scores", "(", "excluding", "that", "of", "P1", ")", "were", "analyzed", "to", "show", "that", "participants", "with", "lower", "time", "error", "in", "the", "moving", "target", "selection", "task", "(", "study", "2", ")", "scored", "higher", "in", "Rhythm", "game", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects ", "tokens": ["ThroughHand", ":", "2D", "Tactile", "Interaction", "to", "Simultaneously", "Recognize", "and", "Touch", "Multiple", "Objects"]}, "filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_016.png", "orig_filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "split": "train"}, {"article_id": "Evaluating an iPad Game to Address Overselectivity in Preliterate AAC Users with Minimal Verbal Behavior", "description": {"raw": "Two line graphs appear with Group A's findings stacked above Group B's findings. Each particpant that completed both the majority of game play and testing appear in their respective group's graph. For group A, that is P1, P2, and P5. For group B that is P7, P8, P11. Each particpant's score for each assessment is dispalyed in contionous line that run across the X axis that displays days of assessments  (1-3, 11-13, and days 24-26). The Y axis is the score on the MCR assessmet (0-14). The top graph has a phase line after day 3 to indicate that intervention began at this time for this group. The phase line is drawn down to graph below but jogs over to day 13 to indicate when treatment started for this group. Each participant's data is represetned by a uniqie shae and line style. Descriptive text of these data are provided in resutls section.", "tokens": ["Two", "line", "graphs", "appear", "with", "Group", "A", "'s", "findings", "stacked", "above", "Group", "B", "'s", "findings", ".", "Each", "particpant", "that", "completed", "both", "the", "majority", "of", "game", "play", "and", "testing", "appear", "in", "their", "respective", "group", "'s", "graph", ".", "For", "group", "A", ",", "that", "is", "P1", ",", "P2", ",", "and", "P5", ".", "For", "group", "B", "that", "is", "P7", ",", "P8", ",", "P11", ".", "Each", "particpant", "'s", "score", "for", "each", "assessment", "is", "dispalyed", "in", "contionous", "line", "that", "run", "across", "the", "X", "axis", "that", "displays", "days", "of", "assessments", "(", "1-3", ",", "11-13", ",", "and", "days", "24-26", ")", ".", "The", "Y", "axis", "is", "the", "score", "on", "the", "MCR", "assessmet", "(", "0-14", ")", ".", "The", "top", "graph", "has", "a", "phase", "line", "after", "day", "3", "to", "indicate", "that", "intervention", "began", "at", "this", "time", "for", "this", "group", ".", "The", "phase", "line", "is", "drawn", "down", "to", "graph", "below", "but", "jogs", "over", "to", "day", "13", "to", "indicate", "when", "treatment", "started", "for", "this", "group", ".", "Each", "participant", "'s", "data", "is", "represetned", "by", "a", "uniqie", "shae", "and", "line", "style", ".", "Descriptive", "text", "of", "these", "data", "are", "provided", "in", "resutls", "section", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Evaluating an iPad Game to Address Overselectivity in Preliterate AAC Users with Minimal Verbal Behavior ", "tokens": ["Evaluating", "an", "iPad", "Game", "to", "Address", "Overselectivity", "in", "Preliterate", "AAC", "Users", "with", "Minimal", "Verbal", "Behavior"]}, "filename": "d39c93d0017363f1aaeb8e372cf8a124d33a8297_Image_005.jpg", "orig_filename": "d39c93d0017363f1aaeb8e372cf8a124d33a8297", "split": "train"}, {"article_id": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "description": {"raw": "This bar chart shows the accuracy of the compared methods.  The baseline is at 36%.  The BOW is at 45.2%.  The Human (no vote) is at 58.5%.  The CNN is at 69.8%.  The Human is at 70.9% and the multimodal model is at 74.0%", "tokens": ["This", "bar", "chart", "shows", "the", "accuracy", "of", "the", "compared", "methods", ".", "The", "baseline", "is", "at", "36", "%", ".", "The", "BOW", "is", "at", "45.2", "%", ".", "The", "Human", "(", "no", "vote", ")", "is", "at", "58.5", "%", ".", "The", "CNN", "is", "at", "69.8", "%", ".", "The", "Human", "is", "at", "70.9", "%", "and", "the", "multimodal", "model", "is", "at", "74.0", "%"]}, "caption": {"raw": "Figure 7. Comparison of different methods on the dataset. The baseline accuracy is based upon a naive classifier selecting the most frequent class in the dataset. The Bag of Words uses text only to perform classification. The Human (no vote) is the average human accuracy if their vote is left out of the ground truth label. CNN uses the visual features only. Human accuracy is the average turker accuracy if their vote counts towards the class label. Finally, our presented multimodal model performs the best with an accuracy of 74.0%.", "tokens": ["Figure", "7", ".", "Comparison", "of", "different", "methods", "on", "the", "dataset", ".", "The", "baseline", "accuracy", "is", "based", "upon", "a", "naive", "classifier", "selecting", "the", "most", "frequent", "class", "in", "the", "dataset", ".", "The", "Bag", "of", "Words", "uses", "text", "only", "to", "perform", "classification", ".", "The", "Human", "(", "no", "vote", ")", "is", "the", "average", "human", "accuracy", "if", "their", "vote", "is", "left", "out", "of", "the", "ground", "truth", "label", ".", "CNN", "uses", "the", "visual", "features", "only", ".", "Human", "accuracy", "is", "the", "average", "turker", "accuracy", "if", "their", "vote", "counts", "towards", "the", "class", "label", ".", "Finally", ",", "our", "presented", "multimodal", "model", "performs", "the", "best", "with", "an", "accuracy", "of", "74.0", "%", "."]}, "context": {"raw": "Multimodal Deep Learning using Images and Text for Information Graphic Classification Figure 7. Comparison of different methods on the dataset. The baseline accuracy is based upon a naive classifier selecting the most frequent class in the dataset. The Bag of Words uses text only to perform classification. The Human (no vote) is the average human accuracy if their vote is left out of the ground truth label. CNN uses the visual features only. Human accuracy is the average turker accuracy if their vote counts towards the class label. Finally, our presented multimodal model performs the best with an accuracy of 74.0%.", "tokens": ["Multimodal", "Deep", "Learning", "using", "Images", "and", "Text", "for", "Information", "Graphic", "Classification", "Figure", "7", ".", "Comparison", "of", "different", "methods", "on", "the", "dataset", ".", "The", "baseline", "accuracy", "is", "based", "upon", "a", "naive", "classifier", "selecting", "the", "most", "frequent", "class", "in", "the", "dataset", ".", "The", "Bag", "of", "Words", "uses", "text", "only", "to", "perform", "classification", ".", "The", "Human", "(", "no", "vote", ")", "is", "the", "average", "human", "accuracy", "if", "their", "vote", "is", "left", "out", "of", "the", "ground", "truth", "label", ".", "CNN", "uses", "the", "visual", "features", "only", ".", "Human", "accuracy", "is", "the", "average", "turker", "accuracy", "if", "their", "vote", "counts", "towards", "the", "class", "label", ".", "Finally", ",", "our", "presented", "multimodal", "model", "performs", "the", "best", "with", "an", "accuracy", "of", "74.0", "%", "."]}, "filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_009.jpg", "orig_filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "split": "train"}, {"article_id": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "description": {"raw": "The bar chart shows the throughput result of each combination of longitude levels (0-7) and latitude levels (0-2).       The values are (ordered in wrist, forearm, and elbow):       (N) 2.873, 2.827, 2.644;       (NW) 2.677, 2.673, 2.503;       (W) 2.765, 2.649, 2.386;       (SW) 2.880, 2.766, 2.373;      (S) 2.921, 2.790, 2.422;      (SE) 2.965, 2.809, 2.561;      (E) 2.951, 2.936, 2.704;      (NE) 2.943, 2.964, 2.752.      Gray lines indicate significant pairs between W x Wrist vs. W x Elbow, W x Forearm vs. W x Elbow, SW x Wrist vs. SW x Elbow, SW x Forearm vs. SW x Elbow, S x Wrist vs. S x Elbow, S x Forearm vs. S x Elbow, and SE x Wrist vs. SE x Elbow.", "tokens": ["The", "bar", "chart", "shows", "the", "throughput", "result", "of", "each", "combination", "of", "longitude", "levels", "(", "0-7", ")", "and", "latitude", "levels", "(", "0-2", ")", ".", "The", "values", "are", "(", "ordered", "in", "wrist", ",", "forearm", ",", "and", "elbow", ")", ":", "(", "N", ")", "2.873", ",", "2.827", ",", "2.644", ";", "(", "NW", ")", "2.677", ",", "2.673", ",", "2.503", ";", "(", "W", ")", "2.765", ",", "2.649", ",", "2.386", ";", "(", "SW", ")", "2.880", ",", "2.766", ",", "2.373", ";", "(", "S", ")", "2.921", ",", "2.790", ",", "2.422", ";", "(", "SE", ")", "2.965", ",", "2.809", ",", "2.561", ";", "(", "E", ")", "2.951", ",", "2.936", ",", "2.704", ";", "(", "NE", ")", "2.943", ",", "2.964", ",", "2.752", ".", "Gray", "lines", "indicate", "significant", "pairs", "between", "W", "x", "Wrist", "vs.", "W", "x", "Elbow", ",", "W", "x", "Forearm", "vs.", "W", "x", "Elbow", ",", "SW", "x", "Wrist", "vs.", "SW", "x", "Elbow", ",", "SW", "x", "Forearm", "vs.", "SW", "x", "Elbow", ",", "S", "x", "Wrist", "vs.", "S", "x", "Elbow", ",", "S", "x", "Forearm", "vs.", "S", "x", "Elbow", ",", "and", "SE", "x", "Wrist", "vs", ".", "SE", "x", "Elbow", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality ", "tokens": ["Armstrong", ":", "An", "Empirical", "Examination", "of", "Pointing", "at", "Non-Dominant", "Arm-Anchored", "UIs", "in", "Virtual", "Reality"]}, "filename": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_016.jpg", "orig_filename": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "split": "train"}, {"article_id": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "description": {"raw": "This image illustrates how progressively downloaded data is displayed when a user pans a visualization that uses the Splash framework.", "tokens": ["This", "image", "illustrates", "how", "progressively", "downloaded", "data", "is", "displayed", "when", "a", "user", "pans", "a", "visualization", "that", "uses", "the", "Splash", "framework", "."]}, "caption": {"raw": "Figure 1: The Splash framework enables real-time navigation for client-server visualization systems by progressively loading data.", "tokens": ["Figure", "1", ":", "The", "Splash", "framework", "enables", "real-time", "navigation", "for", "client-server", "visualization", "systems", "by", "progressively", "loading", "data", "."]}, "context": {"raw": "Dive in!: enabling progressive loading for real-time navigation of data visualizations Figure 1: The Splash framework enables real-time navigation for client-server visualization systems by progressively loading data.", "tokens": ["Dive", "in", "!", ":", "enabling", "progressive", "loading", "for", "real-time", "navigation", "of", "data", "visualizations", "Figure", "1", ":", "The", "Splash", "framework", "enables", "real-time", "navigation", "for", "client-server", "visualization", "systems", "by", "progressively", "loading", "data", "."]}, "filename": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_001.jpg", "orig_filename": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "split": "train"}, {"article_id": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "description": {"raw": "Bar chart with average time on each visual sources for hearing participants: slides, ASL, and instructor. Differences between control and SlidePacer conditions are significant for all visual sources.", "tokens": ["Bar", "chart", "with", "average", "time", "on", "each", "visual", "sources", "for", "hearing", "participants", ":", "slides", ",", "ASL", ",", "and", "instructor", ".", "Differences", "between", "control", "and", "SlidePacer", "conditions", "are", "significant", "for", "all", "visual", "sources", "."]}, "caption": {"raw": "Figure 6. Average time hearing participants spent looking at each visual source.", "tokens": ["Figure", "6", ".", "Average", "time", "hearing", "participants", "spent", "looking", "at", "each", "visual", "source", "."]}, "context": {"raw": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students Figure 6. Average time hearing participants spent looking at each visual source.", "tokens": ["SlidePacer", ":", "A", "Presentation", "Delivery", "Tool", "for", "Instructors", "of", "Deaf", "and", "Hard", "of", "Hearing", "Students", "Figure", "6", ".", "Average", "time", "hearing", "participants", "spent", "looking", "at", "each", "visual", "source", "."]}, "filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_010.png", "orig_filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "split": "train"}, {"article_id": "Wheelchair-based game design for older adults", "description": {"raw": "A graph showing the average completion times per gesture. Body-based gestures average around 2500 milliseconds while wheelchair-based gestures take less time for moving forward and backward with an average of about 2000 milliseconds, and turning gestures taking longer with about 3400 milliseconds.", "tokens": ["A", "graph", "showing", "the", "average", "completion", "times", "per", "gesture", ".", "Body-based", "gestures", "average", "around", "2500", "milliseconds", "while", "wheelchair-based", "gestures", "take", "less", "time", "for", "moving", "forward", "and", "backward", "with", "an", "average", "of", "about", "2000", "milliseconds", ",", "and", "turning", "gestures", "taking", "longer", "with", "about", "3400", "milliseconds", "."]}, "caption": {"raw": "Figure 1. Avg. completion time per gesture in ms (CI=95%).", "tokens": ["Figure", "1", ".", "Avg", ".", "completion", "time", "per", "gesture", "in", "ms", "(", "CI=95", "%", ")", "."]}, "context": {"raw": "Wheelchair-based game design for older adults Figure 1. Avg. completion time per gesture in ms (CI=95%).", "tokens": ["Wheelchair-based", "game", "design", "for", "older", "adults", "Figure", "1", ".", "Avg", ".", "completion", "time", "per", "gesture", "in", "ms", "(", "CI=95", "%", ")", "."]}, "filename": "f8361de968dc956b7a2e1f19b3f8ccfc4ba18b0d_Image_001.jpg", "orig_filename": "f8361de968dc956b7a2e1f19b3f8ccfc4ba18b0d", "split": "train"}, {"article_id": "Exploration and avoidance of surrounding obstacles for the visually impaired", "description": {"raw": "Figure 3 The flow chart of walking mode  It describes how the system works in the walking mode. When pressing Button \"B\" to start the walking mode (the users start to wal), the system will automatically detect the surrounding until some obstacles in 2 meters. Then the wiimote cane will vibrate a short amount of time, and then the system will render the current situation on the Braille display via pre-designed tactile symbols. The user can also continue to the walking mode after finding a open space, or change to the inspection mode.", "tokens": ["Figure", "3", "The", "flow", "chart", "of", "walking", "mode", "It", "describes", "how", "the", "system", "works", "in", "the", "walking", "mode", ".", "When", "pressing", "Button", "``", "B", "''", "to", "start", "the", "walking", "mode", "(", "the", "users", "start", "to", "wal", ")", ",", "the", "system", "will", "automatically", "detect", "the", "surrounding", "until", "some", "obstacles", "in", "2", "meters", ".", "Then", "the", "wiimote", "cane", "will", "vibrate", "a", "short", "amount", "of", "time", ",", "and", "then", "the", "system", "will", "render", "the", "current", "situation", "on", "the", "Braille", "display", "via", "pre-designed", "tactile", "symbols", ".", "The", "user", "can", "also", "continue", "to", "the", "walking", "mode", "after", "finding", "a", "open", "space", ",", "or", "change", "to", "the", "inspection", "mode", "."]}, "caption": {"raw": "Figure 3. The flow chart of walking mode", "tokens": ["Figure", "3", ".", "The", "flow", "chart", "of", "walking", "mode"]}, "context": {"raw": "Exploration and avoidance of surrounding obstacles for the visually impaired Figure 3. The flow chart of walking mode", "tokens": ["Exploration", "and", "avoidance", "of", "surrounding", "obstacles", "for", "the", "visually", "impaired", "Figure", "3", ".", "The", "flow", "chart", "of", "walking", "mode"]}, "filename": "c4205307c399541fee920fffc5ec5e7ad8598ba8_Image_006.gif", "orig_filename": "c4205307c399541fee920fffc5ec5e7ad8598ba8", "split": "train"}, {"article_id": "A General Methodology to Quantify Biases in Natural Language Data", "description": {"raw": "Figure 6: Error bar of user ratings on how samples state ideological stereotypes.", "tokens": ["Figure", "6", ":", "Error", "bar", "of", "user", "ratings", "on", "how", "samples", "state", "ideological", "stereotypes", "."]}, "caption": {"raw": "Figure 6: Error bar of user ratings on how samples state ideological stereotypes, among data detected as most-biased and after mitigation using observational and latent space features. 1: very implicitly; 5: very explicitly.", "tokens": ["Figure", "6", ":", "Error", "bar", "of", "user", "ratings", "on", "how", "samples", "state", "ideological", "stereotypes", ",", "among", "data", "detected", "as", "most-biased", "and", "after", "mitigation", "using", "observational", "and", "latent", "space", "features", ".", "1", ":", "very", "implicitly", ";", "5", ":", "very", "explicitly", "."]}, "context": {"raw": "A General Methodology to Quantify Biases in Natural Language Data Figure 6: Error bar of user ratings on how samples state ideological stereotypes, among data detected as most-biased and after mitigation using observational and latent space features. 1: very implicitly; 5: very explicitly.", "tokens": ["A", "General", "Methodology", "to", "Quantify", "Biases", "in", "Natural", "Language", "Data", "Figure", "6", ":", "Error", "bar", "of", "user", "ratings", "on", "how", "samples", "state", "ideological", "stereotypes", ",", "among", "data", "detected", "as", "most-biased", "and", "after", "mitigation", "using", "observational", "and", "latent", "space", "features", ".", "1", ":", "very", "implicitly", ";", "5", ":", "very", "explicitly", "."]}, "filename": "ce4b730f4eaddd98be36ce4ba95548a95a6d69fb_Image_011.jpg", "orig_filename": "ce4b730f4eaddd98be36ce4ba95548a95a6d69fb", "split": "train"}, {"article_id": "BraillePlay: educational smartphone games for blind children", "description": {"raw": "A bar chart entitle time spent playing games. The vertical axis reads hours played and the horizontal axis reads p1- p8. P1, P2, P6, and P8 spent around an hour total playing VBReader (VBR) and VBWriter (VBW). P3, P4,  abd P7 spent close to 4 hours playing both games. P5 spent about an hour playing VBR and VBW and 4 hours playing VBH.", "tokens": ["A", "bar", "chart", "entitle", "time", "spent", "playing", "games", ".", "The", "vertical", "axis", "reads", "hours", "played", "and", "the", "horizontal", "axis", "reads", "p1-", "p8", ".", "P1", ",", "P2", ",", "P6", ",", "and", "P8", "spent", "around", "an", "hour", "total", "playing", "VBReader", "(", "VBR", ")", "and", "VBWriter", "(", "VBW", ")", ".", "P3", ",", "P4", ",", "abd", "P7", "spent", "close", "to", "4", "hours", "playing", "both", "games", ".", "P5", "spent", "about", "an", "hour", "playing", "VBR", "and", "VBW", "and", "4", "hours", "playing", "VBH", "."]}, "caption": {"raw": "Figure 3. Time spent by participants playing VBHangman (VBH), VBReader (VBR), and VBWriter (VBW).", "tokens": ["Figure", "3", ".", "Time", "spent", "by", "participants", "playing", "VBHangman", "(", "VBH", ")", ",", "VBReader", "(", "VBR", ")", ",", "and", "VBWriter", "(", "VBW", ")", "."]}, "context": {"raw": "BraillePlay: educational smartphone games for blind children Figure 3. Time spent by participants playing VBHangman (VBH), VBReader (VBR), and VBWriter (VBW).", "tokens": ["BraillePlay", ":", "educational", "smartphone", "games", "for", "blind", "children", "Figure", "3", ".", "Time", "spent", "by", "participants", "playing", "VBHangman", "(", "VBH", ")", ",", "VBReader", "(", "VBR", ")", ",", "and", "VBWriter", "(", "VBW", ")", "."]}, "filename": "88d0f2d52847de405a413a43638329de94b8911c_Image_004.png", "orig_filename": "88d0f2d52847de405a413a43638329de94b8911c", "split": "train"}, {"article_id": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "description": {"raw": "Box plot of the time for the participants to learn 26 letters(in seconds). The median value for each letter varies. Letter o has the lowest median(20.095), and letter f has the largest median(81.526), and most(19 letters) are below 50 seconds. The ranges from upper quartile to lower quartile also varies for each letter. Letter a, c, o has small quartile ranges but a few outliers, and letter f, n, q, s has large quartile ranges.", "tokens": ["Box", "plot", "of", "the", "time", "for", "the", "participants", "to", "learn", "26", "letters", "(", "in", "seconds", ")", ".", "The", "median", "value", "for", "each", "letter", "varies", ".", "Letter", "o", "has", "the", "lowest", "median", "(", "20.095", ")", ",", "and", "letter", "f", "has", "the", "largest", "median", "(", "81.526", ")", ",", "and", "most", "(", "19", "letters", ")", "are", "below", "50", "seconds", ".", "The", "ranges", "from", "upper", "quartile", "to", "lower", "quartile", "also", "varies", "for", "each", "letter", ".", "Letter", "a", ",", "c", ",", "o", "has", "small", "quartile", "ranges", "but", "a", "few", "outliers", ",", "and", "letter", "f", ",", "n", ",", "q", ",", "s", "has", "large", "quartile", "ranges", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone ", "tokens": ["LightWrite", ":", "Teach", "Handwriting", "to", "The", "Visually", "Impaired", "with", "A", "Smartphone"]}, "filename": "1bd67672e9376feaac80770695717b13ece2c47b_Image_007.png", "orig_filename": "1bd67672e9376feaac80770695717b13ece2c47b", "split": "train"}, {"article_id": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "description": {"raw": "Bar chart of number of participants are satisfied about how the tracker data reflects their daily activities, shows that 95% of the participants somewhat to strongly agree to this.", "tokens": ["Bar", "chart", "of", "number", "of", "participants", "are", "satisfied", "about", "how", "the", "tracker", "data", "reflects", "their", "daily", "activities", ",", "shows", "that", "95", "%", "of", "the", "participants", "somewhat", "to", "strongly", "agree", "to", "this", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection ", "tokens": ["The", "Technology-Mediated", "Reflection", "Model", ":", "Barriers", "and", "Assistance", "in", "Data-Driven", "Reflection"]}, "filename": "09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_010.jpg", "orig_filename": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "split": "train"}, {"article_id": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics", "description": {"raw": "Figure 7a. Time on task Boxplot with task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).   Figure 7b. Excess motion: Total trial vs last 1 second while locking on target. I/O Braid had more excess motion compared to Buttons and Scroll. The boxplots show median values with quartiles and min/max extent.   Figure 7c. Weighted average subjective feedback.  We mapped the 7-point Likert scale to a score in the range [-3, 3] for Ease of Use, Perceived Accuracy and Tactile Feel. We multiplied the score by the number of times the technique received that rating and computed an average for all the scores. The chart show favorable scores for I/O Braid and Scroll, whereas Buttons was the least popular.", "tokens": ["Figure", "7a", ".", "Time", "on", "task", "Boxplot", "with", "task", "completion", "times", "for", "the", "three", "input", "devices", ".", "I/O", "Braid", "was", "faster", "than", "Buttons", "(", "statistical", "significance", ")", ".", "Figure", "7b", ".", "Excess", "motion", ":", "Total", "trial", "vs", "last", "1", "second", "while", "locking", "on", "target", ".", "I/O", "Braid", "had", "more", "excess", "motion", "compared", "to", "Buttons", "and", "Scroll", ".", "The", "boxplots", "show", "median", "values", "with", "quartiles", "and", "min/max", "extent", ".", "Figure", "7c", ".", "Weighted", "average", "subjective", "feedback", ".", "We", "mapped", "the", "7-point", "Likert", "scale", "to", "a", "score", "in", "the", "range", "[", "-3", ",", "3", "]", "for", "Ease", "of", "Use", ",", "Perceived", "Accuracy", "and", "Tactile", "Feel", ".", "We", "multiplied", "the", "score", "by", "the", "number", "of", "times", "the", "technique", "received", "that", "rating", "and", "computed", "an", "average", "for", "all", "the", "scores", ".", "The", "chart", "show", "favorable", "scores", "for", "I/O", "Braid", "and", "Scroll", ",", "whereas", "Buttons", "was", "the", "least", "popular", "."]}, "caption": {"raw": "Figure 7. a) Task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).", "tokens": ["Figure", "7.", "a", ")", "Task", "completion", "times", "for", "the", "three", "input", "devices", ".", "I/O", "Braid", "was", "faster", "than", "Buttons", "(", "statistical", "significance", ")", "."]}, "context": {"raw": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics Figure 7. a) Task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).", "tokens": ["E-Textile", "Microinteractions", ":", "Augmenting", "Twist", "with", "Flick", ",", "Slide", "and", "Grasp", "Gestures", "for", "Soft", "Electronics", "Figure", "7.", "a", ")", "Task", "completion", "times", "for", "the", "three", "input", "devices", ".", "I/O", "Braid", "was", "faster", "than", "Buttons", "(", "statistical", "significance", ")", "."]}, "filename": "f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_009.jpg", "orig_filename": "f06a381ef1ff7d02a6a1876548e83db20e71442f", "split": "train"}, {"article_id": "Say It All: Feedback for Improving Non-Visual Presentation Accessibility", "description": {"raw": "Figure 6: A bar graph comparing Presentation A11y and the Default Interface. The y axis is out of 7 points. The comparisons are on the subjects of self-perceived accessibility, helpfulness as reminding for mentioning visuals, and causing distraction. Self-perceived accessibility: Presentation A11y scored 5.06, while Default Interface scored 4.00. Helpfulness as reminding for mentioning visuals: Presentation A11y scored 5.56, while Default Interface scored 2.94. Causing distraction: Presentation A11y scored 2.56, while Default Interface scored 2.06.", "tokens": ["Figure", "6", ":", "A", "bar", "graph", "comparing", "Presentation", "A11y", "and", "the", "Default", "Interface", ".", "The", "y", "axis", "is", "out", "of", "7", "points", ".", "The", "comparisons", "are", "on", "the", "subjects", "of", "self-perceived", "accessibility", ",", "helpfulness", "as", "reminding", "for", "mentioning", "visuals", ",", "and", "causing", "distraction", ".", "Self-perceived", "accessibility", ":", "Presentation", "A11y", "scored", "5.06", ",", "while", "Default", "Interface", "scored", "4.00", ".", "Helpfulness", "as", "reminding", "for", "mentioning", "visuals", ":", "Presentation", "A11y", "scored", "5.56", ",", "while", "Default", "Interface", "scored", "2.94", ".", "Causing", "distraction", ":", "Presentation", "A11y", "scored", "2.56", ",", "while", "Default", "Interface", "scored", "2.06", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Say It All: Feedback for Improving Non-Visual Presentation Accessibility ", "tokens": ["Say", "It", "All", ":", "Feedback", "for", "Improving", "Non-Visual", "Presentation", "Accessibility"]}, "filename": "a15cd58325eb7334786ba0de65f961f66965fd44_Image_031.jpg", "orig_filename": "a15cd58325eb7334786ba0de65f961f66965fd44", "split": "train"}, {"article_id": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences", "description": {"raw": "Screenshot of CSV Upload module of PXI Bench. It provides instructions to be followed for filling data in CSV file and also a template CSV with correct column names and one pre-filled entry also.", "tokens": ["Screenshot", "of", "CSV", "Upload", "module", "of", "PXI", "Bench", ".", "It", "provides", "instructions", "to", "be", "followed", "for", "filling", "data", "in", "CSV", "file", "and", "also", "a", "template", "CSV", "with", "correct", "column", "names", "and", "one", "pre-filled", "entry", "also", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "The Player Experience Inventory Bench: Providing Games User Researchers Actionable Insight into Player Experiences ", "tokens": ["The", "Player", "Experience", "Inventory", "Bench", ":", "Providing", "Games", "User", "Researchers", "Actionable", "Insight", "into", "Player", "Experiences"]}, "filename": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef_Image_003.jpg", "orig_filename": "28e5c3eaec14d07028b3cac38db8f33d7a1405ef", "split": "train"}, {"article_id": "Ondulé: Designing and Controlling 3D Printable Springs", "description": {"raw": "Figure 12 shows the interface of our design tool. The figure has two parts. On the left side, the figure shows the Rhino edit scene on the left and the Rhino plugin on the right. The plugin includes three parts. From top to bottom, they are spring generation panel, spring stiffness control panel, and spring behavior design panel. In the spring generation panel, there is a “Control to spring” button and a “Change spring length” button. All the edited springs are recorded in a horizontal box beneath the two buttons. In the spring stiffness control panel, the basic stiffness control allows the user to drag a stiffness level bar to make the change and the advanced stiffness control allows the user to change spring wire diameters and turn gap. In the spring behavior design panel, the user can add different joint design by checking the ratio button. The user can parameterize each joint design in a graphical interface. On the right side of the figure, from top to bottom, it shows the workflows of all three panels. For the top spring generation panel, the user can first select the model body, then generate the spring, and can change the spring length. With spring stiffness control panel, the user can drag the bar to make the change and the new spring with updated stiffness is rendered in the Rhino modeling scene. With the spring behavior design panel, the user can add a joint to the model and the join is rendered in red in the model. By changing the parameters, in this case, changing the compression and extension displacement, the joint is updated directly in the 3D model.", "tokens": ["Figure", "12", "shows", "the", "interface", "of", "our", "design", "tool", ".", "The", "figure", "has", "two", "parts", ".", "On", "the", "left", "side", ",", "the", "figure", "shows", "the", "Rhino", "edit", "scene", "on", "the", "left", "and", "the", "Rhino", "plugin", "on", "the", "right", ".", "The", "plugin", "includes", "three", "parts", ".", "From", "top", "to", "bottom", ",", "they", "are", "spring", "generation", "panel", ",", "spring", "stiffness", "control", "panel", ",", "and", "spring", "behavior", "design", "panel", ".", "In", "the", "spring", "generation", "panel", ",", "there", "is", "a", "“", "Control", "to", "spring", "”", "button", "and", "a", "“", "Change", "spring", "length", "”", "button", ".", "All", "the", "edited", "springs", "are", "recorded", "in", "a", "horizontal", "box", "beneath", "the", "two", "buttons", ".", "In", "the", "spring", "stiffness", "control", "panel", ",", "the", "basic", "stiffness", "control", "allows", "the", "user", "to", "drag", "a", "stiffness", "level", "bar", "to", "make", "the", "change", "and", "the", "advanced", "stiffness", "control", "allows", "the", "user", "to", "change", "spring", "wire", "diameters", "and", "turn", "gap", ".", "In", "the", "spring", "behavior", "design", "panel", ",", "the", "user", "can", "add", "different", "joint", "design", "by", "checking", "the", "ratio", "button", ".", "The", "user", "can", "parameterize", "each", "joint", "design", "in", "a", "graphical", "interface", ".", "On", "the", "right", "side", "of", "the", "figure", ",", "from", "top", "to", "bottom", ",", "it", "shows", "the", "workflows", "of", "all", "three", "panels", ".", "For", "the", "top", "spring", "generation", "panel", ",", "the", "user", "can", "first", "select", "the", "model", "body", ",", "then", "generate", "the", "spring", ",", "and", "can", "change", "the", "spring", "length", ".", "With", "spring", "stiffness", "control", "panel", ",", "the", "user", "can", "drag", "the", "bar", "to", "make", "the", "change", "and", "the", "new", "spring", "with", "updated", "stiffness", "is", "rendered", "in", "the", "Rhino", "modeling", "scene", ".", "With", "the", "spring", "behavior", "design", "panel", ",", "the", "user", "can", "add", "a", "joint", "to", "the", "model", "and", "the", "join", "is", "rendered", "in", "red", "in", "the", "model", ".", "By", "changing", "the", "parameters", ",", "in", "this", "case", ",", "changing", "the", "compression", "and", "extension", "displacement", ",", "the", "joint", "is", "updated", "directly", "in", "the", "3D", "model", "."]}, "caption": {"raw": ". The Ondulé spring design tool interface (left) has four parts: Rhino modeling environment, a spring generation panel, a spring stiffness control panel, and a spring behavior design panel. The workflow for each design panel is shown on the right.", "tokens": [".", "The", "Ondulé", "spring", "design", "tool", "interface", "(", "left", ")", "has", "four", "parts", ":", "Rhino", "modeling", "environment", ",", "a", "spring", "generation", "panel", ",", "a", "spring", "stiffness", "control", "panel", ",", "and", "a", "spring", "behavior", "design", "panel", ".", "The", "workflow", "for", "each", "design", "panel", "is", "shown", "on", "the", "right", "."]}, "context": {"raw": "Ondulé: Designing and Controlling 3D Printable Springs . The Ondulé spring design tool interface (left) has four parts: Rhino modeling environment, a spring generation panel, a spring stiffness control panel, and a spring behavior design panel. The workflow for each design panel is shown on the right.", "tokens": ["Ondulé", ":", "Designing", "and", "Controlling", "3D", "Printable", "Springs", ".", "The", "Ondulé", "spring", "design", "tool", "interface", "(", "left", ")", "has", "four", "parts", ":", "Rhino", "modeling", "environment", ",", "a", "spring", "generation", "panel", ",", "a", "spring", "stiffness", "control", "panel", ",", "and", "a", "spring", "behavior", "design", "panel", ".", "The", "workflow", "for", "each", "design", "panel", "is", "shown", "on", "the", "right", "."]}, "filename": "21e4fe87b1a374e69791fac357242a0ff64e93f3_Image_028.jpg", "orig_filename": "21e4fe87b1a374e69791fac357242a0ff64e93f3", "split": "train"}, {"article_id": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "description": {"raw": "Plot of Effect of Surfaced Hits on ML-20M. Shows boycotts and data strikes. As size of boycotts/data strikes increases, Surfaced Hits are reduced. Zoomed in on effects between 0.71 and 0.78.", "tokens": ["Plot", "of", "Effect", "of", "Surfaced", "Hits", "on", "ML-20M", ".", "Shows", "boycotts", "and", "data", "strikes", ".", "As", "size", "of", "boycotts/data", "strikes", "increases", ",", "Surfaced", "Hits", "are", "reduced", ".", "Zoomed", "in", "on", "effects", "between", "0.71", "and", "0.78", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies ", "tokens": ["“", "Data", "Strikes", "”", ":", "Evaluating", "the", "Effectiveness", "of", "a", "New", "Form", "of", "Collective", "Action", "Against", "Technology", "Companies"]}, "filename": "41cbffad975874060d643c36c8bdb5c72637564e_Image_007.jpg", "orig_filename": "41cbffad975874060d643c36c8bdb5c72637564e", "split": "train"}, {"article_id": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "description": {"raw": "A vertical bar graph of participants' perspective of the markup styles being distracting. The x axis is the 4 markup styles: No Change, Yellow, Italics, and Underline. The y axis is numeric, ranging from 0 to 0.8. The prominent feature is No Change being the least distracting with 0.4. Other styles ranged from 0.5 to 0.7 as their preference with Italics being less distracting than Yellow and Underline.", "tokens": ["A", "vertical", "bar", "graph", "of", "participants", "'", "perspective", "of", "the", "markup", "styles", "being", "distracting", ".", "The", "x", "axis", "is", "the", "4", "markup", "styles", ":", "No", "Change", ",", "Yellow", ",", "Italics", ",", "and", "Underline", ".", "The", "y", "axis", "is", "numeric", ",", "ranging", "from", "0", "to", "0.8", ".", "The", "prominent", "feature", "is", "No", "Change", "being", "the", "least", "distracting", "with", "0.4", ".", "Other", "styles", "ranged", "from", "0.5", "to", "0.7", "as", "their", "preference", "with", "Italics", "being", "less", "distracting", "than", "Yellow", "and", "Underline", "."]}, "caption": {"raw": "Figure 7. Larger Study: Distracting Responses (binary)", "tokens": ["Figure", "7", ".", "Larger", "Study", ":", "Distracting", "Responses", "(", "binary", ")"]}, "context": {"raw": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings Figure 7. Larger Study: Distracting Responses (binary)", "tokens": ["Deaf", "and", "Hard-of-Hearing", "Perspectives", "on", "Imperfect", "Automatic", "Speech", "Recognition", "for", "Captioning", "One-on-One", "Meetings", "Figure", "7", ".", "Larger", "Study", ":", "Distracting", "Responses", "(", "binary", ")"]}, "filename": "471f9168db0fcb72d394222491966b97c098b1cd_Image_017.jpg", "orig_filename": "471f9168db0fcb72d394222491966b97c098b1cd", "split": "train"}, {"article_id": "Toward User-Driven Sound Recognizer Personalization with People Who Are d/Deaf or Hard of Hearing", "description": {"raw": "A stacked bar graph with each participant placed on the x-axis and the number of sound classes on the y-axis. P9 and P4 each recorded between 25-30 sound classes; eight participants each recorded between 15-20; and P10 and P11 recorded between 10-15. 103 classes were indoor mechanical sounds, 59 were presence of people or pets, 30 were non-urgent alerts, 22 were outdoor background, 11 were urgent alerts, and 18 were other sounds.", "tokens": ["A", "stacked", "bar", "graph", "with", "each", "participant", "placed", "on", "the", "x-axis", "and", "the", "number", "of", "sound", "classes", "on", "the", "y-axis", ".", "P9", "and", "P4", "each", "recorded", "between", "25-30", "sound", "classes", ";", "eight", "participants", "each", "recorded", "between", "15-20", ";", "and", "P10", "and", "P11", "recorded", "between", "10-15", ".", "103", "classes", "were", "indoor", "mechanical", "sounds", ",", "59", "were", "presence", "of", "people", "or", "pets", ",", "30", "were", "non-urgent", "alerts", ",", "22", "were", "outdoor", "background", ",", "11", "were", "urgent", "alerts", ",", "and", "18", "were", "other", "sounds", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Toward User-Driven Sound Recognizer Personalization with People Who Are d/Deaf or Hard of Hearing ", "tokens": ["Toward", "User-Driven", "Sound", "Recognizer", "Personalization", "with", "People", "Who", "Are", "d/Deaf", "or", "Hard", "of", "Hearing"]}, "filename": "56f28fa646fe7fc259ec368577db46f0acc317a5_Image_008.png", "orig_filename": "56f28fa646fe7fc259ec368577db46f0acc317a5", "split": "train"}, {"article_id": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards", "description": {"raw": "Red canvas with an orange triangle in the upper left corner and a format task pane to the right of the canvas. The task pane is titled ``Format Background'' and has formatting options under the header ``Fill.'' Next to the label ``Color'' is a button featuring a paint can icon with a red bar underneath and a triangle pointed down, indicating a dropdown menu. Keyboard focus is on this button. At the bottom of the task pane are two buttons, ``Apply to All'' and ``Reset Background.''", "tokens": ["Red", "canvas", "with", "an", "orange", "triangle", "in", "the", "upper", "left", "corner", "and", "a", "format", "task", "pane", "to", "the", "right", "of", "the", "canvas", ".", "The", "task", "pane", "is", "titled", "``", "Format", "Background", "''", "and", "has", "formatting", "options", "under", "the", "header", "``", "Fill", ".", "''", "Next", "to", "the", "label", "``", "Color", "''", "is", "a", "button", "featuring", "a", "paint", "can", "icon", "with", "a", "red", "bar", "underneath", "and", "a", "triangle", "pointed", "down", ",", "indicating", "a", "dropdown", "menu", ".", "Keyboard", "focus", "is", "on", "this", "button", ".", "At", "the", "bottom", "of", "the", "task", "pane", "are", "two", "buttons", ",", "``", "Apply", "to", "All", "''", "and", "``", "Reset", "Background", ".", "''"]}, "caption": {"raw": "The artboard’s state after P2 changed the artboard red. A key command she tried silently switched focus to the art- board and opened a “Format Background” task pane, which she used to change the color of the artboard’s background, leaving her with an orange triangle and a red artboard.Figure 12: P2’s artboard state as she attempts to change the color of the triangle in the upper left corner from blue to red, without any feedback indicating the color changes were successful or what objects the changes were applied to.he would “never dare present my PowerPoint presentation” without someone sighted looking at it frst.Accidental manipulations occurred several times during the task- based usability study due to the lack of feedback both for manipula- tions and object focus, as screen readers did not always announce when the keyboard focus changed from one object to another, orthe announcement was nested in a long string of announcements and was missed by the user. For example, when P2 attempted to change the color of the triangle in task #16 (referenced in Figure 1 and Figure 12), she selected the \"Shape Fill\" button, which flls the selected shape with a default color or can be opened as a dropdown menu to choose more colors (see Figure 12a). When she clicked the button, it flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle (see Figure 12b). P2 was confused and navigated back to the Shape Fill button and clicked it again, and it switched focus back to her triangle once more, prompting her to voice confusion and frustration: “What? Why won’t it do ... It won’t let me get into ...” She tried a key command, which silently switched focus to the artboard and opened a “Format Background” task pane with a dropdown fll button, which she used to select a shade of red that was applied to the artboard, leaving her with an orange triangle and a red artboard (see Figure 12c). After reading through the other formatting options in the task pane and noticing they referred to the “background,” she switched keyboard focus and found that it switched to the triangle, which she had thought she already had focus on. At that point, she decided to end the task, saying, “Uh, I tried. Did not know if it worked, or if I made the whole slide red. [laughs] I did what I could, I don’t know what else I can do.”", "tokens": ["The", "artboard", "’", "s", "state", "after", "P2", "changed", "the", "artboard", "red", ".", "A", "key", "command", "she", "tried", "silently", "switched", "focus", "to", "the", "art-", "board", "and", "opened", "a", "“", "Format", "Background", "”", "task", "pane", ",", "which", "she", "used", "to", "change", "the", "color", "of", "the", "artboard", "’", "s", "background", ",", "leaving", "her", "with", "an", "orange", "triangle", "and", "a", "red", "artboard.Figure", "12", ":", "P2", "’", "s", "artboard", "state", "as", "she", "attempts", "to", "change", "the", "color", "of", "the", "triangle", "in", "the", "upper", "left", "corner", "from", "blue", "to", "red", ",", "without", "any", "feedback", "indicating", "the", "color", "changes", "were", "successful", "or", "what", "objects", "the", "changes", "were", "applied", "to.he", "would", "“", "never", "dare", "present", "my", "PowerPoint", "presentation", "”", "without", "someone", "sighted", "looking", "at", "it", "frst.Accidental", "manipulations", "occurred", "several", "times", "during", "the", "task-", "based", "usability", "study", "due", "to", "the", "lack", "of", "feedback", "both", "for", "manipula-", "tions", "and", "object", "focus", ",", "as", "screen", "readers", "did", "not", "always", "announce", "when", "the", "keyboard", "focus", "changed", "from", "one", "object", "to", "another", ",", "orthe", "announcement", "was", "nested", "in", "a", "long", "string", "of", "announcements", "and", "was", "missed", "by", "the", "user", ".", "For", "example", ",", "when", "P2", "attempted", "to", "change", "the", "color", "of", "the", "triangle", "in", "task", "#", "16", "(", "referenced", "in", "Figure", "1", "and", "Figure", "12", ")", ",", "she", "selected", "the", "``", "Shape", "Fill", "''", "button", ",", "which", "flls", "the", "selected", "shape", "with", "a", "default", "color", "or", "can", "be", "opened", "as", "a", "dropdown", "menu", "to", "choose", "more", "colors", "(", "see", "Figure", "12a", ")", ".", "When", "she", "clicked", "the", "button", ",", "it", "flled", "her", "triangle", "with", "the", "default", "color", "(", "orange", ")", "without", "announcing", "the", "change", ",", "and", "switched", "focus", "from", "the", "menu", "back", "to", "her", "triangle", "(", "see", "Figure", "12b", ")", ".", "P2", "was", "confused", "and", "navigated", "back", "to", "the", "Shape", "Fill", "button", "and", "clicked", "it", "again", ",", "and", "it", "switched", "focus", "back", "to", "her", "triangle", "once", "more", ",", "prompting", "her", "to", "voice", "confusion", "and", "frustration", ":", "“", "What", "?", "Why", "won", "’", "t", "it", "do", "...", "It", "won", "’", "t", "let", "me", "get", "into", "...", "”", "She", "tried", "a", "key", "command", ",", "which", "silently", "switched", "focus", "to", "the", "artboard", "and", "opened", "a", "“", "Format", "Background", "”", "task", "pane", "with", "a", "dropdown", "fll", "button", ",", "which", "she", "used", "to", "select", "a", "shade", "of", "red", "that", "was", "applied", "to", "the", "artboard", ",", "leaving", "her", "with", "an", "orange", "triangle", "and", "a", "red", "artboard", "(", "see", "Figure", "12c", ")", ".", "After", "reading", "through", "the", "other", "formatting", "options", "in", "the", "task", "pane", "and", "noticing", "they", "referred", "to", "the", "“", "background", ",", "”", "she", "switched", "keyboard", "focus", "and", "found", "that", "it", "switched", "to", "the", "triangle", ",", "which", "she", "had", "thought", "she", "already", "had", "focus", "on", ".", "At", "that", "point", ",", "she", "decided", "to", "end", "the", "task", ",", "saying", ",", "“", "Uh", ",", "I", "tried", ".", "Did", "not", "know", "if", "it", "worked", ",", "or", "if", "I", "made", "the", "whole", "slide", "red", ".", "[", "laughs", "]", "I", "did", "what", "I", "could", ",", "I", "don", "’", "t", "know", "what", "else", "I", "can", "do", ".", "”"]}, "context": {"raw": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards The artboard’s state after P2 changed the artboard red. A key command she tried silently switched focus to the art- board and opened a “Format Background” task pane, which she used to change the color of the artboard’s background, leaving her with an orange triangle and a red artboard.Figure 12: P2’s artboard state as she attempts to change the color of the triangle in the upper left corner from blue to red, without any feedback indicating the color changes were successful or what objects the changes were applied to.he would “never dare present my PowerPoint presentation” without someone sighted looking at it frst.Accidental manipulations occurred several times during the task- based usability study due to the lack of feedback both for manipula- tions and object focus, as screen readers did not always announce when the keyboard focus changed from one object to another, orthe announcement was nested in a long string of announcements and was missed by the user. For example, when P2 attempted to change the color of the triangle in task #16 (referenced in Figure 1 and Figure 12), she selected the \"Shape Fill\" button, which flls the selected shape with a default color or can be opened as a dropdown menu to choose more colors (see Figure 12a). When she clicked the button, it flled her triangle with the default color (orange) without announcing the change, and switched focus from the menu back to her triangle (see Figure 12b). P2 was confused and navigated back to the Shape Fill button and clicked it again, and it switched focus back to her triangle once more, prompting her to voice confusion and frustration: “What? Why won’t it do ... It won’t let me get into ...” She tried a key command, which silently switched focus to the artboard and opened a “Format Background” task pane with a dropdown fll button, which she used to select a shade of red that was applied to the artboard, leaving her with an orange triangle and a red artboard (see Figure 12c). After reading through the other formatting options in the task pane and noticing they referred to the “background,” she switched keyboard focus and found that it switched to the triangle, which she had thought she already had focus on. At that point, she decided to end the task, saying, “Uh, I tried. Did not know if it worked, or if I made the whole slide red. [laughs] I did what I could, I don’t know what else I can do.”", "tokens": ["Understanding", "Blind", "Screen-Reader", "Users", "’", "Experiences", "of", "Digital", "Artboards", "The", "artboard", "’", "s", "state", "after", "P2", "changed", "the", "artboard", "red", ".", "A", "key", "command", "she", "tried", "silently", "switched", "focus", "to", "the", "art-", "board", "and", "opened", "a", "“", "Format", "Background", "”", "task", "pane", ",", "which", "she", "used", "to", "change", "the", "color", "of", "the", "artboard", "’", "s", "background", ",", "leaving", "her", "with", "an", "orange", "triangle", "and", "a", "red", "artboard.Figure", "12", ":", "P2", "’", "s", "artboard", "state", "as", "she", "attempts", "to", "change", "the", "color", "of", "the", "triangle", "in", "the", "upper", "left", "corner", "from", "blue", "to", "red", ",", "without", "any", "feedback", "indicating", "the", "color", "changes", "were", "successful", "or", "what", "objects", "the", "changes", "were", "applied", "to.he", "would", "“", "never", "dare", "present", "my", "PowerPoint", "presentation", "”", "without", "someone", "sighted", "looking", "at", "it", "frst.Accidental", "manipulations", "occurred", "several", "times", "during", "the", "task-", "based", "usability", "study", "due", "to", "the", "lack", "of", "feedback", "both", "for", "manipula-", "tions", "and", "object", "focus", ",", "as", "screen", "readers", "did", "not", "always", "announce", "when", "the", "keyboard", "focus", "changed", "from", "one", "object", "to", "another", ",", "orthe", "announcement", "was", "nested", "in", "a", "long", "string", "of", "announcements", "and", "was", "missed", "by", "the", "user", ".", "For", "example", ",", "when", "P2", "attempted", "to", "change", "the", "color", "of", "the", "triangle", "in", "task", "#", "16", "(", "referenced", "in", "Figure", "1", "and", "Figure", "12", ")", ",", "she", "selected", "the", "``", "Shape", "Fill", "''", "button", ",", "which", "flls", "the", "selected", "shape", "with", "a", "default", "color", "or", "can", "be", "opened", "as", "a", "dropdown", "menu", "to", "choose", "more", "colors", "(", "see", "Figure", "12a", ")", ".", "When", "she", "clicked", "the", "button", ",", "it", "flled", "her", "triangle", "with", "the", "default", "color", "(", "orange", ")", "without", "announcing", "the", "change", ",", "and", "switched", "focus", "from", "the", "menu", "back", "to", "her", "triangle", "(", "see", "Figure", "12b", ")", ".", "P2", "was", "confused", "and", "navigated", "back", "to", "the", "Shape", "Fill", "button", "and", "clicked", "it", "again", ",", "and", "it", "switched", "focus", "back", "to", "her", "triangle", "once", "more", ",", "prompting", "her", "to", "voice", "confusion", "and", "frustration", ":", "“", "What", "?", "Why", "won", "’", "t", "it", "do", "...", "It", "won", "’", "t", "let", "me", "get", "into", "...", "”", "She", "tried", "a", "key", "command", ",", "which", "silently", "switched", "focus", "to", "the", "artboard", "and", "opened", "a", "“", "Format", "Background", "”", "task", "pane", "with", "a", "dropdown", "fll", "button", ",", "which", "she", "used", "to", "select", "a", "shade", "of", "red", "that", "was", "applied", "to", "the", "artboard", ",", "leaving", "her", "with", "an", "orange", "triangle", "and", "a", "red", "artboard", "(", "see", "Figure", "12c", ")", ".", "After", "reading", "through", "the", "other", "formatting", "options", "in", "the", "task", "pane", "and", "noticing", "they", "referred", "to", "the", "“", "background", ",", "”", "she", "switched", "keyboard", "focus", "and", "found", "that", "it", "switched", "to", "the", "triangle", ",", "which", "she", "had", "thought", "she", "already", "had", "focus", "on", ".", "At", "that", "point", ",", "she", "decided", "to", "end", "the", "task", ",", "saying", ",", "“", "Uh", ",", "I", "tried", ".", "Did", "not", "know", "if", "it", "worked", ",", "or", "if", "I", "made", "the", "whole", "slide", "red", ".", "[", "laughs", "]", "I", "did", "what", "I", "could", ",", "I", "don", "’", "t", "know", "what", "else", "I", "can", "do", ".", "”"]}, "filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_022.jpg", "orig_filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "split": "train"}, {"article_id": "Stroke-Gesture Input for People with Motor Impairments: Empirical Results & Research Roadmap", "description": {"raw": "Three line chart graphs showing recognition accuracy rates for stroke-gestures articulated by participants with and without motor impairments", "tokens": ["Three", "line", "chart", "graphs", "showing", "recognition", "accuracy", "rates", "for", "stroke-gestures", "articulated", "by", "participants", "with", "and", "without", "motor", "impairments"]}, "caption": {"raw": "Figure 3. Recognition accuracy rates for stroke-gestures articulated by participants with motor impairments (left) and without impairments (middle) under standard training, and recognition rates for participants with motor impairments (right) under mixed training (see text for description), function of the number of training participants P. Note: error bars show 95% CIs.", "tokens": ["Figure", "3", ".", "Recognition", "accuracy", "rates", "for", "stroke-gestures", "articulated", "by", "participants", "with", "motor", "impairments", "(", "left", ")", "and", "without", "impairments", "(", "middle", ")", "under", "standard", "training", ",", "and", "recognition", "rates", "for", "participants", "with", "motor", "impairments", "(", "right", ")", "under", "mixed", "training", "(", "see", "text", "for", "description", ")", ",", "function", "of", "the", "number", "of", "training", "participants", "P.", "Note", ":", "error", "bars", "show", "95", "%", "CIs", "."]}, "context": {"raw": "Stroke-Gesture Input for People with Motor Impairments: Empirical Results & Research Roadmap Figure 3. Recognition accuracy rates for stroke-gestures articulated by participants with motor impairments (left) and without impairments (middle) under standard training, and recognition rates for participants with motor impairments (right) under mixed training (see text for description), function of the number of training participants P. Note: error bars show 95% CIs.", "tokens": ["Stroke-Gesture", "Input", "for", "People", "with", "Motor", "Impairments", ":", "Empirical", "Results", "&", "Research", "Roadmap", "Figure", "3", ".", "Recognition", "accuracy", "rates", "for", "stroke-gestures", "articulated", "by", "participants", "with", "motor", "impairments", "(", "left", ")", "and", "without", "impairments", "(", "middle", ")", "under", "standard", "training", ",", "and", "recognition", "rates", "for", "participants", "with", "motor", "impairments", "(", "right", ")", "under", "mixed", "training", "(", "see", "text", "for", "description", ")", ",", "function", "of", "the", "number", "of", "training", "participants", "P.", "Note", ":", "error", "bars", "show", "95", "%", "CIs", "."]}, "filename": "e8f307fe9618a608b5503003fd1490908dd2077e_Image_013.jpg", "orig_filename": "e8f307fe9618a608b5503003fd1490908dd2077e", "split": "train"}, {"article_id": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "description": {"raw": "Figure 2: A series of three visualizations. On the left, a histogram of 'life expectancy' on x and count on y. Next, text stating '+ country' followed by a binned circle plot of the two fields, 'country' on y, 'life expectancy on x, and count on size. Finally, text stating '+ fertility' and a colored tick plot, with 'life expectancy' on x, 'fertility' on y, and 'country' on color.", "tokens": ["Figure", "2", ":", "A", "series", "of", "three", "visualizations", ".", "On", "the", "left", ",", "a", "histogram", "of", "'life", "expectancy", "'", "on", "x", "and", "count", "on", "y", ".", "Next", ",", "text", "stating", "'+", "country", "'", "followed", "by", "a", "binned", "circle", "plot", "of", "the", "two", "fields", ",", "'country", "'", "on", "y", ",", "'life", "expectancy", "on", "x", ",", "and", "count", "on", "size", ".", "Finally", ",", "text", "stating", "'+", "fertility", "'", "and", "a", "colored", "tick", "plot", ",", "with", "'life", "expectancy", "'", "on", "x", ",", "'fertility", "'", "on", "y", ",", "and", "'country", "'", "on", "color", "."]}, "caption": {"raw": "exploratory analysis tools (e.g., Voyager [27, 28], Tableau) mix the two authoring paradigms to marry agency and automa- tion [9] and provide a more efﬁcient and amenable visualiza- tion authoring experience.", "tokens": ["exploratory", "analysis", "tools", "(", "e.g.", ",", "Voyager", "[", "27", ",", "28", "]", ",", "Tableau", ")", "mix", "the", "two", "authoring", "paradigms", "to", "marry", "agency", "and", "automa-", "tion", "[", "9", "]", "and", "provide", "a", "more", "efﬁcient", "and", "amenable", "visualiza-", "tion", "authoring", "experience", "."]}, "context": {"raw": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations exploratory analysis tools (e.g., Voyager [27, 28], Tableau) mix the two authoring paradigms to marry agency and automa- tion [9] and provide a more efﬁcient and amenable visualiza- tion authoring experience.", "tokens": ["Dziban", ":", "Balancing", "Agency", "&", "Automation", "in", "Visualization", "Design", "via", "Anchored", "Recommendations", "exploratory", "analysis", "tools", "(", "e.g.", ",", "Voyager", "[", "27", ",", "28", "]", ",", "Tableau", ")", "mix", "the", "two", "authoring", "paradigms", "to", "marry", "agency", "and", "automa-", "tion", "[", "9", "]", "and", "provide", "a", "more", "efﬁcient", "and", "amenable", "visualiza-", "tion", "authoring", "experience", "."]}, "filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_004.png", "orig_filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "split": "train"}, {"article_id": "Towards Inclusive External Communication of Autonomous Vehicles for Pedestrians with Vision Impairments", "description": {"raw": "This figure shows 2 plots for dominance (a and b). Subfigure a shows an interaction plot. On the x-axis, there are the two levels for the theme \"information content\" (high and low). On the y-axis, there is the dominance score from 4.45 to 4.80. A red line (red standing for two vehicles communicating) is starting left at approximately 4.65 and goes down to 4.45 five in the low content condition. A blue line (indicating one vehicle communicating) is also starting at approximately 4.65 but goes up to 4.80 for the low content condition.  Subfigure b shows for the x-axis the two levels of sight: seeing on the left and VIP on the right. For the y-axis, dominance is depicted from 4.3 to 4.9. The red line (indicating low information content) starts at 4.7 for the seeing going down to 4.3 for the VIP). The blue line indicating high information content, starts at 4.5 and goes up to 4.9 for the VIP.", "tokens": ["This", "figure", "shows", "2", "plots", "for", "dominance", "(", "a", "and", "b", ")", ".", "Subfigure", "a", "shows", "an", "interaction", "plot", ".", "On", "the", "x-axis", ",", "there", "are", "the", "two", "levels", "for", "the", "theme", "``", "information", "content", "''", "(", "high", "and", "low", ")", ".", "On", "the", "y-axis", ",", "there", "is", "the", "dominance", "score", "from", "4.45", "to", "4.80", ".", "A", "red", "line", "(", "red", "standing", "for", "two", "vehicles", "communicating", ")", "is", "starting", "left", "at", "approximately", "4.65", "and", "goes", "down", "to", "4.45", "five", "in", "the", "low", "content", "condition", ".", "A", "blue", "line", "(", "indicating", "one", "vehicle", "communicating", ")", "is", "also", "starting", "at", "approximately", "4.65", "but", "goes", "up", "to", "4.80", "for", "the", "low", "content", "condition", ".", "Subfigure", "b", "shows", "for", "the", "x-axis", "the", "two", "levels", "of", "sight", ":", "seeing", "on", "the", "left", "and", "VIP", "on", "the", "right", ".", "For", "the", "y-axis", ",", "dominance", "is", "depicted", "from", "4.3", "to", "4.9", ".", "The", "red", "line", "(", "indicating", "low", "information", "content", ")", "starts", "at", "4.7", "for", "the", "seeing", "going", "down", "to", "4.3", "for", "the", "VIP", ")", ".", "The", "blue", "line", "indicating", "high", "information", "content", ",", "starts", "at", "4.5", "and", "goes", "up", "to", "4.9", "for", "the", "VIP", "."]}, "caption": {"raw": "Figure 6: Interaction effects for dominance: (a) equal for high content independent of the number of communicating vehicles. With low content, dominance was higher when one vehicle was communicating. (b) Dominance for the VIP was low for low content and high with high content. Seeing participants reported similar dominance values for low and the high content.", "tokens": ["Figure", "6", ":", "Interaction", "effects", "for", "dominance", ":", "(", "a", ")", "equal", "for", "high", "content", "independent", "of", "the", "number", "of", "communicating", "vehicles", ".", "With", "low", "content", ",", "dominance", "was", "higher", "when", "one", "vehicle", "was", "communicating", ".", "(", "b", ")", "Dominance", "for", "the", "VIP", "was", "low", "for", "low", "content", "and", "high", "with", "high", "content", ".", "Seeing", "participants", "reported", "similar", "dominance", "values", "for", "low", "and", "the", "high", "content", "."]}, "context": {"raw": "Towards Inclusive External Communication of Autonomous Vehicles for Pedestrians with Vision Impairments Figure 6: Interaction effects for dominance: (a) equal for high content independent of the number of communicating vehicles. With low content, dominance was higher when one vehicle was communicating. (b) Dominance for the VIP was low for low content and high with high content. Seeing participants reported similar dominance values for low and the high content.", "tokens": ["Towards", "Inclusive", "External", "Communication", "of", "Autonomous", "Vehicles", "for", "Pedestrians", "with", "Vision", "Impairments", "Figure", "6", ":", "Interaction", "effects", "for", "dominance", ":", "(", "a", ")", "equal", "for", "high", "content", "independent", "of", "the", "number", "of", "communicating", "vehicles", ".", "With", "low", "content", ",", "dominance", "was", "higher", "when", "one", "vehicle", "was", "communicating", ".", "(", "b", ")", "Dominance", "for", "the", "VIP", "was", "low", "for", "low", "content", "and", "high", "with", "high", "content", ".", "Seeing", "participants", "reported", "similar", "dominance", "values", "for", "low", "and", "the", "high", "content", "."]}, "filename": "c0d867d3d13cbd748d77e960b554f8ea2eb0860d_Image_010.jpg", "orig_filename": "c0d867d3d13cbd748d77e960b554f8ea2eb0860d", "split": "train"}, {"article_id": "TextAlive: Integrated Design Environment for Kinetic Typography", "description": {"raw": "Comments and GUI widgets generated for horizontalAlignment (left, ceter, and right with radio buttons), and Draw track, offset slide bar, color, and imagePath.", "tokens": ["Comments", "and", "GUI", "widgets", "generated", "for", "horizontalAlignment", "(", "left", ",", "ceter", ",", "and", "right", "with", "radio", "buttons", ")", ",", "and", "Draw", "track", ",", "offset", "slide", "bar", ",", "color", ",", "and", "imagePath", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "TextAlive: Integrated Design Environment for Kinetic Typography ", "tokens": ["TextAlive", ":", "Integrated", "Design", "Environment", "for", "Kinetic", "Typography"]}, "filename": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557_Image_005.jpg", "orig_filename": "110c0a6b32e87bfc3b1af1210db7fd4e9fd7c557", "split": "train"}, {"article_id": "Usable gestures for blind people: understanding preference and performance", "description": {"raw": "Bar chart of ratings by gesture category, for blind and sighted participants.  Values on a scale from 1 to 7: Tap gestures: Rated 6.1 by blind participants, 6.3 by sighted participants Flick gestures: Rated 6.5 by blind participants, 6.3 by sighted participants Multi-touch gestures: Rated 5.7  by blind participants, 4.3 by sighted participants Shape gestures: Rated 4.9 by blind participants, 5.7  by sighted participants Symbol gestures: Rated 5.4 by blind participants, 5.7 by sighted participants", "tokens": ["Bar", "chart", "of", "ratings", "by", "gesture", "category", ",", "for", "blind", "and", "sighted", "participants", ".", "Values", "on", "a", "scale", "from", "1", "to", "7", ":", "Tap", "gestures", ":", "Rated", "6.1", "by", "blind", "participants", ",", "6.3", "by", "sighted", "participants", "Flick", "gestures", ":", "Rated", "6.5", "by", "blind", "participants", ",", "6.3", "by", "sighted", "participants", "Multi-touch", "gestures", ":", "Rated", "5.7", "by", "blind", "participants", ",", "4.3", "by", "sighted", "participants", "Shape", "gestures", ":", "Rated", "4.9", "by", "blind", "participants", ",", "5.7", "by", "sighted", "participants", "Symbol", "gestures", ":", "Rated", "5.4", "by", "blind", "participants", ",", "5.7", "by", "sighted", "participants"]}, "caption": {"raw": "Figure 4. Participants’ easiness ratings by gesture category.", "tokens": ["Figure", "4", ".", "Participants", "’", "easiness", "ratings", "by", "gesture", "category", "."]}, "context": {"raw": "Usable gestures for blind people: understanding preference and performance Figure 4. Participants’ easiness ratings by gesture category.", "tokens": ["Usable", "gestures", "for", "blind", "people", ":", "understanding", "preference", "and", "performance", "Figure", "4", ".", "Participants", "’", "easiness", "ratings", "by", "gesture", "category", "."]}, "filename": "9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e_Image_006.jpg", "orig_filename": "9e93e21f6e300a80b870a31ed6c33e8c6ec2d81e", "split": "train"}, {"article_id": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery", "description": {"raw": "A graph showing improvements in the models' precision and recall as more training data is added. The x-axis uses a log scale ranging from approximately 500 crops in the training set, to approximately two hundred thousand crops in the training set. Both overall precision and overall recall increase from approximately 60 percent to approximately 80 percent.", "tokens": ["A", "graph", "showing", "improvements", "in", "the", "models", "'", "precision", "and", "recall", "as", "more", "training", "data", "is", "added", ".", "The", "x-axis", "uses", "a", "log", "scale", "ranging", "from", "approximately", "500", "crops", "in", "the", "training", "set", ",", "to", "approximately", "two", "hundred", "thousand", "crops", "in", "the", "training", "set", ".", "Both", "overall", "precision", "and", "overall", "recall", "increase", "from", "approximately", "60", "percent", "to", "approximately", "80", "percent", "."]}, "caption": {"raw": "Figure 8: Performance overall and by feature type as the size of the training set increases. Note the log scale on the x axis.", "tokens": ["Figure", "8", ":", "Performance", "overall", "and", "by", "feature", "type", "as", "the", "size", "of", "the", "training", "set", "increases", ".", "Note", "the", "log", "scale", "on", "the", "x", "axis", "."]}, "context": {"raw": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery Figure 8: Performance overall and by feature type as the size of the training set increases. Note the log scale on the x axis.", "tokens": ["Deep", "Learning", "for", "Automatically", "Detecting", "Sidewalk", "Accessibility", "Problems", "Using", "Streetscape", "Imagery", "Figure", "8", ":", "Performance", "overall", "and", "by", "feature", "type", "as", "the", "size", "of", "the", "training", "set", "increases", ".", "Note", "the", "log", "scale", "on", "the", "x", "axis", "."]}, "filename": "48fc79059aecbe211fe15aaca6c96d2dff7650fc_Image_015.jpg", "orig_filename": "48fc79059aecbe211fe15aaca6c96d2dff7650fc", "split": "train"}, {"article_id": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "description": {"raw": "A bar graph titled Figure 2: H1-a for Noticed Errors (Boolean). The figure is split into three facets (subgraphs), from left to right: WRAT-L, WRAT-M, and WRAT-H. The vertical axis ranges from 0 to 1, with ticks at increments of 0.1. On the horizontal axis are the labels for the WER accuracy levels in the study: Desktop, Cloud, and Human. In the WRAT-L facet, all pairwise comparisons are not significant, and the means are approximately: 0.55 for Desktop, 0.45 for Cloud, and 0.4 for Human. In the WRAT-M facet, only the Desktop-Human pairwise comparison was significant with two stars, and the means are approximately: 0.7 for Desktop, 0.65 for Cloud, and 0.55 for Human. In the WRAT-H facet, all pairwise comparisons are not significant, and the means are approximately: 0.95 for Desktop, 0.9 for Cloud, and 0.85 for Human.", "tokens": ["A", "bar", "graph", "titled", "Figure", "2", ":", "H1-a", "for", "Noticed", "Errors", "(", "Boolean", ")", ".", "The", "figure", "is", "split", "into", "three", "facets", "(", "subgraphs", ")", ",", "from", "left", "to", "right", ":", "WRAT-L", ",", "WRAT-M", ",", "and", "WRAT-H", ".", "The", "vertical", "axis", "ranges", "from", "0", "to", "1", ",", "with", "ticks", "at", "increments", "of", "0.1", ".", "On", "the", "horizontal", "axis", "are", "the", "labels", "for", "the", "WER", "accuracy", "levels", "in", "the", "study", ":", "Desktop", ",", "Cloud", ",", "and", "Human", ".", "In", "the", "WRAT-L", "facet", ",", "all", "pairwise", "comparisons", "are", "not", "significant", ",", "and", "the", "means", "are", "approximately", ":", "0.55", "for", "Desktop", ",", "0.45", "for", "Cloud", ",", "and", "0.4", "for", "Human", ".", "In", "the", "WRAT-M", "facet", ",", "only", "the", "Desktop-Human", "pairwise", "comparison", "was", "significant", "with", "two", "stars", ",", "and", "the", "means", "are", "approximately", ":", "0.7", "for", "Desktop", ",", "0.65", "for", "Cloud", ",", "and", "0.55", "for", "Human", ".", "In", "the", "WRAT-H", "facet", ",", "all", "pairwise", "comparisons", "are", "not", "significant", ",", "and", "the", "means", "are", "approximately", ":", "0.95", "for", "Desktop", ",", "0.9", "for", "Cloud", ",", "and", "0.85", "for", "Human", "."]}, "caption": {"raw": "Figure 2. H1-a for Noticed Errors (Boolean)", "tokens": ["Figure", "2", ".", "H1-a", "for", "Noticed", "Errors", "(", "Boolean", ")"]}, "context": {"raw": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels Figure 2. H1-a for Noticed Errors (Boolean)", "tokens": ["Methods", "for", "Evaluation", "of", "Imperfect", "Captioning", "Tools", "by", "Deaf", "or", "Hard-of-Hearing", "Users", "at", "Different", "Reading", "Literacy", "Levels", "Figure", "2", ".", "H1-a", "for", "Noticed", "Errors", "(", "Boolean", ")"]}, "filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_003.jpg", "orig_filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "split": "train"}, {"article_id": "From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People", "description": {"raw": "Figure 2 is an illustration of the design for the dialog tree of a chatbot. It represents how today's chatbot systems often work. For example, as illustrated in this chart, the chatbot first classifies a user's input message as one out of the four predefined intents. This step is called clustering. Then, the chatbot can trigger a corresponding response generation algorithm to generate a response.", "tokens": ["Figure", "2", "is", "an", "illustration", "of", "the", "design", "for", "the", "dialog", "tree", "of", "a", "chatbot", ".", "It", "represents", "how", "today", "'s", "chatbot", "systems", "often", "work", ".", "For", "example", ",", "as", "illustrated", "in", "this", "chart", ",", "the", "chatbot", "first", "classifies", "a", "user", "'s", "input", "message", "as", "one", "out", "of", "the", "four", "predefined", "intents", ".", "This", "step", "is", "called", "clustering", ".", "Then", ",", "the", "chatbot", "can", "trigger", "a", "corresponding", "response", "generation", "algorithm", "to", "generate", "a", "response", "."]}, "caption": {"raw": "Figure 2: Chatbot", "tokens": ["Figure", "2", ":", "Chatbot"]}, "context": {"raw": "From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People Figure 2: Chatbot", "tokens": ["From", "Human-Human", "Collaboration", "to", "Human-AI", "Collaboration", ":", "Designing", "AI", "Systems", "That", "Can", "Work", "Together", "with", "People", "Figure", "2", ":", "Chatbot"]}, "filename": "660a2244efa8af0b77fd314a1c75dcc01aa677fe_Image_003.jpg", "orig_filename": "660a2244efa8af0b77fd314a1c75dcc01aa677fe", "split": "train"}, {"article_id": "Coco's Videos: An Empirical Investigation of Video-Player Design Features and Children's Media Use", "description": {"raw": "Left: A cartoon bear-like creature with an alarm clock is on the left side of the screen, with a speech bubble that says, \"How long should we watch videos?\" On the right side are two time pickers, with the labels \"hr\" and \"min\" and displays that read \"0\" and \"20.\" On the bottom right is an arrow button.  Middle: On the left is the same cartoon bear with a speech bubble that reads \"What will you do next?\" On the right are nine buttons that each have text and a related image. The text of these read: Eat, Leave, Play outside, Sleep, Play with toys, See friends, Read a book, Bath time, Something else! The one that says \"Play outside\" is highlighted with a light background. In the bottom right corner of the screen is an arrow button.  Right: A video player shows a cartoon. The video player is nearly full-screen, with player controls (seek bar, pause button, etc.) around the edges. A large transparent overlay is in the middle of the screen. On it is a drawing of an alarm clock with a speech bubble that says \"One minute left!\"", "tokens": ["Left", ":", "A", "cartoon", "bear-like", "creature", "with", "an", "alarm", "clock", "is", "on", "the", "left", "side", "of", "the", "screen", ",", "with", "a", "speech", "bubble", "that", "says", ",", "``", "How", "long", "should", "we", "watch", "videos", "?", "''", "On", "the", "right", "side", "are", "two", "time", "pickers", ",", "with", "the", "labels", "``", "hr", "''", "and", "``", "min", "''", "and", "displays", "that", "read", "``", "0", "''", "and", "``", "20", ".", "''", "On", "the", "bottom", "right", "is", "an", "arrow", "button", ".", "Middle", ":", "On", "the", "left", "is", "the", "same", "cartoon", "bear", "with", "a", "speech", "bubble", "that", "reads", "``", "What", "will", "you", "do", "next", "?", "''", "On", "the", "right", "are", "nine", "buttons", "that", "each", "have", "text", "and", "a", "related", "image", ".", "The", "text", "of", "these", "read", ":", "Eat", ",", "Leave", ",", "Play", "outside", ",", "Sleep", ",", "Play", "with", "toys", ",", "See", "friends", ",", "Read", "a", "book", ",", "Bath", "time", ",", "Something", "else", "!", "The", "one", "that", "says", "``", "Play", "outside", "''", "is", "highlighted", "with", "a", "light", "background", ".", "In", "the", "bottom", "right", "corner", "of", "the", "screen", "is", "an", "arrow", "button", ".", "Right", ":", "A", "video", "player", "shows", "a", "cartoon", ".", "The", "video", "player", "is", "nearly", "full-screen", ",", "with", "player", "controls", "(", "seek", "bar", ",", "pause", "button", ",", "etc", ".", ")", "around", "the", "edges", ".", "A", "large", "transparent", "overlay", "is", "in", "the", "middle", "of", "the", "screen", ".", "On", "it", "is", "a", "drawing", "of", "an", "alarm", "clock", "with", "a", "speech", "bubble", "that", "says", "``", "One", "minute", "left", "!", "''"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Coco's Videos: An Empirical Investigation of Video-Player Design Features and Children's Media Use ", "tokens": ["Coco", "'s", "Videos", ":", "An", "Empirical", "Investigation", "of", "Video-Player", "Design", "Features", "and", "Children", "'s", "Media", "Use"]}, "filename": "623885990db455959efa7dbbc2b326a4aaba4122_Image_002.jpg", "orig_filename": "623885990db455959efa7dbbc2b326a4aaba4122", "split": "train"}, {"article_id": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "description": {"raw": "Magnetic field data when touching various locations of the smartwatch's touchscreen. Magnetic field can be similar even if a touching finger is different. Touch location data is necessary for such cases.", "tokens": ["Magnetic", "field", "data", "when", "touching", "various", "locations", "of", "the", "smartwatch", "'s", "touchscreen", ".", "Magnetic", "field", "can", "be", "similar", "even", "if", "a", "touching", "finger", "is", "different", ".", "Touch", "location", "data", "is", "necessary", "for", "such", "cases", "."]}, "caption": {"raw": "Figure 2. Magnetic ﬁeld data when touching various locations of the smartwatch’s touchscreen. Magnetic ﬁeld can be similar even if a touch- ing ﬁnger is different. Touch location data is necessary for such cases.", "tokens": ["Figure", "2", ".", "Magnetic", "ﬁeld", "data", "when", "touching", "various", "locations", "of", "the", "smartwatch", "’", "s", "touchscreen", ".", "Magnetic", "ﬁeld", "can", "be", "similar", "even", "if", "a", "touch-", "ing", "ﬁnger", "is", "different", ".", "Touch", "location", "data", "is", "necessary", "for", "such", "cases", "."]}, "context": {"raw": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer Figure 2. Magnetic ﬁeld data when touching various locations of the smartwatch’s touchscreen. Magnetic ﬁeld can be similar even if a touch- ing ﬁnger is different. Touch location data is necessary for such cases.", "tokens": ["MagTouch", ":", "Robust", "Finger", "Identification", "for", "a", "Smartwatch", "Using", "a", "Magnet", "Ring", "and", "a", "Built-in", "Magnetometer", "Figure", "2", ".", "Magnetic", "ﬁeld", "data", "when", "touching", "various", "locations", "of", "the", "smartwatch", "’", "s", "touchscreen", ".", "Magnetic", "ﬁeld", "can", "be", "similar", "even", "if", "a", "touch-", "ing", "ﬁnger", "is", "different", ".", "Touch", "location", "data", "is", "necessary", "for", "such", "cases", "."]}, "filename": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_002.png", "orig_filename": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "split": "train"}, {"article_id": "Constructive Visualization to Inform the Design and Exploration of Tactile Data Representations", "description": {"raw": "Left image shows a sketch of a table with: a box with four dividers labelled textured tokens, a set of four tokens and braille labels next to each token labeled as legend, 8 long bars labelled reference bars, a laptop and keyboard labelled laptop with spreadsheet, a sheet of paper labelled Braille table, and a set of short bars labelled Braille labels. Right image shows a sketch of a table with: a sketch of a bar graph labelled tactile graphic, a sheet of paper labelled Braille table, and a laptop and keyboard labelled laptop with spreadsheet.", "tokens": ["Left", "image", "shows", "a", "sketch", "of", "a", "table", "with", ":", "a", "box", "with", "four", "dividers", "labelled", "textured", "tokens", ",", "a", "set", "of", "four", "tokens", "and", "braille", "labels", "next", "to", "each", "token", "labeled", "as", "legend", ",", "8", "long", "bars", "labelled", "reference", "bars", ",", "a", "laptop", "and", "keyboard", "labelled", "laptop", "with", "spreadsheet", ",", "a", "sheet", "of", "paper", "labelled", "Braille", "table", ",", "and", "a", "set", "of", "short", "bars", "labelled", "Braille", "labels", ".", "Right", "image", "shows", "a", "sketch", "of", "a", "table", "with", ":", "a", "sketch", "of", "a", "bar", "graph", "labelled", "tactile", "graphic", ",", "a", "sheet", "of", "paper", "labelled", "Braille", "table", ",", "and", "a", "laptop", "and", "keyboard", "labelled", "laptop", "with", "spreadsheet", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Constructive Visualization to Inform the Design and Exploration of Tactile Data Representations ", "tokens": ["Constructive", "Visualization", "to", "Inform", "the", "Design", "and", "Exploration", "of", "Tactile", "Data", "Representations"]}, "filename": "dda64d3fadc0eba6ce009dc5902f0eba451afcbf_Image_002.jpg", "orig_filename": "dda64d3fadc0eba6ce009dc5902f0eba451afcbf", "split": "train"}, {"article_id": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "description": {"raw": "A line chart showing the change in the number of clusters formed as the paramter D (interval between HITs) changes. The change in the number of clusters diminishes after D=1min.", "tokens": ["A", "line", "chart", "showing", "the", "change", "in", "the", "number", "of", "clusters", "formed", "as", "the", "paramter", "D", "(", "interval", "between", "HITs", ")", "changes", ".", "The", "change", "in", "the", "number", "of", "clusters", "diminishes", "after", "D=1min", "."]}, "caption": {"raw": "Figure 4. Line chart of the number of clusters formed. The change in the number becomes small after D=1min.", "tokens": ["Figure", "4", ".", "Line", "chart", "of", "the", "number", "of", "clusters", "formed", ".", "The", "change", "in", "the", "number", "becomes", "small", "after", "D=1min", "."]}, "context": {"raw": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk Figure 4. Line chart of the number of clusters formed. The change in the number becomes small after D=1min.", "tokens": ["A", "Data-Driven", "Analysis", "of", "Workers", "'", "Earnings", "on", "Amazon", "Mechanical", "Turk", "Figure", "4", ".", "Line", "chart", "of", "the", "number", "of", "clusters", "formed", ".", "The", "change", "in", "the", "number", "becomes", "small", "after", "D=1min", "."]}, "filename": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_004.jpg", "orig_filename": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "split": "train"}, {"article_id": "REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR", "description": {"raw": "Figure 9A shows the results of device position accuracy evaluation. It is a spatial plot showing 8 targets equally spaced in a circle. Measured data points of device final position are overlaid, showing a small average error of 3.5 mm. Figure 9B shows average measured device speed versus target direction relative to the EHD. For a desired speed of 20 cm/s, the average measured speed was 19.4 cm/s. For 25 cm/s, the measured average was 26.6 cm/s. For 30 cm/s, it was 31.5 cm/s. For 35 cm/s, it was 35.6 cm/s.", "tokens": ["Figure", "9A", "shows", "the", "results", "of", "device", "position", "accuracy", "evaluation", ".", "It", "is", "a", "spatial", "plot", "showing", "8", "targets", "equally", "spaced", "in", "a", "circle", ".", "Measured", "data", "points", "of", "device", "final", "position", "are", "overlaid", ",", "showing", "a", "small", "average", "error", "of", "3.5", "mm", ".", "Figure", "9B", "shows", "average", "measured", "device", "speed", "versus", "target", "direction", "relative", "to", "the", "EHD", ".", "For", "a", "desired", "speed", "of", "20", "cm/s", ",", "the", "average", "measured", "speed", "was", "19.4", "cm/s", ".", "For", "25", "cm/s", ",", "the", "measured", "average", "was", "26.6", "cm/s", ".", "For", "30", "cm/s", ",", "it", "was", "31.5", "cm/s", ".", "For", "35", "cm/s", ",", "it", "was", "35.6", "cm/s", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR ", "tokens": ["REACH+", ":", "Extending", "the", "Reachability", "of", "Encountered-type", "Haptics", "Devices", "through", "Dynamic", "Redirection", "in", "VR"]}, "filename": "91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f_Image_009.jpg", "orig_filename": "91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f", "split": "train"}, {"article_id": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "description": {"raw": "Figure 4 - Scatter plot plotting the word error rate of one randomly sampled crowd worker transcription on the X axis, and the average word error rate of the rest of the transcriptions they submitted on the Y axis. There was a positive correlation between the two groups across the collected data.", "tokens": ["Figure", "4", "-", "Scatter", "plot", "plotting", "the", "word", "error", "rate", "of", "one", "randomly", "sampled", "crowd", "worker", "transcription", "on", "the", "X", "axis", ",", "and", "the", "average", "word", "error", "rate", "of", "the", "rest", "of", "the", "transcriptions", "they", "submitted", "on", "the", "Y", "axis", ".", "There", "was", "a", "positive", "correlation", "between", "the", "two", "groups", "across", "the", "collected", "data", "."]}, "caption": {"raw": "Figure 4. There was a positive correlation between the quality of one randomly sampled transcription generated by a unique crowd worker (sample), and the quality of the remainder of their transcriptions (outer). This suggests that fltering based on worker performance on one clip could be used to improve average transcription quality.", "tokens": ["Figure", "4", ".", "There", "was", "a", "positive", "correlation", "between", "the", "quality", "of", "one", "randomly", "sampled", "transcription", "generated", "by", "a", "unique", "crowd", "worker", "(", "sample", ")", ",", "and", "the", "quality", "of", "the", "remainder", "of", "their", "transcriptions", "(", "outer", ")", ".", "This", "suggests", "that", "fltering", "based", "on", "worker", "performance", "on", "one", "clip", "could", "be", "used", "to", "improve", "average", "transcription", "quality", "."]}, "context": {"raw": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users Figure 4. There was a positive correlation between the quality of one randomly sampled transcription generated by a unique crowd worker (sample), and the quality of the remainder of their transcriptions (outer). This suggests that fltering based on worker performance on one clip could be used to improve average transcription quality.", "tokens": ["Towards", "More", "Robust", "Speech", "Interactions", "for", "Deaf", "and", "Hard", "of", "Hearing", "Users", "Figure", "4", ".", "There", "was", "a", "positive", "correlation", "between", "the", "quality", "of", "one", "randomly", "sampled", "transcription", "generated", "by", "a", "unique", "crowd", "worker", "(", "sample", ")", ",", "and", "the", "quality", "of", "the", "remainder", "of", "their", "transcriptions", "(", "outer", ")", ".", "This", "suggests", "that", "fltering", "based", "on", "worker", "performance", "on", "one", "clip", "could", "be", "used", "to", "improve", "average", "transcription", "quality", "."]}, "filename": "8bba1845a85370618cd5c400ec8be42208554549_Image_004.jpg", "orig_filename": "8bba1845a85370618cd5c400ec8be42208554549", "split": "train"}, {"article_id": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "description": {"raw": "Figure 7: An origin chart and follow-up authoring paths. The origin plot is a tick plot, displaying \"IMDB Rating\" on the x axis and \"Major Genre\" on the y axis (`genre_vs_rating = Chart(movies).field('IMDB_Rating', 'Major_Genre')` then a newline and `genre_vs_rating`). On the left, the cold path. The first query adds the \"IMDB Votes\" field (`plus_votes = genre_vs_ratings.field('IMDB_Votes')` then a newline and `plus_votes`), resulting in a binned bubble plot with \"IMDB Votes\" replacing \"IMDB Rating\" on the x axis, and \"IMDB Rating\" moving to the size channel. The second query requests \"IMDB Rating\" back on the x channel (`plus_votes2 = plus_votes.field('IMDB_Rating', channel='x')`, newline then `plus_votes2`). The result is a bubble plot with \"IMDB Rating\" and \"IMDB Votes\" switching channels. The third query requests \"IMDB Rating\" to be unbinned (`plus_votes3 = plus_votes2.field('IMDB_Rating', bin=False)`, newline then `plus_votes3`), resulting in a tick plot with color encoding \"IMDB Votes\" instead of size. The final query requests \"IMDB Votes\" to be placed on the size channel (`plus_votes3.field('IMDB_Votes', channel='size')`), resulting in a bubble plot (unbinned) with \"IMDB Rating\" on the x channel, \"Major Genre\" on y, and \"IMDB Votes\" on size. On the right, the anchored path. A single anchored query requesting an additional \"IMDB Votes\" field be added to the original visualization (`genre_vs_rating.anchor().field('IMDB_Votes')`. The result is a bubble plot identical to the final visualization in the cold query lineage.", "tokens": ["Figure", "7", ":", "An", "origin", "chart", "and", "follow-up", "authoring", "paths", ".", "The", "origin", "plot", "is", "a", "tick", "plot", ",", "displaying", "``", "IMDB", "Rating", "''", "on", "the", "x", "axis", "and", "``", "Major", "Genre", "''", "on", "the", "y", "axis", "(", "`", "genre_vs_rating", "=", "Chart", "(", "movies", ")", ".field", "(", "'IMDB_Rating", "'", ",", "'Major_Genre", "'", ")", "`", "then", "a", "newline", "and", "`", "genre_vs_rating", "`", ")", ".", "On", "the", "left", ",", "the", "cold", "path", ".", "The", "first", "query", "adds", "the", "``", "IMDB", "Votes", "''", "field", "(", "`", "plus_votes", "=", "genre_vs_ratings.field", "(", "'IMDB_Votes", "'", ")", "`", "then", "a", "newline", "and", "`", "plus_votes", "`", ")", ",", "resulting", "in", "a", "binned", "bubble", "plot", "with", "``", "IMDB", "Votes", "''", "replacing", "``", "IMDB", "Rating", "''", "on", "the", "x", "axis", ",", "and", "``", "IMDB", "Rating", "''", "moving", "to", "the", "size", "channel", ".", "The", "second", "query", "requests", "``", "IMDB", "Rating", "''", "back", "on", "the", "x", "channel", "(", "`", "plus_votes2", "=", "plus_votes.field", "(", "'IMDB_Rating", "'", ",", "channel=", "'", "x", "'", ")", "`", ",", "newline", "then", "`", "plus_votes2", "`", ")", ".", "The", "result", "is", "a", "bubble", "plot", "with", "``", "IMDB", "Rating", "''", "and", "``", "IMDB", "Votes", "''", "switching", "channels", ".", "The", "third", "query", "requests", "``", "IMDB", "Rating", "''", "to", "be", "unbinned", "(", "`", "plus_votes3", "=", "plus_votes2.field", "(", "'IMDB_Rating", "'", ",", "bin=False", ")", "`", ",", "newline", "then", "`", "plus_votes3", "`", ")", ",", "resulting", "in", "a", "tick", "plot", "with", "color", "encoding", "``", "IMDB", "Votes", "''", "instead", "of", "size", ".", "The", "final", "query", "requests", "``", "IMDB", "Votes", "''", "to", "be", "placed", "on", "the", "size", "channel", "(", "`", "plus_votes3.field", "(", "'IMDB_Votes", "'", ",", "channel='size", "'", ")", "`", ")", ",", "resulting", "in", "a", "bubble", "plot", "(", "unbinned", ")", "with", "``", "IMDB", "Rating", "''", "on", "the", "x", "channel", ",", "``", "Major", "Genre", "''", "on", "y", ",", "and", "``", "IMDB", "Votes", "''", "on", "size", ".", "On", "the", "right", ",", "the", "anchored", "path", ".", "A", "single", "anchored", "query", "requesting", "an", "additional", "``", "IMDB", "Votes", "''", "field", "be", "added", "to", "the", "original", "visualization", "(", "`", "genre_vs_rating.anchor", "(", ")", ".field", "(", "'IMDB_Votes", "'", ")", "`", ".", "The", "result", "is", "a", "bubble", "plot", "identical", "to", "the", "final", "visualization", "in", "the", "cold", "query", "lineage", "."]}, "caption": {"raw": "Figure 7. Trying to coerce cold recommendations towards a speciﬁc goal can result in a frustrating experience in which effectiveness is optimized at the expense of consistency. In this example, an anchored recommenda- tion maintains the unaggregated view of movies plotted by their IMDB Rating and Genre. On the other hand, a cold recommendation initially swaps channel assignments (removing the original visualized relation- ship) and aggregates ﬁelds (removing sample awareness). Cold queries that attempt to correct these changes can result in further deviations (in this case, changing of mark and channels) that require further adjust- ments. Anchored recommendations ease this process by reducing the number of changes made to the prior.", "tokens": ["Figure", "7", ".", "Trying", "to", "coerce", "cold", "recommendations", "towards", "a", "speciﬁc", "goal", "can", "result", "in", "a", "frustrating", "experience", "in", "which", "effectiveness", "is", "optimized", "at", "the", "expense", "of", "consistency", ".", "In", "this", "example", ",", "an", "anchored", "recommenda-", "tion", "maintains", "the", "unaggregated", "view", "of", "movies", "plotted", "by", "their", "IMDB", "Rating", "and", "Genre", ".", "On", "the", "other", "hand", ",", "a", "cold", "recommendation", "initially", "swaps", "channel", "assignments", "(", "removing", "the", "original", "visualized", "relation-", "ship", ")", "and", "aggregates", "ﬁelds", "(", "removing", "sample", "awareness", ")", ".", "Cold", "queries", "that", "attempt", "to", "correct", "these", "changes", "can", "result", "in", "further", "deviations", "(", "in", "this", "case", ",", "changing", "of", "mark", "and", "channels", ")", "that", "require", "further", "adjust-", "ments", ".", "Anchored", "recommendations", "ease", "this", "process", "by", "reducing", "the", "number", "of", "changes", "made", "to", "the", "prior", "."]}, "context": {"raw": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations Figure 7. Trying to coerce cold recommendations towards a speciﬁc goal can result in a frustrating experience in which effectiveness is optimized at the expense of consistency. In this example, an anchored recommenda- tion maintains the unaggregated view of movies plotted by their IMDB Rating and Genre. On the other hand, a cold recommendation initially swaps channel assignments (removing the original visualized relation- ship) and aggregates ﬁelds (removing sample awareness). Cold queries that attempt to correct these changes can result in further deviations (in this case, changing of mark and channels) that require further adjust- ments. Anchored recommendations ease this process by reducing the number of changes made to the prior.", "tokens": ["Dziban", ":", "Balancing", "Agency", "&", "Automation", "in", "Visualization", "Design", "via", "Anchored", "Recommendations", "Figure", "7", ".", "Trying", "to", "coerce", "cold", "recommendations", "towards", "a", "speciﬁc", "goal", "can", "result", "in", "a", "frustrating", "experience", "in", "which", "effectiveness", "is", "optimized", "at", "the", "expense", "of", "consistency", ".", "In", "this", "example", ",", "an", "anchored", "recommenda-", "tion", "maintains", "the", "unaggregated", "view", "of", "movies", "plotted", "by", "their", "IMDB", "Rating", "and", "Genre", ".", "On", "the", "other", "hand", ",", "a", "cold", "recommendation", "initially", "swaps", "channel", "assignments", "(", "removing", "the", "original", "visualized", "relation-", "ship", ")", "and", "aggregates", "ﬁelds", "(", "removing", "sample", "awareness", ")", ".", "Cold", "queries", "that", "attempt", "to", "correct", "these", "changes", "can", "result", "in", "further", "deviations", "(", "in", "this", "case", ",", "changing", "of", "mark", "and", "channels", ")", "that", "require", "further", "adjust-", "ments", ".", "Anchored", "recommendations", "ease", "this", "process", "by", "reducing", "the", "number", "of", "changes", "made", "to", "the", "prior", "."]}, "filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_032.jpg", "orig_filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "split": "train"}, {"article_id": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment", "description": {"raw": "Fig7.  The distribution of localization accuracy in meters. All the data is the following in a csv format.  Error in meters,Total,Success,Fail  0-0.5,16,15,1  0.5-1.0,87,79,8  1.0-1.5,61,56,5  1.5-2.0,55,46,9  2.0-2.5,18,14,4  2.5-3.0,12,8,4  3.0-3.5,3,2,1  3.5-4.0,1,0,1  5.5-6.0,1,1,0", "tokens": ["Fig7", ".", "The", "distribution", "of", "localization", "accuracy", "in", "meters", ".", "All", "the", "data", "is", "the", "following", "in", "a", "csv", "format", ".", "Error", "in", "meters", ",", "Total", ",", "Success", ",", "Fail", "0-0.5,16,15,1", "0.5-1.0,87,79,8", "1.0-1.5,61,56,5", "1.5-2.0,55,46,9", "2.0-2.5,18,14,4", "2.5-3.0,12,8,4", "3.0-3.5,3,2,1", "3.5-4.0,1,0,1", "5.5-6.0,1,1,0"]}, "caption": {"raw": "Figure 7. The distribution of localization accuracy in meters (N", "tokens": ["Figure", "7", ".", "The", "distribution", "of", "localization", "accuracy", "in", "meters", "(", "N"]}, "context": {"raw": "NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment Figure 7. The distribution of localization accuracy in meters (N", "tokens": ["NavCog3", ":", "An", "Evaluation", "of", "a", "Smartphone-Based", "Blind", "Indoor", "Navigation", "Assistant", "with", "Semantic", "Features", "in", "a", "Large-Scale", "Environment", "Figure", "7", ".", "The", "distribution", "of", "localization", "accuracy", "in", "meters", "(", "N"]}, "filename": "b535ecced835fbcfffaa15a78abfa49f44709daa_Image_012.jpg", "orig_filename": "b535ecced835fbcfffaa15a78abfa49f44709daa", "split": "train"}, {"article_id": "Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects", "description": {"raw": "Comparison of touch / no touch for varying overlay thicknesses of PLA.  A graph shows the asymptotic dependency between increasing overlay thicknesses (from 0 to 20 mm) and the Signal-to-noise ratio. For thicknesses up to 10 mm the Signal-to-noise ratio drastically decreases with increasing thickness (resulting in a high gradient), but lies above a minimal value of 5, indicating robust measurements. For higher thicknesses the gradient asymptotically decreases.  Furthermore, a table lists the measured average capacitance readings when touched and not touched by a finger (including standard deviations), and the respective Signal-to-noise ratio for varying overlay thicknesses of 0.25, 0.5, 1, 2, 10, and 15 mm.", "tokens": ["Comparison", "of", "touch", "/", "no", "touch", "for", "varying", "overlay", "thicknesses", "of", "PLA", ".", "A", "graph", "shows", "the", "asymptotic", "dependency", "between", "increasing", "overlay", "thicknesses", "(", "from", "0", "to", "20", "mm", ")", "and", "the", "Signal-to-noise", "ratio", ".", "For", "thicknesses", "up", "to", "10", "mm", "the", "Signal-to-noise", "ratio", "drastically", "decreases", "with", "increasing", "thickness", "(", "resulting", "in", "a", "high", "gradient", ")", ",", "but", "lies", "above", "a", "minimal", "value", "of", "5", ",", "indicating", "robust", "measurements", ".", "For", "higher", "thicknesses", "the", "gradient", "asymptotically", "decreases", ".", "Furthermore", ",", "a", "table", "lists", "the", "measured", "average", "capacitance", "readings", "when", "touched", "and", "not", "touched", "by", "a", "finger", "(", "including", "standard", "deviations", ")", ",", "and", "the", "respective", "Signal-to-noise", "ratio", "for", "varying", "overlay", "thicknesses", "of", "0.25", ",", "0.5", ",", "1", ",", "2", ",", "10", ",", "and", "15", "mm", "."]}, "caption": {"raw": "= 194.799, p < 0.001). For thicker overlays, the SNR falls below a minimal SNR of 5:1, which is generally considered as the lower bound for robust touch detection. Therefore, ﬂat subsurface touch electrodes can be placed at most 10 mm underneath the outermost point on the surface; further the maximum height difference (along the surface normal of the touch electrodes) between any two points on the surface cannot exceed 10 mm. If these requirements cannot be met, the sensor should be implemented using the curved surface touch electrodes technique.", "tokens": ["=", "194.799", ",", "p", "<", "0.001", ")", ".", "For", "thicker", "overlays", ",", "the", "SNR", "falls", "below", "a", "minimal", "SNR", "of", "5:1", ",", "which", "is", "generally", "considered", "as", "the", "lower", "bound", "for", "robust", "touch", "detection", ".", "Therefore", ",", "ﬂat", "subsurface", "touch", "electrodes", "can", "be", "placed", "at", "most", "10", "mm", "underneath", "the", "outermost", "point", "on", "the", "surface", ";", "further", "the", "maximum", "height", "difference", "(", "along", "the", "surface", "normal", "of", "the", "touch", "electrodes", ")", "between", "any", "two", "points", "on", "the", "surface", "can", "not", "exceed", "10", "mm", ".", "If", "these", "requirements", "can", "not", "be", "met", ",", "the", "sensor", "should", "be", "implemented", "using", "the", "curved", "surface", "touch", "electrodes", "technique", "."]}, "context": {"raw": "Capricate: A Fabrication Pipeline to Design and 3D Print Capacitive Touch Sensors for Interactive Objects = 194.799, p < 0.001). For thicker overlays, the SNR falls below a minimal SNR of 5:1, which is generally considered as the lower bound for robust touch detection. Therefore, ﬂat subsurface touch electrodes can be placed at most 10 mm underneath the outermost point on the surface; further the maximum height difference (along the surface normal of the touch electrodes) between any two points on the surface cannot exceed 10 mm. If these requirements cannot be met, the sensor should be implemented using the curved surface touch electrodes technique.", "tokens": ["Capricate", ":", "A", "Fabrication", "Pipeline", "to", "Design", "and", "3D", "Print", "Capacitive", "Touch", "Sensors", "for", "Interactive", "Objects", "=", "194.799", ",", "p", "<", "0.001", ")", ".", "For", "thicker", "overlays", ",", "the", "SNR", "falls", "below", "a", "minimal", "SNR", "of", "5:1", ",", "which", "is", "generally", "considered", "as", "the", "lower", "bound", "for", "robust", "touch", "detection", ".", "Therefore", ",", "ﬂat", "subsurface", "touch", "electrodes", "can", "be", "placed", "at", "most", "10", "mm", "underneath", "the", "outermost", "point", "on", "the", "surface", ";", "further", "the", "maximum", "height", "difference", "(", "along", "the", "surface", "normal", "of", "the", "touch", "electrodes", ")", "between", "any", "two", "points", "on", "the", "surface", "can", "not", "exceed", "10", "mm", ".", "If", "these", "requirements", "can", "not", "be", "met", ",", "the", "sensor", "should", "be", "implemented", "using", "the", "curved", "surface", "touch", "electrodes", "technique", "."]}, "filename": "1480dab32d9676b8a3db7946eccd7c8fc172273d_Image_006.jpg", "orig_filename": "1480dab32d9676b8a3db7946eccd7c8fc172273d", "split": "train"}, {"article_id": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code", "description": {"raw": "This is a bar chart of the mean score for the tasks.    With Keywords: Without Tool 2, with tool 2.42  Without Keywords: Without Tool 1.57, with tool 1.57  Conditions: Without Tool 2, with tool 2.7  All: Without Tool 1.9, with tool 2.2", "tokens": ["This", "is", "a", "bar", "chart", "of", "the", "mean", "score", "for", "the", "tasks", ".", "With", "Keywords", ":", "Without", "Tool", "2", ",", "with", "tool", "2.42", "Without", "Keywords", ":", "Without", "Tool", "1.57", ",", "with", "tool", "1.57", "Conditions", ":", "Without", "Tool", "2", ",", "with", "tool", "2.7", "All", ":", "Without", "Tool", "1.9", ",", "with", "tool", "2.2"]}, "caption": {"raw": "Figure 6. This chart shows the average score for all par- ticipants on the three tasks broken down by task. The bars represent the standard error.", "tokens": ["Figure", "6", ".", "This", "chart", "shows", "the", "average", "score", "for", "all", "par-", "ticipants", "on", "the", "three", "tasks", "broken", "down", "by", "task", ".", "The", "bars", "represent", "the", "standard", "error", "."]}, "context": {"raw": "StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code Figure 6. This chart shows the average score for all par- ticipants on the three tasks broken down by task. The bars represent the standard error.", "tokens": ["StructJumper", ":", "A", "Tool", "to", "Help", "Blind", "Programmers", "Navigate", "and", "Understand", "the", "Structure", "of", "Code", "Figure", "6", ".", "This", "chart", "shows", "the", "average", "score", "for", "all", "par-", "ticipants", "on", "the", "three", "tasks", "broken", "down", "by", "task", ".", "The", "bars", "represent", "the", "standard", "error", "."]}, "filename": "13eccfcfd4ce807e14da418115a386b918870582_Image_006.jpg", "orig_filename": "13eccfcfd4ce807e14da418115a386b918870582", "split": "train"}, {"article_id": "Examining Menstrual Tracking to Inform the Design of Personal Informatics Tools", "description": {"raw": "The Period Tracker app asks for a detault cycle length. It says, \"until period tracker has three months of user inputted data, it will use a default period length to calculate future period start dates\".", "tokens": ["The", "Period", "Tracker", "app", "asks", "for", "a", "detault", "cycle", "length", ".", "It", "says", ",", "``", "until", "period", "tracker", "has", "three", "months", "of", "user", "inputted", "data", ",", "it", "will", "use", "a", "default", "period", "length", "to", "calculate", "future", "period", "start", "dates", "''", "."]}, "caption": {"raw": "(a)(b)", "tokens": ["(", "a", ")", "(", "b", ")"]}, "context": {"raw": "Examining Menstrual Tracking to Inform the Design of Personal Informatics Tools (a)(b)", "tokens": ["Examining", "Menstrual", "Tracking", "to", "Inform", "the", "Design", "of", "Personal", "Informatics", "Tools", "(", "a", ")", "(", "b", ")"]}, "filename": "ecc022fa1266d35a0cbf171cf501db7fa495bb99_Image_004.jpg", "orig_filename": "ecc022fa1266d35a0cbf171cf501db7fa495bb99", "split": "train"}, {"article_id": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation", "description": {"raw": "The left bar chart is showing reasons participants reported for why delete indicators might stop them from using the delete function. The chart is detailed in section 4.3 and shows most reported reason was to avoid negative assumptions, whilst fewest reported the reason was to avoid showing vulnerability.  The right bar chart is showing reasons participants reported for why delete indicators would not stop them from using the delete function. The chart is detailed in section 4.3.2 and shows most reported reason was the cost of it being read.", "tokens": ["The", "left", "bar", "chart", "is", "showing", "reasons", "participants", "reported", "for", "why", "delete", "indicators", "might", "stop", "them", "from", "using", "the", "delete", "function", ".", "The", "chart", "is", "detailed", "in", "section", "4.3", "and", "shows", "most", "reported", "reason", "was", "to", "avoid", "negative", "assumptions", ",", "whilst", "fewest", "reported", "the", "reason", "was", "to", "avoid", "showing", "vulnerability", ".", "The", "right", "bar", "chart", "is", "showing", "reasons", "participants", "reported", "for", "why", "delete", "indicators", "would", "not", "stop", "them", "from", "using", "the", "delete", "function", ".", "The", "chart", "is", "detailed", "in", "section", "4.3.2", "and", "shows", "most", "reported", "reason", "was", "the", "cost", "of", "it", "being", "read", "."]}, "caption": {"raw": "Figure 6: Reasons participants reported for why delete in- dicators would not stop them from using the deletion func- tion.", "tokens": ["Figure", "6", ":", "Reasons", "participants", "reported", "for", "why", "delete", "in-", "dicators", "would", "not", "stop", "them", "from", "using", "the", "deletion", "func-", "tion", "."]}, "context": {"raw": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation Figure 6: Reasons participants reported for why delete in- dicators would not stop them from using the deletion func- tion.", "tokens": ["“", "Oops", "...", "”", ":", "Mobile", "Message", "Deletion", "in", "Conversation", "Error", "and", "Regret", "Remediation", "Figure", "6", ":", "Reasons", "participants", "reported", "for", "why", "delete", "in-", "dicators", "would", "not", "stop", "them", "from", "using", "the", "deletion", "func-", "tion", "."]}, "filename": "fa7c681ffd0ea94a482755623a107fce48740b0d_Image_006.jpg", "orig_filename": "fa7c681ffd0ea94a482755623a107fce48740b0d", "split": "train"}, {"article_id": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "description": {"raw": "Figure 4c Single Daily View containing one day of data. Sleep is represented as a ring and a bar. The bar contains bedtime, waketime, and movement during the night.", "tokens": ["Figure", "4c", "Single", "Daily", "View", "containing", "one", "day", "of", "data", ".", "Sleep", "is", "represented", "as", "a", "ring", "and", "a", "bar", ".", "The", "bar", "contains", "bedtime", ",", "waketime", ",", "and", "movement", "during", "the", "night", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together ", "tokens": ["DreamCatcher", ":", "Exploring", "How", "Parents", "and", "School-Age", "Children", "can", "Track", "and", "Review", "Sleep", "Information", "Together"]}, "filename": "373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_009.jpg", "orig_filename": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "split": "train"}, {"article_id": "ASSETS: G: BrailleBlocks: Braille Toys for Cross-Ability Collaboration", "description": {"raw": "Two screens from the BrailleBlocks companion application. On the top is the selection chart from the Animal Name Game. The chart shows a cartoon dog, elephant, duck, and sheep. On the bottom is a level 1 of the Word Scramble Game. The screen shows instructions on how to play the game. It also shows a visual Braille representation of the letters \"t\", \"c\", and \"a\". Below the Braille are a list of words that the letters can unscramble to make.", "tokens": ["Two", "screens", "from", "the", "BrailleBlocks", "companion", "application", ".", "On", "the", "top", "is", "the", "selection", "chart", "from", "the", "Animal", "Name", "Game", ".", "The", "chart", "shows", "a", "cartoon", "dog", ",", "elephant", ",", "duck", ",", "and", "sheep", ".", "On", "the", "bottom", "is", "a", "level", "1", "of", "the", "Word", "Scramble", "Game", ".", "The", "screen", "shows", "instructions", "on", "how", "to", "play", "the", "game", ".", "It", "also", "shows", "a", "visual", "Braille", "representation", "of", "the", "letters", "``", "t", "''", ",", "``", "c", "''", ",", "and", "``", "a", "''", ".", "Below", "the", "Braille", "are", "a", "list", "of", "words", "that", "the", "letters", "can", "unscramble", "to", "make", "."]}, "caption": {"raw": "Figure 3 (top). The Animal Name Game selection page. Figure 4 (bottom). Level 1 of the Word Scramble game.", "tokens": ["Figure", "3", "(", "top", ")", ".", "The", "Animal", "Name", "Game", "selection", "page", ".", "Figure", "4", "(", "bottom", ")", ".", "Level", "1", "of", "the", "Word", "Scramble", "game", "."]}, "context": {"raw": "ASSETS: G: BrailleBlocks: Braille Toys for Cross-Ability Collaboration Figure 3 (top). The Animal Name Game selection page. Figure 4 (bottom). Level 1 of the Word Scramble game.", "tokens": ["ASSETS", ":", "G", ":", "BrailleBlocks", ":", "Braille", "Toys", "for", "Cross-Ability", "Collaboration", "Figure", "3", "(", "top", ")", ".", "The", "Animal", "Name", "Game", "selection", "page", ".", "Figure", "4", "(", "bottom", ")", ".", "Level", "1", "of", "the", "Word", "Scramble", "game", "."]}, "filename": "ac34c667f089ba776f496042d5f5a3aab8effa8c_Image_003.jpg", "orig_filename": "ac34c667f089ba776f496042d5f5a3aab8effa8c", "split": "train"}, {"article_id": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation", "description": {"raw": "A line plot showing the progression of training at each epoch. The     accuracy increases until 237 then plateaus.", "tokens": ["A", "line", "plot", "showing", "the", "progression", "of", "training", "at", "each", "epoch", ".", "The", "accuracy", "increases", "until", "237", "then", "plateaus", "."]}, "caption": {"raw": "Figure 8: Validation accuracy of our model at every epoch of training. The accuracy was computed using the held-out val- idation data. The network corresponding to the 237th epoch was selected as our classifer.‌", "tokens": ["Figure", "8", ":", "Validation", "accuracy", "of", "our", "model", "at", "every", "epoch", "of", "training", ".", "The", "accuracy", "was", "computed", "using", "the", "held-out", "val-", "idation", "data", ".", "The", "network", "corresponding", "to", "the", "237th", "epoch", "was", "selected", "as", "our", "classifer.‌"]}, "context": {"raw": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation Figure 8: Validation accuracy of our model at every epoch of training. The accuracy was computed using the held-out val- idation data. The network corresponding to the 237th epoch was selected as our classifer.‌", "tokens": ["Lost", "in", "Style", ":", "Gaze-driven", "Adaptive", "Aid", "for", "VR", "Navigation", "Figure", "8", ":", "Validation", "accuracy", "of", "our", "model", "at", "every", "epoch", "of", "training", ".", "The", "accuracy", "was", "computed", "using", "the", "held-out", "val-", "idation", "data", ".", "The", "network", "corresponding", "to", "the", "237th", "epoch", "was", "selected", "as", "our", "classifer.‌"]}, "filename": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_011.jpg", "orig_filename": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab", "split": "train"}, {"article_id": "Tactile graphics with a voice demonstration", "description": {"raw": "This is an image of the system in use. It shows a tactile graphics of a bar chart with QR code labels. There is a user scanning a QR code using finger pointing mode.", "tokens": ["This", "is", "an", "image", "of", "the", "system", "in", "use", ".", "It", "shows", "a", "tactile", "graphics", "of", "a", "bar", "chart", "with", "QR", "code", "labels", ".", "There", "is", "a", "user", "scanning", "a", "QR", "code", "using", "finger", "pointing", "mode", "."]}, "caption": {"raw": "{cmbaker, milnel2, jeffsco, bennec3, ladner}@cs.washington.edu", "tokens": ["{", "cmbaker", ",", "milnel2", ",", "jeffsco", ",", "bennec3", ",", "ladner", "}", "@", "cs.washington.edu"]}, "context": {"raw": "Tactile graphics with a voice demonstration {cmbaker, milnel2, jeffsco, bennec3, ladner}@cs.washington.edu", "tokens": ["Tactile", "graphics", "with", "a", "voice", "demonstration", "{", "cmbaker", ",", "milnel2", ",", "jeffsco", ",", "bennec3", ",", "ladner", "}", "@", "cs.washington.edu"]}, "filename": "99046873563a36fe02f26eee252b994884eae7a1_Image_001.jpg", "orig_filename": "99046873563a36fe02f26eee252b994884eae7a1", "split": "train"}, {"article_id": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users", "description": {"raw": "Figure 3: Desired information for app notifications. Pairs of bars represent deaf and hard-of-hearing (HH) participants. The following types of information are included: identity, location, volume, length, pitch, urgency, and confidence. Possible participant responses were: absolutely essential, very important, moderately important, of little importance, and not important at all. A stacked bar chart shows user responses for each type of information. Most participants responded that identity, location, urgency, and confidence were at least moderately important, while fewer participants gave volume, length, and pitch the same importance. Deaf and hard-of-hearing responses were very similar.", "tokens": ["Figure", "3", ":", "Desired", "information", "for", "app", "notifications", ".", "Pairs", "of", "bars", "represent", "deaf", "and", "hard-of-hearing", "(", "HH", ")", "participants", ".", "The", "following", "types", "of", "information", "are", "included", ":", "identity", ",", "location", ",", "volume", ",", "length", ",", "pitch", ",", "urgency", ",", "and", "confidence", ".", "Possible", "participant", "responses", "were", ":", "absolutely", "essential", ",", "very", "important", ",", "moderately", "important", ",", "of", "little", "importance", ",", "and", "not", "important", "at", "all", ".", "A", "stacked", "bar", "chart", "shows", "user", "responses", "for", "each", "type", "of", "information", ".", "Most", "participants", "responded", "that", "identity", ",", "location", ",", "urgency", ",", "and", "confidence", "were", "at", "least", "moderately", "important", ",", "while", "fewer", "participants", "gave", "volume", ",", "length", ",", "and", "pitch", "the", "same", "importance", ".", "Deaf", "and", "hard-of-hearing", "responses", "were", "very", "similar", "."]}, "caption": {"raw": "Figure 3: Desired information for app notiﬁcations. Pairs of bars represent deaf and hard-of-hearing (HH) participants.", "tokens": ["Figure", "3", ":", "Desired", "information", "for", "app", "notiﬁcations", ".", "Pairs", "of", "bars", "represent", "deaf", "and", "hard-of-hearing", "(", "HH", ")", "participants", "."]}, "context": {"raw": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users Figure 3: Desired information for app notiﬁcations. Pairs of bars represent deaf and hard-of-hearing (HH) participants.", "tokens": ["A", "Personalizable", "Mobile", "Sound", "Detector", "App", "Design", "for", "Deaf", "and", "Hard-of-Hearing", "Users", "Figure", "3", ":", "Desired", "information", "for", "app", "notiﬁcations", ".", "Pairs", "of", "bars", "represent", "deaf", "and", "hard-of-hearing", "(", "HH", ")", "participants", "."]}, "filename": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77_Image_005.jpg", "orig_filename": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77", "split": "train"}, {"article_id": "No Touch Pig!: Investigating Child-Parent Use of a System for Training Executive Function", "description": {"raw": "Left: Mockup of app screen. There is a timer bar at the top, completely full. Below the bar, there are three piles of dirt with three sprouts of grass growing out of them. Underneath this picture, it says \"Don't touch the grass; let it grow!\" Middle: Mockup of app screen. There is a timer bar at the top, a little more than halfway full. Below the bar, there are three piles of dirt with three plants growing out of them. There is a watering can on the side, with water coming out. Underneath this picture, it says \"Water and wait for the plants to grow!\" Right: Mockup of app screen. There is a timer bar at the top, almost empty. Below the bar, there are three trees with acorns on them. Underneath this picture, it says \"You waited, and the plants grew into trees! Now you can collect acorns.\"", "tokens": ["Left", ":", "Mockup", "of", "app", "screen", ".", "There", "is", "a", "timer", "bar", "at", "the", "top", ",", "completely", "full", ".", "Below", "the", "bar", ",", "there", "are", "three", "piles", "of", "dirt", "with", "three", "sprouts", "of", "grass", "growing", "out", "of", "them", ".", "Underneath", "this", "picture", ",", "it", "says", "``", "Do", "n't", "touch", "the", "grass", ";", "let", "it", "grow", "!", "''", "Middle", ":", "Mockup", "of", "app", "screen", ".", "There", "is", "a", "timer", "bar", "at", "the", "top", ",", "a", "little", "more", "than", "halfway", "full", ".", "Below", "the", "bar", ",", "there", "are", "three", "piles", "of", "dirt", "with", "three", "plants", "growing", "out", "of", "them", ".", "There", "is", "a", "watering", "can", "on", "the", "side", ",", "with", "water", "coming", "out", ".", "Underneath", "this", "picture", ",", "it", "says", "``", "Water", "and", "wait", "for", "the", "plants", "to", "grow", "!", "''", "Right", ":", "Mockup", "of", "app", "screen", ".", "There", "is", "a", "timer", "bar", "at", "the", "top", ",", "almost", "empty", ".", "Below", "the", "bar", ",", "there", "are", "three", "trees", "with", "acorns", "on", "them", ".", "Underneath", "this", "picture", ",", "it", "says", "``", "You", "waited", ",", "and", "the", "plants", "grew", "into", "trees", "!", "Now", "you", "can", "collect", "acorns", ".", "''"]}, "caption": {"raw": "they might do so more efectively with more guidance. It would be useful to explore whether the system can scafold parents’ scafolding [73] and guide parents toward support- ing children productively.", "tokens": ["they", "might", "do", "so", "more", "efectively", "with", "more", "guidance", ".", "It", "would", "be", "useful", "to", "explore", "whether", "the", "system", "can", "scafold", "parents", "’", "scafolding", "[", "73", "]", "and", "guide", "parents", "toward", "support-", "ing", "children", "productively", "."]}, "context": {"raw": "No Touch Pig!: Investigating Child-Parent Use of a System for Training Executive Function they might do so more efectively with more guidance. It would be useful to explore whether the system can scafold parents’ scafolding [73] and guide parents toward support- ing children productively.", "tokens": ["No", "Touch", "Pig", "!", ":", "Investigating", "Child-Parent", "Use", "of", "a", "System", "for", "Training", "Executive", "Function", "they", "might", "do", "so", "more", "efectively", "with", "more", "guidance", ".", "It", "would", "be", "useful", "to", "explore", "whether", "the", "system", "can", "scafold", "parents", "’", "scafolding", "[", "73", "]", "and", "guide", "parents", "toward", "support-", "ing", "children", "productively", "."]}, "filename": "beec9b27e310c14ccf0235582c11ce4aa9e4214c_Image_007.jpg", "orig_filename": "beec9b27e310c14ccf0235582c11ce4aa9e4214c", "split": "train"}, {"article_id": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users", "description": {"raw": "Figure 1: Three bar charts presenting the percentage of participants who are interested in knowing about particular sounds (a) at home, (b) at work, and (c) while mobile. Each chart contains pairs of bars: one representing deaf participants, and the other hard-of-hearing participants.  (a) A bar chart of the percentage of participants interested in particular sounds at home. The sounds, sorted in decreasing order of deaf participants are: appliances running, emergency alarms, appliance alerts, intruders, knocking on door, doorbell, sounds outside of the house, wake-up alarms, people knocking things over, dog barking, phone ringing, baby crying, people shouting, people laughing, children fighting, children playing, and other. The percent of deaf participants who selected each sound decreases consistently from about 70% to about 10%. Hard-of-hearing participants followed a similar trend, but about 15% more selected knocking on door, wake-up alarms, and phone ringing than deaf participants. (b) A bar chart of the percentage of participants interested in particular sounds at work. The sounds, sorted in decreasing order of deaf participants are: emergency alarms, presence of co-workers, co-workers calling attention, surrounding conversations, knocking on door, announcements, co-worker activity, gun shots, phone ringing, other, and faxes. Deaf percentages decreased from around 50% to 10%. Hard-of-hearing participants followed a similar trend, with about 20% more interest in co-workers calling attention, surrounding conversations, announcements, and phone ringing. (c) A bar chart of the percentage of participants interested in particular sounds while mobile. The sounds, sorted in decreasing order of deaf participants are: sirens, bikes or people behind, if you are in the way, honking, announcements, vehicles driving by, sounds in nature, dogs barking, airplanes or helicopters, and other. Deaf percentages decrease from about 70% to 5%.", "tokens": ["Figure", "1", ":", "Three", "bar", "charts", "presenting", "the", "percentage", "of", "participants", "who", "are", "interested", "in", "knowing", "about", "particular", "sounds", "(", "a", ")", "at", "home", ",", "(", "b", ")", "at", "work", ",", "and", "(", "c", ")", "while", "mobile", ".", "Each", "chart", "contains", "pairs", "of", "bars", ":", "one", "representing", "deaf", "participants", ",", "and", "the", "other", "hard-of-hearing", "participants", ".", "(", "a", ")", "A", "bar", "chart", "of", "the", "percentage", "of", "participants", "interested", "in", "particular", "sounds", "at", "home", ".", "The", "sounds", ",", "sorted", "in", "decreasing", "order", "of", "deaf", "participants", "are", ":", "appliances", "running", ",", "emergency", "alarms", ",", "appliance", "alerts", ",", "intruders", ",", "knocking", "on", "door", ",", "doorbell", ",", "sounds", "outside", "of", "the", "house", ",", "wake-up", "alarms", ",", "people", "knocking", "things", "over", ",", "dog", "barking", ",", "phone", "ringing", ",", "baby", "crying", ",", "people", "shouting", ",", "people", "laughing", ",", "children", "fighting", ",", "children", "playing", ",", "and", "other", ".", "The", "percent", "of", "deaf", "participants", "who", "selected", "each", "sound", "decreases", "consistently", "from", "about", "70", "%", "to", "about", "10", "%", ".", "Hard-of-hearing", "participants", "followed", "a", "similar", "trend", ",", "but", "about", "15", "%", "more", "selected", "knocking", "on", "door", ",", "wake-up", "alarms", ",", "and", "phone", "ringing", "than", "deaf", "participants", ".", "(", "b", ")", "A", "bar", "chart", "of", "the", "percentage", "of", "participants", "interested", "in", "particular", "sounds", "at", "work", ".", "The", "sounds", ",", "sorted", "in", "decreasing", "order", "of", "deaf", "participants", "are", ":", "emergency", "alarms", ",", "presence", "of", "co-workers", ",", "co-workers", "calling", "attention", ",", "surrounding", "conversations", ",", "knocking", "on", "door", ",", "announcements", ",", "co-worker", "activity", ",", "gun", "shots", ",", "phone", "ringing", ",", "other", ",", "and", "faxes", ".", "Deaf", "percentages", "decreased", "from", "around", "50", "%", "to", "10", "%", ".", "Hard-of-hearing", "participants", "followed", "a", "similar", "trend", ",", "with", "about", "20", "%", "more", "interest", "in", "co-workers", "calling", "attention", ",", "surrounding", "conversations", ",", "announcements", ",", "and", "phone", "ringing", ".", "(", "c", ")", "A", "bar", "chart", "of", "the", "percentage", "of", "participants", "interested", "in", "particular", "sounds", "while", "mobile", ".", "The", "sounds", ",", "sorted", "in", "decreasing", "order", "of", "deaf", "participants", "are", ":", "sirens", ",", "bikes", "or", "people", "behind", ",", "if", "you", "are", "in", "the", "way", ",", "honking", ",", "announcements", ",", "vehicles", "driving", "by", ",", "sounds", "in", "nature", ",", "dogs", "barking", ",", "airplanes", "or", "helicopters", ",", "and", "other", ".", "Deaf", "percentages", "decrease", "from", "about", "70", "%", "to", "5", "%", "."]}, "caption": {"raw": "Sounds of interest at home                        (b) Sounds of interest at work                      (c) Sounds of interest while mobileFigure 1: Sounds of interest to deaf and hard-of-hearing participants (a) at home, (b) at work, and (c) while mobile. (a) How often sounds missed at home           (b) How often sounds missed at work         (c) How often sounds missed while mobileFigure 2: Frequency of missed sounds (a) at home, (b) at work, and (c) while mobile.Various onset detection methods have been developed and used to boost audio event detection accuracy (e.g., [5]). We explored the performance of a sliding window GMM sound detection algorithm using training data gathered in our user study. The state-of-the-art in sound detection is advancing, and we expect future work applying these algorithms to yield more accurate results.", "tokens": ["Sounds", "of", "interest", "at", "home", "(", "b", ")", "Sounds", "of", "interest", "at", "work", "(", "c", ")", "Sounds", "of", "interest", "while", "mobileFigure", "1", ":", "Sounds", "of", "interest", "to", "deaf", "and", "hard-of-hearing", "participants", "(", "a", ")", "at", "home", ",", "(", "b", ")", "at", "work", ",", "and", "(", "c", ")", "while", "mobile", ".", "(", "a", ")", "How", "often", "sounds", "missed", "at", "home", "(", "b", ")", "How", "often", "sounds", "missed", "at", "work", "(", "c", ")", "How", "often", "sounds", "missed", "while", "mobileFigure", "2", ":", "Frequency", "of", "missed", "sounds", "(", "a", ")", "at", "home", ",", "(", "b", ")", "at", "work", ",", "and", "(", "c", ")", "while", "mobile.Various", "onset", "detection", "methods", "have", "been", "developed", "and", "used", "to", "boost", "audio", "event", "detection", "accuracy", "(", "e.g.", ",", "[", "5", "]", ")", ".", "We", "explored", "the", "performance", "of", "a", "sliding", "window", "GMM", "sound", "detection", "algorithm", "using", "training", "data", "gathered", "in", "our", "user", "study", ".", "The", "state-of-the-art", "in", "sound", "detection", "is", "advancing", ",", "and", "we", "expect", "future", "work", "applying", "these", "algorithms", "to", "yield", "more", "accurate", "results", "."]}, "context": {"raw": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users Sounds of interest at home                        (b) Sounds of interest at work                      (c) Sounds of interest while mobileFigure 1: Sounds of interest to deaf and hard-of-hearing participants (a) at home, (b) at work, and (c) while mobile. (a) How often sounds missed at home           (b) How often sounds missed at work         (c) How often sounds missed while mobileFigure 2: Frequency of missed sounds (a) at home, (b) at work, and (c) while mobile.Various onset detection methods have been developed and used to boost audio event detection accuracy (e.g., [5]). We explored the performance of a sliding window GMM sound detection algorithm using training data gathered in our user study. The state-of-the-art in sound detection is advancing, and we expect future work applying these algorithms to yield more accurate results.", "tokens": ["A", "Personalizable", "Mobile", "Sound", "Detector", "App", "Design", "for", "Deaf", "and", "Hard-of-Hearing", "Users", "Sounds", "of", "interest", "at", "home", "(", "b", ")", "Sounds", "of", "interest", "at", "work", "(", "c", ")", "Sounds", "of", "interest", "while", "mobileFigure", "1", ":", "Sounds", "of", "interest", "to", "deaf", "and", "hard-of-hearing", "participants", "(", "a", ")", "at", "home", ",", "(", "b", ")", "at", "work", ",", "and", "(", "c", ")", "while", "mobile", ".", "(", "a", ")", "How", "often", "sounds", "missed", "at", "home", "(", "b", ")", "How", "often", "sounds", "missed", "at", "work", "(", "c", ")", "How", "often", "sounds", "missed", "while", "mobileFigure", "2", ":", "Frequency", "of", "missed", "sounds", "(", "a", ")", "at", "home", ",", "(", "b", ")", "at", "work", ",", "and", "(", "c", ")", "while", "mobile.Various", "onset", "detection", "methods", "have", "been", "developed", "and", "used", "to", "boost", "audio", "event", "detection", "accuracy", "(", "e.g.", ",", "[", "5", "]", ")", ".", "We", "explored", "the", "performance", "of", "a", "sliding", "window", "GMM", "sound", "detection", "algorithm", "using", "training", "data", "gathered", "in", "our", "user", "study", ".", "The", "state-of-the-art", "in", "sound", "detection", "is", "advancing", ",", "and", "we", "expect", "future", "work", "applying", "these", "algorithms", "to", "yield", "more", "accurate", "results", "."]}, "filename": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77_Image_002.jpg", "orig_filename": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77", "split": "train"}, {"article_id": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "description": {"raw": "Bar chart with perceived pace. SlidePacer has an effect on perceived pace from approapriate to slightly slow.", "tokens": ["Bar", "chart", "with", "perceived", "pace", ".", "SlidePacer", "has", "an", "effect", "on", "perceived", "pace", "from", "approapriate", "to", "slightly", "slow", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students ", "tokens": ["SlidePacer", ":", "A", "Presentation", "Delivery", "Tool", "for", "Instructors", "of", "Deaf", "and", "Hard", "of", "Hearing", "Students"]}, "filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_009.png", "orig_filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "split": "train"}, {"article_id": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments", "description": {"raw": "It shows four screenshots of AIGuide. The top-left screenshot shows the Selection Interface, which contains a list with three items: Fruit Bar, Lipton Iced Tea and Lucky Charms. The top-right shows the guidance interface, which contains two labels. One indicates that the item was found. Other indicates that the item is two feet away, 15 degrees left and 5 inches below the camera view. Also, it has a guide, confirm , exit and restart button. The bottom-left shows the user settings, which contains a toggle switch for camera access, two toggle switches to control haptic and sound feedback, a scrolling bar for the speaking rate and a submenu for the measuring system. The bottom right shows the tutorial interface, which contains a page number at the top, the description of that explains how the localization phase works, a button to play a demo, and two buttons to switch page.", "tokens": ["It", "shows", "four", "screenshots", "of", "AIGuide", ".", "The", "top-left", "screenshot", "shows", "the", "Selection", "Interface", ",", "which", "contains", "a", "list", "with", "three", "items", ":", "Fruit", "Bar", ",", "Lipton", "Iced", "Tea", "and", "Lucky", "Charms", ".", "The", "top-right", "shows", "the", "guidance", "interface", ",", "which", "contains", "two", "labels", ".", "One", "indicates", "that", "the", "item", "was", "found", ".", "Other", "indicates", "that", "the", "item", "is", "two", "feet", "away", ",", "15", "degrees", "left", "and", "5", "inches", "below", "the", "camera", "view", ".", "Also", ",", "it", "has", "a", "guide", ",", "confirm", ",", "exit", "and", "restart", "button", ".", "The", "bottom-left", "shows", "the", "user", "settings", ",", "which", "contains", "a", "toggle", "switch", "for", "camera", "access", ",", "two", "toggle", "switches", "to", "control", "haptic", "and", "sound", "feedback", ",", "a", "scrolling", "bar", "for", "the", "speaking", "rate", "and", "a", "submenu", "for", "the", "measuring", "system", ".", "The", "bottom", "right", "shows", "the", "tutorial", "interface", ",", "which", "contains", "a", "page", "number", "at", "the", "top", ",", "the", "description", "of", "that", "explains", "how", "the", "localization", "phase", "works", ",", "a", "button", "to", "play", "a", "demo", ",", "and", "two", "buttons", "to", "switch", "page", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments ", "tokens": ["AIGuide", ":", "An", "Augmented", "Reality", "Hand", "Guidance", "Application", "for", "People", "with", "Visual", "Impairments"]}, "filename": "9475d822749fd4754e46de81e21b28445e4f98a7_Image_004.jpg", "orig_filename": "9475d822749fd4754e46de81e21b28445e4f98a7", "split": "train"}, {"article_id": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "description": {"raw": "Figure 6 - Line graph with iteration step on the X axis and cosine similarity on the Y axis, with one line for each of three intelligibility levels (30, 40, 50). Worker transcriptions tended to converge resulting in overall increasing cosine similarity with each iteration step.    Figure 7 - Line graph showing word error rate of worker transcriptions in a 10-step iterative workflow for the Alexa commands dataset. ASR transcription word error rate is constant across all 10 steps (at about 0.84) and average individual transcription word error rate is constant across all 10 steps (at about 0.4). Both are represented by horizontal lines. The iterative approach produced significantly lower word error rates than both other approaches, with a downwards trending line as the number of iteration steps increased.", "tokens": ["Figure", "6", "-", "Line", "graph", "with", "iteration", "step", "on", "the", "X", "axis", "and", "cosine", "similarity", "on", "the", "Y", "axis", ",", "with", "one", "line", "for", "each", "of", "three", "intelligibility", "levels", "(", "30", ",", "40", ",", "50", ")", ".", "Worker", "transcriptions", "tended", "to", "converge", "resulting", "in", "overall", "increasing", "cosine", "similarity", "with", "each", "iteration", "step", ".", "Figure", "7", "-", "Line", "graph", "showing", "word", "error", "rate", "of", "worker", "transcriptions", "in", "a", "10-step", "iterative", "workflow", "for", "the", "Alexa", "commands", "dataset", ".", "ASR", "transcription", "word", "error", "rate", "is", "constant", "across", "all", "10", "steps", "(", "at", "about", "0.84", ")", "and", "average", "individual", "transcription", "word", "error", "rate", "is", "constant", "across", "all", "10", "steps", "(", "at", "about", "0.4", ")", ".", "Both", "are", "represented", "by", "horizontal", "lines", ".", "The", "iterative", "approach", "produced", "significantly", "lower", "word", "error", "rates", "than", "both", "other", "approaches", ",", "with", "a", "downwards", "trending", "line", "as", "the", "number", "of", "iteration", "steps", "increased", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users ", "tokens": ["Towards", "More", "Robust", "Speech", "Interactions", "for", "Deaf", "and", "Hard", "of", "Hearing", "Users"]}, "filename": "8bba1845a85370618cd5c400ec8be42208554549_Image_010.jpg", "orig_filename": "8bba1845a85370618cd5c400ec8be42208554549", "split": "train"}, {"article_id": "Verge-it: Gaze Interaction for a Binocular Head-Worn Display using Modulated Disparity Vergence Eye Movement", "description": {"raw": "(a) There are two bar graph and two line graph. X-axis labeled as Phase and it is from 0 to 2 pi. Both barr graphs has similar heights across the x-axis and both line graph has peak at the 3/4 pi position and 15/8 pi position, and the peak at the 3/4 pi position is more higher than the peak at the 15/8 pi position  (b) cosine x graph and minus cosine x graph are ploated across the x-axis. from 0 to 2 pi", "tokens": ["(", "a", ")", "There", "are", "two", "bar", "graph", "and", "two", "line", "graph", ".", "X-axis", "labeled", "as", "Phase", "and", "it", "is", "from", "0", "to", "2", "pi", ".", "Both", "barr", "graphs", "has", "similar", "heights", "across", "the", "x-axis", "and", "both", "line", "graph", "has", "peak", "at", "the", "3/4", "pi", "position", "and", "15/8", "pi", "position", ",", "and", "the", "peak", "at", "the", "3/4", "pi", "position", "is", "more", "higher", "than", "the", "peak", "at", "the", "15/8", "pi", "position", "(", "b", ")", "cosine", "x", "graph", "and", "minus", "cosine", "x", "graph", "are", "ploated", "across", "the", "x-axis", ".", "from", "0", "to", "2", "pi"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Verge-it: Gaze Interaction for a Binocular Head-Worn Display using Modulated Disparity Vergence Eye Movement ", "tokens": ["Verge-it", ":", "Gaze", "Interaction", "for", "a", "Binocular", "Head-Worn", "Display", "using", "Modulated", "Disparity", "Vergence", "Eye", "Movement"]}, "filename": "4f2a57fd4fc6fac14d15cf73711d867f58277500_Image_009.jpg", "orig_filename": "4f2a57fd4fc6fac14d15cf73711d867f58277500", "split": "train"}, {"article_id": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping", "description": {"raw": "A bar chat showing perceived usefulness, perceived ease of use and user acceptance, it shows that magnification is less preferred. Overall rates are positive. Error bars show moderate stand deviation.", "tokens": ["A", "bar", "chat", "showing", "perceived", "usefulness", ",", "perceived", "ease", "of", "use", "and", "user", "acceptance", ",", "it", "shows", "that", "magnification", "is", "less", "preferred", ".", "Overall", "rates", "are", "positive", ".", "Error", "bars", "show", "moderate", "stand", "deviation", "."]}, "caption": {"raw": "Figure 5. Perceived usefulness, perceived ease of use and user acceptance show that magniﬁcation is less preferred.", "tokens": ["Figure", "5", ".", "Perceived", "usefulness", ",", "perceived", "ease", "of", "use", "and", "user", "acceptance", "show", "that", "magniﬁcation", "is", "less", "preferred", "."]}, "context": {"raw": "Enhancing Android accessibility for users with hand tremor by reducing fine pointing and steady tapping Figure 5. Perceived usefulness, perceived ease of use and user acceptance show that magniﬁcation is less preferred.", "tokens": ["Enhancing", "Android", "accessibility", "for", "users", "with", "hand", "tremor", "by", "reducing", "fine", "pointing", "and", "steady", "tapping", "Figure", "5", ".", "Perceived", "usefulness", ",", "perceived", "ease", "of", "use", "and", "user", "acceptance", "show", "that", "magniﬁcation", "is", "less", "preferred", "."]}, "filename": "dfa09494b50030571735aafbe5c114cd9ba80eba_Image_005.png", "orig_filename": "dfa09494b50030571735aafbe5c114cd9ba80eba", "split": "train"}, {"article_id": "Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments", "description": {"raw": "Figure 7: \"Letter-value box plot of the time until the participants first walked through the wall in room 3, room 4, and across rooms according to avatar anthropomorphism and visibility. \"", "tokens": ["Figure", "7", ":", "``", "Letter-value", "box", "plot", "of", "the", "time", "until", "the", "participants", "first", "walked", "through", "the", "wall", "in", "room", "3", ",", "room", "4", ",", "and", "across", "rooms", "according", "to", "avatar", "anthropomorphism", "and", "visibility.", "``"]}, "caption": {"raw": "Figure 7. Letter-value box plot1 with data points of the time until the participants ﬁrst walked through the wall in room 3 (left), room 4 (center), and across rooms (right) according to avatar anthropomorphism and visibility. C, H, R, and F represent Controller, Human Hand, Robot, and Full-body Human, respectively. In the cross-room results, the data points are colored according to the rooms where the participants ﬁrst walked through the wall.", "tokens": ["Figure", "7", ".", "Letter-value", "box", "plot1", "with", "data", "points", "of", "the", "time", "until", "the", "participants", "ﬁrst", "walked", "through", "the", "wall", "in", "room", "3", "(", "left", ")", ",", "room", "4", "(", "center", ")", ",", "and", "across", "rooms", "(", "right", ")", "according", "to", "avatar", "anthropomorphism", "and", "visibility", ".", "C", ",", "H", ",", "R", ",", "and", "F", "represent", "Controller", ",", "Human", "Hand", ",", "Robot", ",", "and", "Full-body", "Human", ",", "respectively", ".", "In", "the", "cross-room", "results", ",", "the", "data", "points", "are", "colored", "according", "to", "the", "rooms", "where", "the", "participants", "ﬁrst", "walked", "through", "the", "wall", "."]}, "context": {"raw": "Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments Figure 7. Letter-value box plot1 with data points of the time until the participants ﬁrst walked through the wall in room 3 (left), room 4 (center), and across rooms (right) according to avatar anthropomorphism and visibility. C, H, R, and F represent Controller, Human Hand, Robot, and Full-body Human, respectively. In the cross-room results, the data points are colored according to the rooms where the participants ﬁrst walked through the wall.", "tokens": ["Do", "You", "Feel", "Like", "Passing", "Through", "Walls", "?", ":", "Effect", "of", "Self-Avatar", "Appearance", "on", "Facilitating", "Realistic", "Behavior", "in", "Virtual", "Environments", "Figure", "7", ".", "Letter-value", "box", "plot1", "with", "data", "points", "of", "the", "time", "until", "the", "participants", "ﬁrst", "walked", "through", "the", "wall", "in", "room", "3", "(", "left", ")", ",", "room", "4", "(", "center", ")", ",", "and", "across", "rooms", "(", "right", ")", "according", "to", "avatar", "anthropomorphism", "and", "visibility", ".", "C", ",", "H", ",", "R", ",", "and", "F", "represent", "Controller", ",", "Human", "Hand", ",", "Robot", ",", "and", "Full-body", "Human", ",", "respectively", ".", "In", "the", "cross-room", "results", ",", "the", "data", "points", "are", "colored", "according", "to", "the", "rooms", "where", "the", "participants", "ﬁrst", "walked", "through", "the", "wall", "."]}, "filename": "b93cf632919c3ddf2a7efcd161bf03fe288761de_Image_007.png", "orig_filename": "b93cf632919c3ddf2a7efcd161bf03fe288761de", "split": "train"}, {"article_id": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "description": {"raw": "This Figure is divided into three parts: a, b, and c.\n\nPart (a) of this figure shows a stacked bar plot showing percentages of 5-point likert-scale responses. The title is “This text was easy to read”, the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The likert-scale responses for “Neutral” is centered, and “Disagree” and “Strongly Disagree” are on the left side, and “Agree” and “Strongly Agree” are on the right side. For all the conditions, the majority of responses are on the right side, and there is very little on the left side.\n\nFor \"Original\" 4% responded “Strongly Disagree”, 4% for “Disagree”, 24% for “Neutral”, 56% for “Agree”, and 12% for “Strongly Agree”. \nFor \"Automatic\" 0% responded “Strongly Disagree”, 4% for “Disagree”, 8% for “Neutral”, 60% for “Agree”, and 28% for “Strongly Agree”.\nFor \"Pop-up\" 0% responded “Strongly Disagree”, 8% for “Disagree”, 12% for “Neutral”, 48% for “Agree”, and 32% for “Strongly Agree”. \nFor \"Decoration\" 0% responded “Strongly Disagree”, 4% for “Disagree”, 12% for “Neutral”, 52% for “Agree”, and 32% for “Strongly Agree”.\n\n\nPart (b) of this figure shows a stacked bar plot showing percentages of 5-point Likert-scale responses. The title is “I was able to understand this text well.”, the y-axis has 4 categories; \"Original\", \"Automatic\", \"Pop-up\", and \"Decoration\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. There is a bracket indicating p<.05 significance between the “Original” and “Pop-up” plots. There is also a bracket indicating p<.05 significance between the “Original” and “Decoration” plots. The Likert-scale responses for “Neutral” is centered, and “Disagree” and “Strongly Disagree” are on the left side, and “Agree” and “Strongly Agree” are on the right side. For all conditions, the majority of responses are on the right side.\n\nFor \"Original\" 0% responded “Strongly Disagree”, 12% for “Disagree”, 20% for “Neutral”, 60% for “Agree”, and 8% for “Strongly Agree”. \nFor \"Automatic\" 0% responded “Strongly Disagree”, 4% for “Disagree”, 20% for “Neutral”, 40% for “Agree”, and 36% for “Strongly Agree”.\nFor \"Pop-up\" 0% responded “Strongly Disagree”, 0% for “Disagree”, 12% for “Neutral”, 48% for “Agree”, and 40% for “Strongly Agree”. \nFor \"Decoration\" 0% responded “Strongly Disagree”, 0% for “Disagree”, 12% for “Neutral”, 44% for “Agree”, and 44% for “Strongly Agree”.\n\n\nPart (c) of this figure shows a boxplot for Comprehension Scores for four different conditions, “Original”, “Automatic”, “Pop-up”, and “Decoration”, which are on the x-axis. The y-axis is the score, and is a percentage from 0 to 100. For “Original” and “Automatic” the boxplots appear to be almost equal and most of the boxplot is between 30% and 70%. For “Pop-up”, it is higher up, with the first quartile starting at around 70%. For “Decoration” there is a wider range, the first quartile is at around 30% and the third quartile is at 100%.\n\nThe “Original” boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 62.70%, 3rd quartile 66.67%, and a maximum of 100%.\nThe “Automatic” boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.7%, mean 58.7%, 3rd quartile 66.67%, and a maximum of 100%.\nThe “Pop-up” boxplot has a minimum of 33.33%, 1st quartile 66.67%, median 66.7%, mean 76%, 3rd quartile 100%, and a maximum of 100%.\nThe “Decoration” boxplot has a minimum of 0%, 1st quartile 33.33%, median 66.67%, mean 60%, 3rd quartile 100%, and a maximum of 100%.", "tokens": ["This", "Figure", "is", "divided", "into", "three", "parts", ":", "a", ",", "b", ",", "and", "c.", "Part", "(", "a", ")", "of", "this", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "5-point", "likert-scale", "responses", ".", "The", "title", "is", "“", "This", "text", "was", "easy", "to", "read", "”", ",", "the", "y-axis", "has", "4", "categories", ";", "``", "Original", "''", ",", "``", "Automatic", "''", ",", "``", "Pop-up", "''", ",", "and", "``", "Decoration", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "The", "likert-scale", "responses", "for", "“", "Neutral", "”", "is", "centered", ",", "and", "“", "Disagree", "”", "and", "“", "Strongly", "Disagree", "”", "are", "on", "the", "left", "side", ",", "and", "“", "Agree", "”", "and", "“", "Strongly", "Agree", "”", "are", "on", "the", "right", "side", ".", "For", "all", "the", "conditions", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ",", "and", "there", "is", "very", "little", "on", "the", "left", "side", ".", "For", "``", "Original", "''", "4", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "24", "%", "for", "“", "Neutral", "”", ",", "56", "%", "for", "“", "Agree", "”", ",", "and", "12", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Automatic", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "8", "%", "for", "“", "Neutral", "”", ",", "60", "%", "for", "“", "Agree", "”", ",", "and", "28", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Pop-up", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "8", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "48", "%", "for", "“", "Agree", "”", ",", "and", "32", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Decoration", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "52", "%", "for", "“", "Agree", "”", ",", "and", "32", "%", "for", "“", "Strongly", "Agree", "”", ".", "Part", "(", "b", ")", "of", "this", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "5-point", "Likert-scale", "responses", ".", "The", "title", "is", "“", "I", "was", "able", "to", "understand", "this", "text", "well.", "”", ",", "the", "y-axis", "has", "4", "categories", ";", "``", "Original", "''", ",", "``", "Automatic", "''", ",", "``", "Pop-up", "''", ",", "and", "``", "Decoration", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "There", "is", "a", "bracket", "indicating", "p", "<", ".05", "significance", "between", "the", "“", "Original", "”", "and", "“", "Pop-up", "”", "plots", ".", "There", "is", "also", "a", "bracket", "indicating", "p", "<", ".05", "significance", "between", "the", "“", "Original", "”", "and", "“", "Decoration", "”", "plots", ".", "The", "Likert-scale", "responses", "for", "“", "Neutral", "”", "is", "centered", ",", "and", "“", "Disagree", "”", "and", "“", "Strongly", "Disagree", "”", "are", "on", "the", "left", "side", ",", "and", "“", "Agree", "”", "and", "“", "Strongly", "Agree", "”", "are", "on", "the", "right", "side", ".", "For", "all", "conditions", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ".", "For", "``", "Original", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "12", "%", "for", "“", "Disagree", "”", ",", "20", "%", "for", "“", "Neutral", "”", ",", "60", "%", "for", "“", "Agree", "”", ",", "and", "8", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Automatic", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "4", "%", "for", "“", "Disagree", "”", ",", "20", "%", "for", "“", "Neutral", "”", ",", "40", "%", "for", "“", "Agree", "”", ",", "and", "36", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Pop-up", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "0", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "48", "%", "for", "“", "Agree", "”", ",", "and", "40", "%", "for", "“", "Strongly", "Agree", "”", ".", "For", "``", "Decoration", "''", "0", "%", "responded", "“", "Strongly", "Disagree", "”", ",", "0", "%", "for", "“", "Disagree", "”", ",", "12", "%", "for", "“", "Neutral", "”", ",", "44", "%", "for", "“", "Agree", "”", ",", "and", "44", "%", "for", "“", "Strongly", "Agree", "”", ".", "Part", "(", "c", ")", "of", "this", "figure", "shows", "a", "boxplot", "for", "Comprehension", "Scores", "for", "four", "different", "conditions", ",", "“", "Original", "”", ",", "“", "Automatic", "”", ",", "“", "Pop-up", "”", ",", "and", "“", "Decoration", "”", ",", "which", "are", "on", "the", "x-axis", ".", "The", "y-axis", "is", "the", "score", ",", "and", "is", "a", "percentage", "from", "0", "to", "100", ".", "For", "“", "Original", "”", "and", "“", "Automatic", "”", "the", "boxplots", "appear", "to", "be", "almost", "equal", "and", "most", "of", "the", "boxplot", "is", "between", "30", "%", "and", "70", "%", ".", "For", "“", "Pop-up", "”", ",", "it", "is", "higher", "up", ",", "with", "the", "first", "quartile", "starting", "at", "around", "70", "%", ".", "For", "“", "Decoration", "”", "there", "is", "a", "wider", "range", ",", "the", "first", "quartile", "is", "at", "around", "30", "%", "and", "the", "third", "quartile", "is", "at", "100", "%", ".", "The", "“", "Original", "”", "boxplot", "has", "a", "minimum", "of", "0", "%", ",", "1st", "quartile", "33.33", "%", ",", "median", "66.67", "%", ",", "mean", "62.70", "%", ",", "3rd", "quartile", "66.67", "%", ",", "and", "a", "maximum", "of", "100", "%", ".", "The", "“", "Automatic", "”", "boxplot", "has", "a", "minimum", "of", "0", "%", ",", "1st", "quartile", "33.33", "%", ",", "median", "66.7", "%", ",", "mean", "58.7", "%", ",", "3rd", "quartile", "66.67", "%", ",", "and", "a", "maximum", "of", "100", "%", ".", "The", "“", "Pop-up", "”", "boxplot", "has", "a", "minimum", "of", "33.33", "%", ",", "1st", "quartile", "66.67", "%", ",", "median", "66.7", "%", ",", "mean", "76", "%", ",", "3rd", "quartile", "100", "%", ",", "and", "a", "maximum", "of", "100", "%", ".", "The", "“", "Decoration", "”", "boxplot", "has", "a", "minimum", "of", "0", "%", ",", "1st", "quartile", "33.33", "%", ",", "median", "66.67", "%", ",", "mean", "60", "%", ",", "3rd", "quartile", "100", "%", ",", "and", "a", "maximum", "of", "100", "%", "."]}, "caption": {"raw": "(c)  Figure 4. Participants’ responses to questions about all four conditions in the experimental study, including subjective Likert-scale responses for (a) the text was easy to read and (b) I was able to understand this text well, with significant pairwise differences marked with asterisks (* p <0.05). In (c), analysis on objective comprehension questions did not reveal any significant differences between the four conditions.", "tokens": ["(", "c", ")", "Figure", "4", ".", "Participants", "’", "responses", "to", "questions", "about", "all", "four", "conditions", "in", "the", "experimental", "study", ",", "including", "subjective", "Likert-scale", "responses", "for", "(", "a", ")", "the", "text", "was", "easy", "to", "read", "and", "(", "b", ")", "I", "was", "able", "to", "understand", "this", "text", "well", ",", "with", "significant", "pairwise", "differences", "marked", "with", "asterisks", "(", "*", "p", "<", "0.05", ")", ".", "In", "(", "c", ")", ",", "analysis", "on", "objective", "comprehension", "questions", "did", "not", "reveal", "any", "significant", "differences", "between", "the", "four", "conditions", "."]}, "context": {"raw": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy (c)  Figure 4. Participants’ responses to questions about all four conditions in the experimental study, including subjective Likert-scale responses for (a) the text was easy to read and (b) I was able to understand this text well, with significant pairwise differences marked with asterisks (* p <0.05). In (c), analysis on objective comprehension questions did not reveal any significant differences between the four conditions.", "tokens": ["Automatic", "Text", "Simplification", "Tools", "for", "Deaf", "and", "Hard", "of", "Hearing", "Adults", ":", "Benefits", "of", "Lexical", "Simplification", "and", "Providing", "Users", "with", "Autonomy", "(", "c", ")", "Figure", "4", ".", "Participants", "’", "responses", "to", "questions", "about", "all", "four", "conditions", "in", "the", "experimental", "study", ",", "including", "subjective", "Likert-scale", "responses", "for", "(", "a", ")", "the", "text", "was", "easy", "to", "read", "and", "(", "b", ")", "I", "was", "able", "to", "understand", "this", "text", "well", ",", "with", "significant", "pairwise", "differences", "marked", "with", "asterisks", "(", "*", "p", "<", "0.05", ")", ".", "In", "(", "c", ")", ",", "analysis", "on", "objective", "comprehension", "questions", "did", "not", "reveal", "any", "significant", "differences", "between", "the", "four", "conditions", "."]}, "filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_006.jpg", "orig_filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "split": "train"}, {"article_id": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings", "description": {"raw": "A horizontal stacked bar graph of participants' ranks of the markup styles. The x axis is numeric, ranging from 0 to 100%. The y axis is the 4 markup styles: Yellow, Underline, No Change, and Italics. The most prominent feature is No Change with the highest ranking. The ranks are Ranked #1, Ranked #2, Ranked #3, and Ranked #4. Yellow had 18.7%, 11.2%, 13.1%, and 57% for the ranks. Underline had 2.8%, 27.1%, 47.7%, and 22.4%. No Change had 66.4%, 12.1%, 8.4%, and 13.1%. Italics had 12.1%, 49.5%, 30.8%, and 7.5%.", "tokens": ["A", "horizontal", "stacked", "bar", "graph", "of", "participants", "'", "ranks", "of", "the", "markup", "styles", ".", "The", "x", "axis", "is", "numeric", ",", "ranging", "from", "0", "to", "100", "%", ".", "The", "y", "axis", "is", "the", "4", "markup", "styles", ":", "Yellow", ",", "Underline", ",", "No", "Change", ",", "and", "Italics", ".", "The", "most", "prominent", "feature", "is", "No", "Change", "with", "the", "highest", "ranking", ".", "The", "ranks", "are", "Ranked", "#", "1", ",", "Ranked", "#", "2", ",", "Ranked", "#", "3", ",", "and", "Ranked", "#", "4", ".", "Yellow", "had", "18.7", "%", ",", "11.2", "%", ",", "13.1", "%", ",", "and", "57", "%", "for", "the", "ranks", ".", "Underline", "had", "2.8", "%", ",", "27.1", "%", ",", "47.7", "%", ",", "and", "22.4", "%", ".", "No", "Change", "had", "66.4", "%", ",", "12.1", "%", ",", "8.4", "%", ",", "and", "13.1", "%", ".", "Italics", "had", "12.1", "%", ",", "49.5", "%", ",", "30.8", "%", ",", "and", "7.5", "%", "."]}, "caption": {"raw": "Figure 9. Larger Study: Ranking of Markup Types", "tokens": ["Figure", "9", ".", "Larger", "Study", ":", "Ranking", "of", "Markup", "Types"]}, "context": {"raw": "Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings Figure 9. Larger Study: Ranking of Markup Types", "tokens": ["Deaf", "and", "Hard-of-Hearing", "Perspectives", "on", "Imperfect", "Automatic", "Speech", "Recognition", "for", "Captioning", "One-on-One", "Meetings", "Figure", "9", ".", "Larger", "Study", ":", "Ranking", "of", "Markup", "Types"]}, "filename": "471f9168db0fcb72d394222491966b97c098b1cd_Image_019.jpg", "orig_filename": "471f9168db0fcb72d394222491966b97c098b1cd", "split": "train"}, {"article_id": "Which robot am I thinking about? The impact of action and appearance on people's evaluations of a moral robot", "description": {"raw": "Column graph shows results of Experiment 1: people's blame judgments for each of the four agents, for both action and inaction.", "tokens": ["Column", "graph", "shows", "results", "of", "Experiment", "1", ":", "people", "'s", "blame", "judgments", "for", "each", "of", "the", "four", "agents", ",", "for", "both", "action", "and", "inaction", "."]}, "caption": {"raw": "Expanding the analysis to the four-level Agent factor (under neutral phrasing) revealed that people’s blame patterns for action vs. inaction for both the humanoid robot and the AI were similar to the blame pattern for the human agent (ps > .21)—i.e., being blamed more for action than inaction—whereas blame for the mechanical robot differed significantly from blame for the human agent, F(1, 310) = 6.08, p = .014 (see Figure 4). Particularly intriguing is the direct comparison of mechanical robot and humanoid robot, because their accompanying narratives and labels were identical (“advanced state-of-the-art repair robot”). For the neutral phrasing, the mechanical robot received 7.4 points more blame for inaction than action, whereas the humanoid robot received 10.3 points fewer for inaction than action, F(1, 310) = 2.54, p = .11.", "tokens": ["Expanding", "the", "analysis", "to", "the", "four-level", "Agent", "factor", "(", "under", "neutral", "phrasing", ")", "revealed", "that", "people", "’", "s", "blame", "patterns", "for", "action", "vs.", "inaction", "for", "both", "the", "humanoid", "robot", "and", "the", "AI", "were", "similar", "to", "the", "blame", "pattern", "for", "the", "human", "agent", "(", "ps", ">", ".21", ")", "—i.e.", ",", "being", "blamed", "more", "for", "action", "than", "inaction—whereas", "blame", "for", "the", "mechanical", "robot", "differed", "significantly", "from", "blame", "for", "the", "human", "agent", ",", "F", "(", "1", ",", "310", ")", "=", "6.08", ",", "p", "=", ".014", "(", "see", "Figure", "4", ")", ".", "Particularly", "intriguing", "is", "the", "direct", "comparison", "of", "mechanical", "robot", "and", "humanoid", "robot", ",", "because", "their", "accompanying", "narratives", "and", "labels", "were", "identical", "(", "“", "advanced", "state-of-the-art", "repair", "robot", "”", ")", ".", "For", "the", "neutral", "phrasing", ",", "the", "mechanical", "robot", "received", "7.4", "points", "more", "blame", "for", "inaction", "than", "action", ",", "whereas", "the", "humanoid", "robot", "received", "10.3", "points", "fewer", "for", "inaction", "than", "action", ",", "F", "(", "1", ",", "310", ")", "=", "2.54", ",", "p", "=", ".11", "."]}, "context": {"raw": "Which robot am I thinking about? The impact of action and appearance on people's evaluations of a moral robot Expanding the analysis to the four-level Agent factor (under neutral phrasing) revealed that people’s blame patterns for action vs. inaction for both the humanoid robot and the AI were similar to the blame pattern for the human agent (ps > .21)—i.e., being blamed more for action than inaction—whereas blame for the mechanical robot differed significantly from blame for the human agent, F(1, 310) = 6.08, p = .014 (see Figure 4). Particularly intriguing is the direct comparison of mechanical robot and humanoid robot, because their accompanying narratives and labels were identical (“advanced state-of-the-art repair robot”). For the neutral phrasing, the mechanical robot received 7.4 points more blame for inaction than action, whereas the humanoid robot received 10.3 points fewer for inaction than action, F(1, 310) = 2.54, p = .11.", "tokens": ["Which", "robot", "am", "I", "thinking", "about", "?", "The", "impact", "of", "action", "and", "appearance", "on", "people", "'s", "evaluations", "of", "a", "moral", "robot", "Expanding", "the", "analysis", "to", "the", "four-level", "Agent", "factor", "(", "under", "neutral", "phrasing", ")", "revealed", "that", "people", "’", "s", "blame", "patterns", "for", "action", "vs.", "inaction", "for", "both", "the", "humanoid", "robot", "and", "the", "AI", "were", "similar", "to", "the", "blame", "pattern", "for", "the", "human", "agent", "(", "ps", ">", ".21", ")", "—i.e.", ",", "being", "blamed", "more", "for", "action", "than", "inaction—whereas", "blame", "for", "the", "mechanical", "robot", "differed", "significantly", "from", "blame", "for", "the", "human", "agent", ",", "F", "(", "1", ",", "310", ")", "=", "6.08", ",", "p", "=", ".014", "(", "see", "Figure", "4", ")", ".", "Particularly", "intriguing", "is", "the", "direct", "comparison", "of", "mechanical", "robot", "and", "humanoid", "robot", ",", "because", "their", "accompanying", "narratives", "and", "labels", "were", "identical", "(", "“", "advanced", "state-of-the-art", "repair", "robot", "”", ")", ".", "For", "the", "neutral", "phrasing", ",", "the", "mechanical", "robot", "received", "7.4", "points", "more", "blame", "for", "inaction", "than", "action", ",", "whereas", "the", "humanoid", "robot", "received", "10.3", "points", "fewer", "for", "inaction", "than", "action", ",", "F", "(", "1", ",", "310", ")", "=", "2.54", ",", "p", "=", ".11", "."]}, "filename": "bc2d3dfd2555c7590021db88d60d729dde30be82_Image_006.jpg", "orig_filename": "bc2d3dfd2555c7590021db88d60d729dde30be82", "split": "train"}, {"article_id": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality", "description": {"raw": "A combination plot of a bar chart for mean error rate and a point chart using cross circle, coloured orange, for mean offset distance (in cm). The left y-axis indicates mean error rate and the right y-axis indicates the mean offset distance. The x-axis indicates the pointing condition. The first bar is VC, coloured grey. The 2nd, 3rd, and 4th bars, coloured yellow, represent pointing techniques with VB, and 5th, 6th, 7th bars, coloured blue, represent pointing techniques with WD. The error bar of each pointing conditions and the significant difference between each other are also visualised.", "tokens": ["A", "combination", "plot", "of", "a", "bar", "chart", "for", "mean", "error", "rate", "and", "a", "point", "chart", "using", "cross", "circle", ",", "coloured", "orange", ",", "for", "mean", "offset", "distance", "(", "in", "cm", ")", ".", "The", "left", "y-axis", "indicates", "mean", "error", "rate", "and", "the", "right", "y-axis", "indicates", "the", "mean", "offset", "distance", ".", "The", "x-axis", "indicates", "the", "pointing", "condition", ".", "The", "first", "bar", "is", "VC", ",", "coloured", "grey", ".", "The", "2nd", ",", "3rd", ",", "and", "4th", "bars", ",", "coloured", "yellow", ",", "represent", "pointing", "techniques", "with", "VB", ",", "and", "5th", ",", "6th", ",", "7th", "bars", ",", "coloured", "blue", ",", "represent", "pointing", "techniques", "with", "WD", ".", "The", "error", "bar", "of", "each", "pointing", "conditions", "and", "the", "significant", "difference", "between", "each", "other", "are", "also", "visualised", "."]}, "caption": {"raw": "Figure 7. Mean %Err (bar) and OD (cross circle) for pointing conditions. The corresponding error bars are visualized to indicate standard devia- tion. The statistical signiﬁcances evaluated by pairwise t-test are marked with stars (∗∗ = p < 0.01 and ∗ = p < 0.05).", "tokens": ["Figure", "7", ".", "Mean", "%", "Err", "(", "bar", ")", "and", "OD", "(", "cross", "circle", ")", "for", "pointing", "conditions", ".", "The", "corresponding", "error", "bars", "are", "visualized", "to", "indicate", "standard", "devia-", "tion", ".", "The", "statistical", "signiﬁcances", "evaluated", "by", "pairwise", "t-test", "are", "marked", "with", "stars", "(", "∗∗", "=", "p", "<", "0.01", "and", "∗", "=", "p", "<", "0.05", ")", "."]}, "context": {"raw": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality Figure 7. Mean %Err (bar) and OD (cross circle) for pointing conditions. The corresponding error bars are visualized to indicate standard devia- tion. The statistical signiﬁcances evaluated by pairwise t-test are marked with stars (∗∗ = p < 0.01 and ∗ = p < 0.05).", "tokens": ["Understanding", "Viewport-", "and", "World-based", "Pointing", "with", "Everyday", "Smart", "Devices", "in", "Immersive", "Augmented", "Reality", "Figure", "7", ".", "Mean", "%", "Err", "(", "bar", ")", "and", "OD", "(", "cross", "circle", ")", "for", "pointing", "conditions", ".", "The", "corresponding", "error", "bars", "are", "visualized", "to", "indicate", "standard", "devia-", "tion", ".", "The", "statistical", "signiﬁcances", "evaluated", "by", "pairwise", "t-test", "are", "marked", "with", "stars", "(", "∗∗", "=", "p", "<", "0.01", "and", "∗", "=", "p", "<", "0.05", ")", "."]}, "filename": "7fa3a268386650d6940c6450eb41de2f28ff0a6a_Image_006.jpg", "orig_filename": "7fa3a268386650d6940c6450eb41de2f28ff0a6a", "split": "train"}, {"article_id": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection", "description": {"raw": "Bar chart of number of participants that considered to stop using their tracker, shows that 40% of the participants somewhat to strongly agree to having considered to stop using their tracker.", "tokens": ["Bar", "chart", "of", "number", "of", "participants", "that", "considered", "to", "stop", "using", "their", "tracker", ",", "shows", "that", "40", "%", "of", "the", "participants", "somewhat", "to", "strongly", "agree", "to", "having", "considered", "to", "stop", "using", "their", "tracker", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "The Technology-Mediated Reflection Model: Barriers and Assistance in Data-Driven Reflection ", "tokens": ["The", "Technology-Mediated", "Reflection", "Model", ":", "Barriers", "and", "Assistance", "in", "Data-Driven", "Reflection"]}, "filename": "09ad32856da3e455b58e637c4d0f4efe34573ca3_Image_002.jpg", "orig_filename": "09ad32856da3e455b58e637c4d0f4efe34573ca3", "split": "train"}, {"article_id": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards", "description": {"raw": "Box chart of durations for tasks 1-11. Time in minutes on the Y-axis, and task numbers on the X-axis. There is a higher variance in duration for task 4 than for the rest of the tasks, and it ranges from 0.25 to just over 4 minutes, with the majority between 1 and 2.5 minutes. Tasks 8 and 10 have noticeably short durations, close to zero, with little variance. 1, 2, and 11 range between 0 and 0.5 minutes. 3, 5, and 7 range between 0.25 and 1.25 minutes, and 6 and 9 reach nearly 1.5 minutes.    Box chart of durations for tasks 2-23. Half of the tasks (12, 13, 14, 15, 16, and 17) have an extremely high variance. The boxes for 12, 16, and 17 range from around 1.25 minutes to 5 minutes. 13 has whiskers from 1 to 5, and the box is between 2 and 4. 14 has whiskers from 0 to 5 (the entire Y axis) and a box between 1.25 and 4.5. 15 is lower, the box ranging between 0.5 and 3 minutes. Tasks 19, and 21 have noticeably short durations, between nearly zero and 0.25 minutes, with little variance. 18, 20, 22, and 23 have boxes which span 1.5 minutes; 18 ranges from almost 0 up to 1.25, 22 ranges from 1.5 to 3 with a whisker extending below 0.5. 22 and 23 are nearly identical, ranging from about 0.75 to 2.5 minutes.", "tokens": ["Box", "chart", "of", "durations", "for", "tasks", "1-11", ".", "Time", "in", "minutes", "on", "the", "Y-axis", ",", "and", "task", "numbers", "on", "the", "X-axis", ".", "There", "is", "a", "higher", "variance", "in", "duration", "for", "task", "4", "than", "for", "the", "rest", "of", "the", "tasks", ",", "and", "it", "ranges", "from", "0.25", "to", "just", "over", "4", "minutes", ",", "with", "the", "majority", "between", "1", "and", "2.5", "minutes", ".", "Tasks", "8", "and", "10", "have", "noticeably", "short", "durations", ",", "close", "to", "zero", ",", "with", "little", "variance", ".", "1", ",", "2", ",", "and", "11", "range", "between", "0", "and", "0.5", "minutes", ".", "3", ",", "5", ",", "and", "7", "range", "between", "0.25", "and", "1.25", "minutes", ",", "and", "6", "and", "9", "reach", "nearly", "1.5", "minutes", ".", "Box", "chart", "of", "durations", "for", "tasks", "2-23", ".", "Half", "of", "the", "tasks", "(", "12", ",", "13", ",", "14", ",", "15", ",", "16", ",", "and", "17", ")", "have", "an", "extremely", "high", "variance", ".", "The", "boxes", "for", "12", ",", "16", ",", "and", "17", "range", "from", "around", "1.25", "minutes", "to", "5", "minutes", ".", "13", "has", "whiskers", "from", "1", "to", "5", ",", "and", "the", "box", "is", "between", "2", "and", "4", ".", "14", "has", "whiskers", "from", "0", "to", "5", "(", "the", "entire", "Y", "axis", ")", "and", "a", "box", "between", "1.25", "and", "4.5", ".", "15", "is", "lower", ",", "the", "box", "ranging", "between", "0.5", "and", "3", "minutes", ".", "Tasks", "19", ",", "and", "21", "have", "noticeably", "short", "durations", ",", "between", "nearly", "zero", "and", "0.25", "minutes", ",", "with", "little", "variance", ".", "18", ",", "20", ",", "22", ",", "and", "23", "have", "boxes", "which", "span", "1.5", "minutes", ";", "18", "ranges", "from", "almost", "0", "up", "to", "1.25", ",", "22", "ranges", "from", "1.5", "to", "3", "with", "a", "whisker", "extending", "below", "0.5", ".", "22", "and", "23", "are", "nearly", "identical", ",", "ranging", "from", "about", "0.75", "to", "2.5", "minutes", "."]}, "caption": {"raw": "Task durations for the interpretive tasks, #1-11.Task durations for the generative tasks, #12-23.", "tokens": ["Task", "durations", "for", "the", "interpretive", "tasks", ",", "#", "1-11.Task", "durations", "for", "the", "generative", "tasks", ",", "#", "12-23", "."]}, "context": {"raw": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards Task durations for the interpretive tasks, #1-11.Task durations for the generative tasks, #12-23.", "tokens": ["Understanding", "Blind", "Screen-Reader", "Users", "’", "Experiences", "of", "Digital", "Artboards", "Task", "durations", "for", "the", "interpretive", "tasks", ",", "#", "1-11.Task", "durations", "for", "the", "generative", "tasks", ",", "#", "12-23", "."]}, "filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_010.jpg", "orig_filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "split": "train"}, {"article_id": "Designing an Animated Character System for American Sign Language", "description": {"raw": "Figure 6: Identification Accuracy, the percent who identified signs from stationary vs. animated characters, without training. This figure presents a bar chart, with separate bars for stationary (light blue) and animated (dark blue) characters. Y-axis is Identification Accuracy, ranging from 0-80. X-axis is ASL Sign, including four signs: WHERE, UNDERSTAND, MAYBE, and MOTIVATION.", "tokens": ["Figure", "6", ":", "Identification", "Accuracy", ",", "the", "percent", "who", "identified", "signs", "from", "stationary", "vs.", "animated", "characters", ",", "without", "training", ".", "This", "figure", "presents", "a", "bar", "chart", ",", "with", "separate", "bars", "for", "stationary", "(", "light", "blue", ")", "and", "animated", "(", "dark", "blue", ")", "characters", ".", "Y-axis", "is", "Identification", "Accuracy", ",", "ranging", "from", "0-80", ".", "X-axis", "is", "ASL", "Sign", ",", "including", "four", "signs", ":", "WHERE", ",", "UNDERSTAND", ",", "MAYBE", ",", "and", "MOTIVATION", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Designing an Animated Character System for American Sign Language ", "tokens": ["Designing", "an", "Animated", "Character", "System", "for", "American", "Sign", "Language"]}, "filename": "8209b931c94fea5d107e6ef2461b64e00fd52249_Image_007.jpg", "orig_filename": "8209b931c94fea5d107e6ef2461b64e00fd52249", "split": "train"}, {"article_id": "Personal Space in Play: Physical and Digital Boundaries in Large-Display Cooperative and Competitive Games", "description": {"raw": "Figure 4: ``Heat maps presented and referred to in the paper are assembled as part of an overall comparison. Along the x-axis, its notes the different conditions (all, competitive, and cooperative) and the y-axis denotes fixed and floating (as alternating rows), and the nature of the recorded data (digital space enemies targeted, physical space finger touch points, and physical floor space).''", "tokens": ["Figure", "4", ":", "``", "Heat", "maps", "presented", "and", "referred", "to", "in", "the", "paper", "are", "assembled", "as", "part", "of", "an", "overall", "comparison", ".", "Along", "the", "x-axis", ",", "its", "notes", "the", "different", "conditions", "(", "all", ",", "competitive", ",", "and", "cooperative", ")", "and", "the", "y-axis", "denotes", "fixed", "and", "floating", "(", "as", "alternating", "rows", ")", ",", "and", "the", "nature", "of", "the", "recorded", "data", "(", "digital", "space", "enemies", "targeted", ",", "physical", "space", "finger", "touch", "points", ",", "and", "physical", "floor", "space", ")", ".", "''"]}, "caption": {"raw": "Figure 4. The visualizations of the participants position on the ﬂoor (bot- tom), ﬁnger touch point position (middle), and ﬁnally their digital posi- tion in the form of where they killed their enemies (top).", "tokens": ["Figure", "4", ".", "The", "visualizations", "of", "the", "participants", "position", "on", "the", "ﬂoor", "(", "bot-", "tom", ")", ",", "ﬁnger", "touch", "point", "position", "(", "middle", ")", ",", "and", "ﬁnally", "their", "digital", "posi-", "tion", "in", "the", "form", "of", "where", "they", "killed", "their", "enemies", "(", "top", ")", "."]}, "context": {"raw": "Personal Space in Play: Physical and Digital Boundaries in Large-Display Cooperative and Competitive Games Figure 4. The visualizations of the participants position on the ﬂoor (bot- tom), ﬁnger touch point position (middle), and ﬁnally their digital posi- tion in the form of where they killed their enemies (top).", "tokens": ["Personal", "Space", "in", "Play", ":", "Physical", "and", "Digital", "Boundaries", "in", "Large-Display", "Cooperative", "and", "Competitive", "Games", "Figure", "4", ".", "The", "visualizations", "of", "the", "participants", "position", "on", "the", "ﬂoor", "(", "bot-", "tom", ")", ",", "ﬁnger", "touch", "point", "position", "(", "middle", ")", ",", "and", "ﬁnally", "their", "digital", "posi-", "tion", "in", "the", "form", "of", "where", "they", "killed", "their", "enemies", "(", "top", ")", "."]}, "filename": "a402b43d934bad20577e0c56fc5e5373bb97db15_Image_006.jpg", "orig_filename": "a402b43d934bad20577e0c56fc5e5373bb97db15", "split": "train"}, {"article_id": "How to Present Game Difficulty Choices?: Exploring the Impact on Player Experience", "description": {"raw": "Stacked rank results chart for the following data: Rank 1 E 33 Rank 2 E 4 Rank 3 E 5 Rank 1 A 6 Rank 2 A 18 Rank 3 A 7 Rank 1 M 15 Rank 2 M 14 Rank 3 M 6", "tokens": ["Stacked", "rank", "results", "chart", "for", "the", "following", "data", ":", "Rank", "1", "E", "33", "Rank", "2", "E", "4", "Rank", "3", "E", "5", "Rank", "1", "A", "6", "Rank", "2", "A", "18", "Rank", "3", "A", "7", "Rank", "1", "M", "15", "Rank", "2", "M", "14", "Rank", "3", "M", "6"]}, "caption": {"raw": "Figure 7: Ranking by participants regarding preference in study 2 (with the game fl0w).", "tokens": ["Figure", "7", ":", "Ranking", "by", "participants", "regarding", "preference", "in", "study", "2", "(", "with", "the", "game", "fl0w", ")", "."]}, "context": {"raw": "How to Present Game Difficulty Choices?: Exploring the Impact on Player Experience Figure 7: Ranking by participants regarding preference in study 2 (with the game fl0w).", "tokens": ["How", "to", "Present", "Game", "Difficulty", "Choices", "?", ":", "Exploring", "the", "Impact", "on", "Player", "Experience", "Figure", "7", ":", "Ranking", "by", "participants", "regarding", "preference", "in", "study", "2", "(", "with", "the", "game", "fl0w", ")", "."]}, "filename": "79f77995d06aa29b5a37e5fc3a4d5e7548abf248_Image_008.jpg", "orig_filename": "79f77995d06aa29b5a37e5fc3a4d5e7548abf248", "split": "train"}, {"article_id": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets", "description": {"raw": "The chart displays the relationship between force and pressure for all 8 configurations.", "tokens": ["The", "chart", "displays", "the", "relationship", "between", "force", "and", "pressure", "for", "all", "8", "configurations", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets ", "tokens": ["JetController", ":", "High-speed", "Ungrounded", "3-DoF", "Force", "Feedback", "Controllers", "using", "Air", "Propulsion", "Jets"]}, "filename": "abdf4590d986efc1465e3d2df1d13ec0af57110c_Image_008.jpg", "orig_filename": "abdf4590d986efc1465e3d2df1d13ec0af57110c", "split": "train"}, {"article_id": "Comparison of Methods for Teaching Accessibility in University Computing Courses", "description": {"raw": "Figure 1 describes our data collection intervals among the computing students whose degree required to take the HCI intervention course. An identical survey was administered three times: at beginning of the HCI course (Pre-survey), at the end of the HCI course (Post-survey), and 18-24 months after the HCI course (Senior-survey).", "tokens": ["Figure", "1", "describes", "our", "data", "collection", "intervals", "among", "the", "computing", "students", "whose", "degree", "required", "to", "take", "the", "HCI", "intervention", "course", ".", "An", "identical", "survey", "was", "administered", "three", "times", ":", "at", "beginning", "of", "the", "HCI", "course", "(", "Pre-survey", ")", ",", "at", "the", "end", "of", "the", "HCI", "course", "(", "Post-survey", ")", ",", "and", "18-24", "months", "after", "the", "HCI", "course", "(", "Senior-survey", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Comparison of Methods for Teaching Accessibility in University Computing Courses ", "tokens": ["Comparison", "of", "Methods", "for", "Teaching", "Accessibility", "in", "University", "Computing", "Courses"]}, "filename": "3243dde72340fe11b84bf83a9cc4e54a8eb6402d_Image_003.png", "orig_filename": "3243dde72340fe11b84bf83a9cc4e54a8eb6402d", "split": "train"}, {"article_id": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "description": {"raw": "Probability density plot, showing the PDF as a lightly shaded interval, the predicted arrival time as a dark vertical bar, and the scheduled arrival time as a light vertical bar.", "tokens": ["Probability", "density", "plot", ",", "showing", "the", "PDF", "as", "a", "lightly", "shaded", "interval", ",", "the", "predicted", "arrival", "time", "as", "a", "dark", "vertical", "bar", ",", "and", "the", "scheduled", "arrival", "time", "as", "a", "light", "vertical", "bar", "."]}, "caption": {"raw": "Probability density function (PDF) plots use an area encoding for probability that allows people to understand the shape of a distribution at", "tokens": ["Probability", "density", "function", "(", "PDF", ")", "plots", "use", "an", "area", "encoding", "for", "probability", "that", "allows", "people", "to", "understand", "the", "shape", "of", "a", "distribution", "at"]}, "context": {"raw": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making Probability density function (PDF) plots use an area encoding for probability that allows people to understand the shape of a distribution at", "tokens": ["Uncertainty", "Displays", "Using", "Quantile", "Dotplots", "or", "CDFs", "Improve", "Transit", "Decision-Making", "Probability", "density", "function", "(", "PDF", ")", "plots", "use", "an", "area", "encoding", "for", "probability", "that", "allows", "people", "to", "understand", "the", "shape", "of", "a", "distribution", "at"]}, "filename": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_007.jpg", "orig_filename": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "split": "train"}, {"article_id": "Effect of target size on non-visual text-entry", "description": {"raw": "The bar graph depicts no differences between the number of slips of large, medium and small size but a considerable difference to tiny that above 60% slip errors.", "tokens": ["The", "bar", "graph", "depicts", "no", "differences", "between", "the", "number", "of", "slips", "of", "large", ",", "medium", "and", "small", "size", "but", "a", "considerable", "difference", "to", "tiny", "that", "above", "60", "%", "slip", "errors", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Effect of target size on non-visual text-entry ", "tokens": ["Effect", "of", "target", "size", "on", "non-visual", "text-entry"]}, "filename": "f65de96c15e33484bc85079b354f053fd9fe8ccc_Image_006.jpg", "orig_filename": "f65de96c15e33484bc85079b354f053fd9fe8ccc", "split": "train"}, {"article_id": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "description": {"raw": "Figure 1: Text stating \"I'd like to visualize 'Origin', 'Miles_per_Gallon', and 'Displacement'\". Two visualizations are shown. The first, a colored tick plot with 'Miles_per_Gallon' on x, 'Origin' on y, and 'Displacement' on color. Next, a colored scatterplot, with 'Miles_per_Gallon' on x, 'Displacement' on y, and 'Origin' on color. There is a question mark between them.", "tokens": ["Figure", "1", ":", "Text", "stating", "``", "I", "'d", "like", "to", "visualize", "'Origin", "'", ",", "'Miles_per_Gallon", "'", ",", "and", "'Displacement", "'", "''", ".", "Two", "visualizations", "are", "shown", ".", "The", "first", ",", "a", "colored", "tick", "plot", "with", "'Miles_per_Gallon", "'", "on", "x", ",", "'Origin", "'", "on", "y", ",", "and", "'Displacement", "'", "on", "color", ".", "Next", ",", "a", "colored", "scatterplot", ",", "with", "'Miles_per_Gallon", "'", "on", "x", ",", "'Displacement", "'", "on", "y", ",", "and", "'Origin", "'", "on", "color", ".", "There", "is", "a", "question", "mark", "between", "them", "."]}, "caption": {"raw": "Figure 1. Which chart should a recommender suggest? Recommender systems are often forced to make decisions in the face of ambiguous user intent. Sometimes, these decisions will hamper exploration.", "tokens": ["Figure", "1", ".", "Which", "chart", "should", "a", "recommender", "suggest", "?", "Recommender", "systems", "are", "often", "forced", "to", "make", "decisions", "in", "the", "face", "of", "ambiguous", "user", "intent", ".", "Sometimes", ",", "these", "decisions", "will", "hamper", "exploration", "."]}, "context": {"raw": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations Figure 1. Which chart should a recommender suggest? Recommender systems are often forced to make decisions in the face of ambiguous user intent. Sometimes, these decisions will hamper exploration.", "tokens": ["Dziban", ":", "Balancing", "Agency", "&", "Automation", "in", "Visualization", "Design", "via", "Anchored", "Recommendations", "Figure", "1", ".", "Which", "chart", "should", "a", "recommender", "suggest", "?", "Recommender", "systems", "are", "often", "forced", "to", "make", "decisions", "in", "the", "face", "of", "ambiguous", "user", "intent", ".", "Sometimes", ",", "these", "decisions", "will", "hamper", "exploration", "."]}, "filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_001.png", "orig_filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "split": "train"}, {"article_id": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "description": {"raw": "For the box plot, the median for three stages are 1, 7 and 22 respectively. The range between the lower and the upper quartile is the smallest for stage 0, and is the largest for stage 1. The line plot for each participants' performance for three stages indicate that all participants improved from stage 0 to stage 2.", "tokens": ["For", "the", "box", "plot", ",", "the", "median", "for", "three", "stages", "are", "1", ",", "7", "and", "22", "respectively", ".", "The", "range", "between", "the", "lower", "and", "the", "upper", "quartile", "is", "the", "smallest", "for", "stage", "0", ",", "and", "is", "the", "largest", "for", "stage", "1", ".", "The", "line", "plot", "for", "each", "participants", "'", "performance", "for", "three", "stages", "indicate", "that", "all", "participants", "improved", "from", "stage", "0", "to", "stage", "2", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone ", "tokens": ["LightWrite", ":", "Teach", "Handwriting", "to", "The", "Visually", "Impaired", "with", "A", "Smartphone"]}, "filename": "1bd67672e9376feaac80770695717b13ece2c47b_Image_010.png", "orig_filename": "1bd67672e9376feaac80770695717b13ece2c47b", "split": "train"}, {"article_id": "Dive in!: enabling progressive loading for real-time navigation of data visualizations", "description": {"raw": "This chart illustrates the theoretical performance of progressive downloading across different LOD compression ratios and SPI settings.", "tokens": ["This", "chart", "illustrates", "the", "theoretical", "performance", "of", "progressive", "downloading", "across", "different", "LOD", "compression", "ratios", "and", "SPI", "settings", "."]}, "caption": {"raw": "Figure 5: Modeled response times under varying LOD hierar- chy compression ratios and SPI settings. Progressive loading vastly improves interaction latency.", "tokens": ["Figure", "5", ":", "Modeled", "response", "times", "under", "varying", "LOD", "hierar-", "chy", "compression", "ratios", "and", "SPI", "settings", ".", "Progressive", "loading", "vastly", "improves", "interaction", "latency", "."]}, "context": {"raw": "Dive in!: enabling progressive loading for real-time navigation of data visualizations Figure 5: Modeled response times under varying LOD hierar- chy compression ratios and SPI settings. Progressive loading vastly improves interaction latency.", "tokens": ["Dive", "in", "!", ":", "enabling", "progressive", "loading", "for", "real-time", "navigation", "of", "data", "visualizations", "Figure", "5", ":", "Modeled", "response", "times", "under", "varying", "LOD", "hierar-", "chy", "compression", "ratios", "and", "SPI", "settings", ".", "Progressive", "loading", "vastly", "improves", "interaction", "latency", "."]}, "filename": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a_Image_005.jpg", "orig_filename": "e44a1f6ef33c38a0aecb7dc5221055419b8f467a", "split": "train"}, {"article_id": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "description": {"raw": "A bar graph titled Figure 6: H1-e for Evaluation of Accuracy (Numeric). The figure is split into three facets (subgraphs), from left to right: WRAT-L, WRAT-M, and WRAT-H. The vertical axis ranges from 0 to 100, with ticks at increments of 10. On the horizontal axis are the labels for the WER accuracy levels in the study: Desktop, Cloud, and Human. In the WRAT-L facet, all pairwise comparisons are not significant, and the means are approximately: 55 for Desktop, 60 for Cloud, and 65 for Human. In the WRAT-M facet, the Desktop-Cloud and Desktop-Human pairwise comparison was significant with three stars, the Cloud-Human pairwise comparison had one star, and the means are approximately: 55 for Desktop, 65 for Cloud, and 70 for Human. In the WRAT-H facet, the Desktop-Cloud and Desktop-Human pairwise comparisons were significant at three stars, and the means are approximately: 50 for Desktop, 65 for Cloud, and 75 for Human.", "tokens": ["A", "bar", "graph", "titled", "Figure", "6", ":", "H1-e", "for", "Evaluation", "of", "Accuracy", "(", "Numeric", ")", ".", "The", "figure", "is", "split", "into", "three", "facets", "(", "subgraphs", ")", ",", "from", "left", "to", "right", ":", "WRAT-L", ",", "WRAT-M", ",", "and", "WRAT-H", ".", "The", "vertical", "axis", "ranges", "from", "0", "to", "100", ",", "with", "ticks", "at", "increments", "of", "10", ".", "On", "the", "horizontal", "axis", "are", "the", "labels", "for", "the", "WER", "accuracy", "levels", "in", "the", "study", ":", "Desktop", ",", "Cloud", ",", "and", "Human", ".", "In", "the", "WRAT-L", "facet", ",", "all", "pairwise", "comparisons", "are", "not", "significant", ",", "and", "the", "means", "are", "approximately", ":", "55", "for", "Desktop", ",", "60", "for", "Cloud", ",", "and", "65", "for", "Human", ".", "In", "the", "WRAT-M", "facet", ",", "the", "Desktop-Cloud", "and", "Desktop-Human", "pairwise", "comparison", "was", "significant", "with", "three", "stars", ",", "the", "Cloud-Human", "pairwise", "comparison", "had", "one", "star", ",", "and", "the", "means", "are", "approximately", ":", "55", "for", "Desktop", ",", "65", "for", "Cloud", ",", "and", "70", "for", "Human", ".", "In", "the", "WRAT-H", "facet", ",", "the", "Desktop-Cloud", "and", "Desktop-Human", "pairwise", "comparisons", "were", "significant", "at", "three", "stars", ",", "and", "the", "means", "are", "approximately", ":", "50", "for", "Desktop", ",", "65", "for", "Cloud", ",", "and", "75", "for", "Human", "."]}, "caption": {"raw": "Figure 6. H1-e for Evaluation of Accuracy (Numeric)", "tokens": ["Figure", "6", ".", "H1-e", "for", "Evaluation", "of", "Accuracy", "(", "Numeric", ")"]}, "context": {"raw": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels Figure 6. H1-e for Evaluation of Accuracy (Numeric)", "tokens": ["Methods", "for", "Evaluation", "of", "Imperfect", "Captioning", "Tools", "by", "Deaf", "or", "Hard-of-Hearing", "Users", "at", "Different", "Reading", "Literacy", "Levels", "Figure", "6", ".", "H1-e", "for", "Evaluation", "of", "Accuracy", "(", "Numeric", ")"]}, "filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_005.jpg", "orig_filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "split": "train"}, {"article_id": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "description": {"raw": "Trend of the AdoptRate of Smart-toggle in Experiment 4. The error bars in the graph indicate standard deviation.", "tokens": ["Trend", "of", "the", "AdoptRate", "of", "Smart-toggle", "in", "Experiment", "4", ".", "The", "error", "bars", "in", "the", "graph", "indicate", "standard", "deviation", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard ", "tokens": ["Diagnosing", "and", "Coping", "with", "Mode", "Errors", "in", "Korean-English", "Dual-language", "Keyboard"]}, "filename": "408a505662902bbaf20ef32ac6deaf7a78e52650_Image_022.jpg", "orig_filename": "408a505662902bbaf20ef32ac6deaf7a78e52650", "split": "train"}, {"article_id": "Helping students keep up with real-time captions by pausing and highlighting", "description": {"raw": "A bar graph of the comprehension scores is shown. It shows a plot of the baseline and tool scores. The tools improve the performance in both cases, but the difference between the highlighting tool and the baseline is larger than the different between the pausing tool and the baseline.", "tokens": ["A", "bar", "graph", "of", "the", "comprehension", "scores", "is", "shown", ".", "It", "shows", "a", "plot", "of", "the", "baseline", "and", "tool", "scores", ".", "The", "tools", "improve", "the", "performance", "in", "both", "cases", ",", "but", "the", "difference", "between", "the", "highlighting", "tool", "and", "the", "baseline", "is", "larger", "than", "the", "different", "between", "the", "pausing", "tool", "and", "the", "baseline", "."]}, "caption": {"raw": "Figure 8. The results of the comprehension tests used in our study with 95% conﬁdence intervals shown. There was a positive improvement seen in students scores using both the pausing and highlighting players (7.32% and 14.56% respectively), however, only the highlighting player was signiﬁcant (p < 0.001). The improvement seen between the pausing and highlighting players was also signiﬁcant (p < 0.01). While there is a difference in the score of the baseline as well, this effect was not signiﬁcant (p > 0.05).", "tokens": ["Figure", "8", ".", "The", "results", "of", "the", "comprehension", "tests", "used", "in", "our", "study", "with", "95", "%", "conﬁdence", "intervals", "shown", ".", "There", "was", "a", "positive", "improvement", "seen", "in", "students", "scores", "using", "both", "the", "pausing", "and", "highlighting", "players", "(", "7.32", "%", "and", "14.56", "%", "respectively", ")", ",", "however", ",", "only", "the", "highlighting", "player", "was", "signiﬁcant", "(", "p", "<", "0.001", ")", ".", "The", "improvement", "seen", "between", "the", "pausing", "and", "highlighting", "players", "was", "also", "signiﬁcant", "(", "p", "<", "0.01", ")", ".", "While", "there", "is", "a", "difference", "in", "the", "score", "of", "the", "baseline", "as", "well", ",", "this", "effect", "was", "not", "signiﬁcant", "(", "p", ">", "0.05", ")", "."]}, "context": {"raw": "Helping students keep up with real-time captions by pausing and highlighting Figure 8. The results of the comprehension tests used in our study with 95% conﬁdence intervals shown. There was a positive improvement seen in students scores using both the pausing and highlighting players (7.32% and 14.56% respectively), however, only the highlighting player was signiﬁcant (p < 0.001). The improvement seen between the pausing and highlighting players was also signiﬁcant (p < 0.01). While there is a difference in the score of the baseline as well, this effect was not signiﬁcant (p > 0.05).", "tokens": ["Helping", "students", "keep", "up", "with", "real-time", "captions", "by", "pausing", "and", "highlighting", "Figure", "8", ".", "The", "results", "of", "the", "comprehension", "tests", "used", "in", "our", "study", "with", "95", "%", "conﬁdence", "intervals", "shown", ".", "There", "was", "a", "positive", "improvement", "seen", "in", "students", "scores", "using", "both", "the", "pausing", "and", "highlighting", "players", "(", "7.32", "%", "and", "14.56", "%", "respectively", ")", ",", "however", ",", "only", "the", "highlighting", "player", "was", "signiﬁcant", "(", "p", "<", "0.001", ")", ".", "The", "improvement", "seen", "between", "the", "pausing", "and", "highlighting", "players", "was", "also", "signiﬁcant", "(", "p", "<", "0.01", ")", ".", "While", "there", "is", "a", "difference", "in", "the", "score", "of", "the", "baseline", "as", "well", ",", "this", "effect", "was", "not", "signiﬁcant", "(", "p", ">", "0.05", ")", "."]}, "filename": "1c039ecfbef9bc19146b87920a0561570ff72732_Image_009.jpg", "orig_filename": "1c039ecfbef9bc19146b87920a0561570ff72732", "split": "train"}, {"article_id": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments", "description": {"raw": "It shows four screenshots of AIGuide. The top-left screenshot shows the Selection Interface, which contains a list with three items: Fruit Bar, Lipton Iced Tea and Lucky Charms. The top-right shows the guidance interface, which contains two labels. One indicates that the item was found. Other indicates that the item is two feet away, 15 degrees left and 5 inches below the camera view. Also, it has a guide, confirm , exit and restart button. The bottom-left shows the user settings, which contains a toggle switch for camera access, two toggle switches to control haptic and sound feedback, a scrolling bar for the speaking rate and a submenu for the measuring system. The bottom right shows the tutorial interface, which contains a page number at the top, the description of that explains how the localization phase works, a button to play a demo, and two buttons to switch page.", "tokens": ["It", "shows", "four", "screenshots", "of", "AIGuide", ".", "The", "top-left", "screenshot", "shows", "the", "Selection", "Interface", ",", "which", "contains", "a", "list", "with", "three", "items", ":", "Fruit", "Bar", ",", "Lipton", "Iced", "Tea", "and", "Lucky", "Charms", ".", "The", "top-right", "shows", "the", "guidance", "interface", ",", "which", "contains", "two", "labels", ".", "One", "indicates", "that", "the", "item", "was", "found", ".", "Other", "indicates", "that", "the", "item", "is", "two", "feet", "away", ",", "15", "degrees", "left", "and", "5", "inches", "below", "the", "camera", "view", ".", "Also", ",", "it", "has", "a", "guide", ",", "confirm", ",", "exit", "and", "restart", "button", ".", "The", "bottom-left", "shows", "the", "user", "settings", ",", "which", "contains", "a", "toggle", "switch", "for", "camera", "access", ",", "two", "toggle", "switches", "to", "control", "haptic", "and", "sound", "feedback", ",", "a", "scrolling", "bar", "for", "the", "speaking", "rate", "and", "a", "submenu", "for", "the", "measuring", "system", ".", "The", "bottom", "right", "shows", "the", "tutorial", "interface", ",", "which", "contains", "a", "page", "number", "at", "the", "top", ",", "the", "description", "of", "that", "explains", "how", "the", "localization", "phase", "works", ",", "a", "button", "to", "play", "a", "demo", ",", "and", "two", "buttons", "to", "switch", "page", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments ", "tokens": ["AIGuide", ":", "An", "Augmented", "Reality", "Hand", "Guidance", "Application", "for", "People", "with", "Visual", "Impairments"]}, "filename": "9475d822749fd4754e46de81e21b28445e4f98a7_Image_003.jpg", "orig_filename": "9475d822749fd4754e46de81e21b28445e4f98a7", "split": "train"}, {"article_id": "MANA: Designing and Validating a User-Centered Mobility Analysis System", "description": {"raw": "This figure shows a graph of the sensor data from the three sensors. On the x-axis is time in seconds and on the y-axis is either acceleration (in metres per second squared) or rotation rate (in radians per second). All six channels of accelerometer and gyroscope data are shown. The data from the foot sensors is more periodic than the waist sensor, which appears noisier.", "tokens": ["This", "figure", "shows", "a", "graph", "of", "the", "sensor", "data", "from", "the", "three", "sensors", ".", "On", "the", "x-axis", "is", "time", "in", "seconds", "and", "on", "the", "y-axis", "is", "either", "acceleration", "(", "in", "metres", "per", "second", "squared", ")", "or", "rotation", "rate", "(", "in", "radians", "per", "second", ")", ".", "All", "six", "channels", "of", "accelerometer", "and", "gyroscope", "data", "are", "shown", ".", "The", "data", "from", "the", "foot", "sensors", "is", "more", "periodic", "than", "the", "waist", "sensor", ",", "which", "appears", "noisier", "."]}, "caption": {"raw": "Figure 6. Sensor locations on the body, and the transformation between sensor body and global reference frames.", "tokens": ["Figure", "6", ".", "Sensor", "locations", "on", "the", "body", ",", "and", "the", "transformation", "between", "sensor", "body", "and", "global", "reference", "frames", "."]}, "context": {"raw": "MANA: Designing and Validating a User-Centered Mobility Analysis System Figure 6. Sensor locations on the body, and the transformation between sensor body and global reference frames.", "tokens": ["MANA", ":", "Designing", "and", "Validating", "a", "User-Centered", "Mobility", "Analysis", "System", "Figure", "6", ".", "Sensor", "locations", "on", "the", "body", ",", "and", "the", "transformation", "between", "sensor", "body", "and", "global", "reference", "frames", "."]}, "filename": "2a6f6d7a0996d638a88f5842f4d31574354ece8f_Image_007.jpg", "orig_filename": "2a6f6d7a0996d638a88f5842f4d31574354ece8f", "split": "train"}, {"article_id": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels", "description": {"raw": "A bar graph titled Figure 13: H2-e for Evaluation of Accuracy (Numeric). The vertical axis ranges from 0 to 100, with ticks at increments of 10. On the horizontal axis are the labels for the WRAT accuracy levels in the study: WRAT-L, WRAT-M, and WRAT-H. All of the pairwise comparisons were not significant. Please refer the subsection on the current page for the means and standard errors for the WRAT levels.", "tokens": ["A", "bar", "graph", "titled", "Figure", "13", ":", "H2-e", "for", "Evaluation", "of", "Accuracy", "(", "Numeric", ")", ".", "The", "vertical", "axis", "ranges", "from", "0", "to", "100", ",", "with", "ticks", "at", "increments", "of", "10", ".", "On", "the", "horizontal", "axis", "are", "the", "labels", "for", "the", "WRAT", "accuracy", "levels", "in", "the", "study", ":", "WRAT-L", ",", "WRAT-M", ",", "and", "WRAT-H.", "All", "of", "the", "pairwise", "comparisons", "were", "not", "significant", ".", "Please", "refer", "the", "subsection", "on", "the", "current", "page", "for", "the", "means", "and", "standard", "errors", "for", "the", "WRAT", "levels", "."]}, "caption": {"raw": "Figure 13. H2-e for Evaluation of Accuracy (Numeric)", "tokens": ["Figure", "13", ".", "H2-e", "for", "Evaluation", "of", "Accuracy", "(", "Numeric", ")"]}, "context": {"raw": "Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels Figure 13. H2-e for Evaluation of Accuracy (Numeric)", "tokens": ["Methods", "for", "Evaluation", "of", "Imperfect", "Captioning", "Tools", "by", "Deaf", "or", "Hard-of-Hearing", "Users", "at", "Different", "Reading", "Literacy", "Levels", "Figure", "13", ".", "H2-e", "for", "Evaluation", "of", "Accuracy", "(", "Numeric", ")"]}, "filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095_Image_009.jpg", "orig_filename": "a1dd85f22c7e9b2a4d3f403ca105c839b5303095", "split": "train"}, {"article_id": "PersonalTouch: Improving Touchscreen Usability by Personalizing Accessibility Settings based on Individual User's Touchscreen Interaction", "description": {"raw": "Figure 2: Touch input tasks for collecting individual touch data. The tasks are designed to collect each of the 6 standard gestures supported by Android and iOS: (a) tap and long press, (b) swipe, (c) horizontal scroll, (d) vertical scroll, (e) pinch, and (f) rotate.", "tokens": ["Figure", "2", ":", "Touch", "input", "tasks", "for", "collecting", "individual", "touch", "data", ".", "The", "tasks", "are", "designed", "to", "collect", "each", "of", "the", "6", "standard", "gestures", "supported", "by", "Android", "and", "iOS", ":", "(", "a", ")", "tap", "and", "long", "press", ",", "(", "b", ")", "swipe", ",", "(", "c", ")", "horizontal", "scroll", ",", "(", "d", ")", "vertical", "scroll", ",", "(", "e", ")", "pinch", ",", "and", "(", "f", ")", "rotate", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "PersonalTouch: Improving Touchscreen Usability by Personalizing Accessibility Settings based on Individual User's Touchscreen Interaction ", "tokens": ["PersonalTouch", ":", "Improving", "Touchscreen", "Usability", "by", "Personalizing", "Accessibility", "Settings", "based", "on", "Individual", "User", "'s", "Touchscreen", "Interaction"]}, "filename": "2920de9be8d55f3c454d677d92cb6b3ebf8a4222_Image_003.png", "orig_filename": "2920de9be8d55f3c454d677d92cb6b3ebf8a4222", "split": "train"}, {"article_id": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "description": {"raw": "A box plot displaying the F2 Mean across each condition, with units in Hertz. First results are presented for the 9 participants who spoke in all three conditions, with median value of 2510.066 for Markup, 2707.750 for No ASR, and 2577.583 for ASR.  There were significant differences between Markup and No ASR, as well as ASR and No ASR.  Next, results are shown for all 12 participants, with median value of 2524.027 for Markup and 2594.337 for ASR.  There was no significant difference between these two conditions.", "tokens": ["A", "box", "plot", "displaying", "the", "F2", "Mean", "across", "each", "condition", ",", "with", "units", "in", "Hertz", ".", "First", "results", "are", "presented", "for", "the", "9", "participants", "who", "spoke", "in", "all", "three", "conditions", ",", "with", "median", "value", "of", "2510.066", "for", "Markup", ",", "2707.750", "for", "No", "ASR", ",", "and", "2577.583", "for", "ASR", ".", "There", "were", "significant", "differences", "between", "Markup", "and", "No", "ASR", ",", "as", "well", "as", "ASR", "and", "No", "ASR", ".", "Next", ",", "results", "are", "shown", "for", "all", "12", "participants", ",", "with", "median", "value", "of", "2524.027", "for", "Markup", "and", "2594.337", "for", "ASR", ".", "There", "was", "no", "significant", "difference", "between", "these", "two", "conditions", "."]}, "caption": {"raw": "Figure 7: Box plots for F2 formant means across conditions", "tokens": ["Figure", "7", ":", "Box", "plots", "for", "F2", "formant", "means", "across", "conditions"]}, "context": {"raw": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers Figure 7: Box plots for F2 formant means across conditions", "tokens": ["Behavioral", "Changes", "in", "Speakers", "who", "are", "Automatically", "Captioned", "in", "Meetings", "with", "Deaf", "or", "Hard-of-Hearing", "Peers", "Figure", "7", ":", "Box", "plots", "for", "F2", "formant", "means", "across", "conditions"]}, "filename": "4cecd70a9e46a761774a54ed11d613b33721b95d_Image_010.gif", "orig_filename": "4cecd70a9e46a761774a54ed11d613b33721b95d", "split": "train"}, {"article_id": "Context-Based Interface Prototyping and Evaluation for (Shared) Autonomous Vehicles Using a Lightweight Immersive Video-Based Simulator", "description": {"raw": "Radar plot displaying means and standarddeviations of the four IPQ subscales for both conditions of the user study.", "tokens": ["Radar", "plot", "displaying", "means", "and", "standarddeviations", "of", "the", "four", "IPQ", "subscales", "for", "both", "conditions", "of", "the", "user", "study", "."]}, "caption": {"raw": "Figure 6. Means (SD) of the IPQ [38, 39] subscales [from 0 = low to", "tokens": ["Figure", "6", ".", "Means", "(", "SD", ")", "of", "the", "IPQ", "[", "38", ",", "39", "]", "subscales", "[", "from", "0", "=", "low", "to"]}, "context": {"raw": "Context-Based Interface Prototyping and Evaluation for (Shared) Autonomous Vehicles Using a Lightweight Immersive Video-Based Simulator Figure 6. Means (SD) of the IPQ [38, 39] subscales [from 0 = low to", "tokens": ["Context-Based", "Interface", "Prototyping", "and", "Evaluation", "for", "(", "Shared", ")", "Autonomous", "Vehicles", "Using", "a", "Lightweight", "Immersive", "Video-Based", "Simulator", "Figure", "6", ".", "Means", "(", "SD", ")", "of", "the", "IPQ", "[", "38", ",", "39", "]", "subscales", "[", "from", "0", "=", "low", "to"]}, "filename": "0677defc03e335e85f83f6027d0b3bad8d802ae4_Image_006.jpg", "orig_filename": "0677defc03e335e85f83f6027d0b3bad8d802ae4", "split": "train"}, {"article_id": "Nonvisual Interaction Techniques at the Keyboard Surface", "description": {"raw": "The graph shows that our system, Sprites peorformed 3x times better in task completion rate.", "tokens": ["The", "graph", "shows", "that", "our", "system", ",", "Sprites", "peorformed", "3x", "times", "better", "in", "task", "completion", "rate", "."]}, "caption": {"raw": "Figure 9: Graph showing task completion rates for different kinds of tasks in our user study", "tokens": ["Figure", "9", ":", "Graph", "showing", "task", "completion", "rates", "for", "different", "kinds", "of", "tasks", "in", "our", "user", "study"]}, "context": {"raw": "Nonvisual Interaction Techniques at the Keyboard Surface Figure 9: Graph showing task completion rates for different kinds of tasks in our user study", "tokens": ["Nonvisual", "Interaction", "Techniques", "at", "the", "Keyboard", "Surface", "Figure", "9", ":", "Graph", "showing", "task", "completion", "rates", "for", "different", "kinds", "of", "tasks", "in", "our", "user", "study"]}, "filename": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65_Image_009.png", "orig_filename": "5fde0fa5fe2968ce9d50b52e7cd2b3aa9fc0ab65", "split": "train"}, {"article_id": "Understanding the Role of Technology to Support Breastfeeding", "description": {"raw": "A bar chart illustrating the distribution of the IBM constructs on 5-point Likert scales. For attitude, breastfeeding parents responded strongly agree(59.95%). Bottle-feeding parents responded strongly disagree(30.56%), strongly agree(33.33%). Partners responded strongly agree(35.29%). Parents-to-be responded neither agree nor disagree(33.33%), strongly agree(33.33%). For norm, breastfeeding parents responded strongly disagree(27.32%), disagree(27.32%). Bottle-feeding parents responded strongly disagree(25%) and strongly agree(25%). Partners responded neither agree nor disagree(23.53%), agree(32.94%). Parents-to-be responded neither agree nor disagree(43%). For personal agency, breastfeeding parents responded agree(23.22%), strongly agree(35.95%). Bottle-feeding parents responded strongly disagree(39.58%). Partners responded disagree(25%), agree(25%) and strongly agree(21.32%). Parents-to-be responded neither agree nor disagree(32.14%). For environment, breastfeeding parents responded neither agree nor disagree(25.1%), responded agree(23.28%). Bottle-feeding parents responded neither agree nor disagree(27.08%), strongly agree(27.08%). Partners responded neither agree nor disagree(26.94%), agree(25%). Parents-to-be responded neither agree nor disagree(47.5%). For knowledge, breastfeeding parent responded strongly agree(38.88%). Bottle-feeding parents responded agree(23.33%), strongly agree(36.67%). Partners responded agree(33.05%), strongly agree(46.39%). Parents-to-be responded neither agree nor disagree(28.33%), agree(33.33%). For salience, breastfeeding parents responded strongly agree(59.79%). Bottle-feeding parents responded agree(33.33%), strongly agree(33.33%). Partners responded strongly agree(41.18%). Parents-to-be responded strongly agree(40%). For habit, Breastfeeding parents responded agree(30.65%), strongly agree(46.77%). Bottle-feeding parents responded strongly agree(58.33%). Partners responded agree(47.06%).", "tokens": ["A", "bar", "chart", "illustrating", "the", "distribution", "of", "the", "IBM", "constructs", "on", "5-point", "Likert", "scales", ".", "For", "attitude", ",", "breastfeeding", "parents", "responded", "strongly", "agree", "(", "59.95", "%", ")", ".", "Bottle-feeding", "parents", "responded", "strongly", "disagree", "(", "30.56", "%", ")", ",", "strongly", "agree", "(", "33.33", "%", ")", ".", "Partners", "responded", "strongly", "agree", "(", "35.29", "%", ")", ".", "Parents-to-be", "responded", "neither", "agree", "nor", "disagree", "(", "33.33", "%", ")", ",", "strongly", "agree", "(", "33.33", "%", ")", ".", "For", "norm", ",", "breastfeeding", "parents", "responded", "strongly", "disagree", "(", "27.32", "%", ")", ",", "disagree", "(", "27.32", "%", ")", ".", "Bottle-feeding", "parents", "responded", "strongly", "disagree", "(", "25", "%", ")", "and", "strongly", "agree", "(", "25", "%", ")", ".", "Partners", "responded", "neither", "agree", "nor", "disagree", "(", "23.53", "%", ")", ",", "agree", "(", "32.94", "%", ")", ".", "Parents-to-be", "responded", "neither", "agree", "nor", "disagree", "(", "43", "%", ")", ".", "For", "personal", "agency", ",", "breastfeeding", "parents", "responded", "agree", "(", "23.22", "%", ")", ",", "strongly", "agree", "(", "35.95", "%", ")", ".", "Bottle-feeding", "parents", "responded", "strongly", "disagree", "(", "39.58", "%", ")", ".", "Partners", "responded", "disagree", "(", "25", "%", ")", ",", "agree", "(", "25", "%", ")", "and", "strongly", "agree", "(", "21.32", "%", ")", ".", "Parents-to-be", "responded", "neither", "agree", "nor", "disagree", "(", "32.14", "%", ")", ".", "For", "environment", ",", "breastfeeding", "parents", "responded", "neither", "agree", "nor", "disagree", "(", "25.1", "%", ")", ",", "responded", "agree", "(", "23.28", "%", ")", ".", "Bottle-feeding", "parents", "responded", "neither", "agree", "nor", "disagree", "(", "27.08", "%", ")", ",", "strongly", "agree", "(", "27.08", "%", ")", ".", "Partners", "responded", "neither", "agree", "nor", "disagree", "(", "26.94", "%", ")", ",", "agree", "(", "25", "%", ")", ".", "Parents-to-be", "responded", "neither", "agree", "nor", "disagree", "(", "47.5", "%", ")", ".", "For", "knowledge", ",", "breastfeeding", "parent", "responded", "strongly", "agree", "(", "38.88", "%", ")", ".", "Bottle-feeding", "parents", "responded", "agree", "(", "23.33", "%", ")", ",", "strongly", "agree", "(", "36.67", "%", ")", ".", "Partners", "responded", "agree", "(", "33.05", "%", ")", ",", "strongly", "agree", "(", "46.39", "%", ")", ".", "Parents-to-be", "responded", "neither", "agree", "nor", "disagree", "(", "28.33", "%", ")", ",", "agree", "(", "33.33", "%", ")", ".", "For", "salience", ",", "breastfeeding", "parents", "responded", "strongly", "agree", "(", "59.79", "%", ")", ".", "Bottle-feeding", "parents", "responded", "agree", "(", "33.33", "%", ")", ",", "strongly", "agree", "(", "33.33", "%", ")", ".", "Partners", "responded", "strongly", "agree", "(", "41.18", "%", ")", ".", "Parents-to-be", "responded", "strongly", "agree", "(", "40", "%", ")", ".", "For", "habit", ",", "Breastfeeding", "parents", "responded", "agree", "(", "30.65", "%", ")", ",", "strongly", "agree", "(", "46.77", "%", ")", ".", "Bottle-feeding", "parents", "responded", "strongly", "agree", "(", "58.33", "%", ")", ".", "Partners", "responded", "agree", "(", "47.06", "%", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Understanding the Role of Technology to Support Breastfeeding ", "tokens": ["Understanding", "the", "Role", "of", "Technology", "to", "Support", "Breastfeeding"]}, "filename": "79b8dcb17db71e75b3401e6743e194cf0943b960_Image_004.jpg", "orig_filename": "79b8dcb17db71e75b3401e6743e194cf0943b960", "split": "train"}, {"article_id": "Revisiting Blind Photography in the Context of Teachable Object Recognizers", "description": {"raw": "This figure has two bar charts at the top and bottom. The bar chart at the top describes the proportion of participants who use a certain technology never, once a month, several times a month, once a week, several times a week, once a day, and several times a day as follows.  Experience of using a mobile device Never: 0.0% Once a month: 0.0% Several times a month: 0.0% Once a week: 0.0% Several times a week: 0.0% Once a day: 11.11% Several times a day: 88.89%  Experience of taking pictures using a mobile phone Never: 44.44% Once a month: 22.22% Several times a month: 0.0% Once a week: 0.0% Several times a week: 22.22% Once a day: 0.0% Several times a day: 11.11%  Experience of sharing your own photos or videos with others Never: 66.67% Once a month: 0.0% Several times a month: 22.22% Once a week: 0.0% Several times a week: 11.11% Once a day: 0.0% Several times a day: 0.0%  Experience of using apps (for any purpose) on a mobile phone Never: 0.0% Once a month: 0.0% Several times a month: 0.0% Once a week: 0.0% Several times a week: 0.0% Once a day: 22.22% Several times a day: 77.78%  Experience of using apps for object recognition (for example, Aipoly, TapTapSee) on a mobile phone Never: 44.44% Once a month: 0.0% Several times a month: 0.0% Once a week: 11.11% Several times a week: 11.11% Once a day: 0.0% Several times a day: 33.33%  Experience of using Braille labels to distinguish objects Never: 22.22% Once a month: 0.0% Several times a month: 11.11% Once a week: 0.0% Several times a week: 22.22% Once a day: 0.0% Several times a day: 44.44%  The bar chart at the bottom describes the proportion of participants who strongly disagree, disagree, neither agree nor disagree (neutral), agree, strongly agree with a statement about technology as follows.  Statement: I enjoy taking a photo on a mobile phone. Strongly disagree: 0.0% Disagree: 44.44% Neutral: 11.11% Agree: 22.22% Strongly agree: 22.22%  Statement: I think it is important to keep up with the latest trends in te", "tokens": ["This", "figure", "has", "two", "bar", "charts", "at", "the", "top", "and", "bottom", ".", "The", "bar", "chart", "at", "the", "top", "describes", "the", "proportion", "of", "participants", "who", "use", "a", "certain", "technology", "never", ",", "once", "a", "month", ",", "several", "times", "a", "month", ",", "once", "a", "week", ",", "several", "times", "a", "week", ",", "once", "a", "day", ",", "and", "several", "times", "a", "day", "as", "follows", ".", "Experience", "of", "using", "a", "mobile", "device", "Never", ":", "0.0", "%", "Once", "a", "month", ":", "0.0", "%", "Several", "times", "a", "month", ":", "0.0", "%", "Once", "a", "week", ":", "0.0", "%", "Several", "times", "a", "week", ":", "0.0", "%", "Once", "a", "day", ":", "11.11", "%", "Several", "times", "a", "day", ":", "88.89", "%", "Experience", "of", "taking", "pictures", "using", "a", "mobile", "phone", "Never", ":", "44.44", "%", "Once", "a", "month", ":", "22.22", "%", "Several", "times", "a", "month", ":", "0.0", "%", "Once", "a", "week", ":", "0.0", "%", "Several", "times", "a", "week", ":", "22.22", "%", "Once", "a", "day", ":", "0.0", "%", "Several", "times", "a", "day", ":", "11.11", "%", "Experience", "of", "sharing", "your", "own", "photos", "or", "videos", "with", "others", "Never", ":", "66.67", "%", "Once", "a", "month", ":", "0.0", "%", "Several", "times", "a", "month", ":", "22.22", "%", "Once", "a", "week", ":", "0.0", "%", "Several", "times", "a", "week", ":", "11.11", "%", "Once", "a", "day", ":", "0.0", "%", "Several", "times", "a", "day", ":", "0.0", "%", "Experience", "of", "using", "apps", "(", "for", "any", "purpose", ")", "on", "a", "mobile", "phone", "Never", ":", "0.0", "%", "Once", "a", "month", ":", "0.0", "%", "Several", "times", "a", "month", ":", "0.0", "%", "Once", "a", "week", ":", "0.0", "%", "Several", "times", "a", "week", ":", "0.0", "%", "Once", "a", "day", ":", "22.22", "%", "Several", "times", "a", "day", ":", "77.78", "%", "Experience", "of", "using", "apps", "for", "object", "recognition", "(", "for", "example", ",", "Aipoly", ",", "TapTapSee", ")", "on", "a", "mobile", "phone", "Never", ":", "44.44", "%", "Once", "a", "month", ":", "0.0", "%", "Several", "times", "a", "month", ":", "0.0", "%", "Once", "a", "week", ":", "11.11", "%", "Several", "times", "a", "week", ":", "11.11", "%", "Once", "a", "day", ":", "0.0", "%", "Several", "times", "a", "day", ":", "33.33", "%", "Experience", "of", "using", "Braille", "labels", "to", "distinguish", "objects", "Never", ":", "22.22", "%", "Once", "a", "month", ":", "0.0", "%", "Several", "times", "a", "month", ":", "11.11", "%", "Once", "a", "week", ":", "0.0", "%", "Several", "times", "a", "week", ":", "22.22", "%", "Once", "a", "day", ":", "0.0", "%", "Several", "times", "a", "day", ":", "44.44", "%", "The", "bar", "chart", "at", "the", "bottom", "describes", "the", "proportion", "of", "participants", "who", "strongly", "disagree", ",", "disagree", ",", "neither", "agree", "nor", "disagree", "(", "neutral", ")", ",", "agree", ",", "strongly", "agree", "with", "a", "statement", "about", "technology", "as", "follows", ".", "Statement", ":", "I", "enjoy", "taking", "a", "photo", "on", "a", "mobile", "phone", ".", "Strongly", "disagree", ":", "0.0", "%", "Disagree", ":", "44.44", "%", "Neutral", ":", "11.11", "%", "Agree", ":", "22.22", "%", "Strongly", "agree", ":", "22.22", "%", "Statement", ":", "I", "think", "it", "is", "important", "to", "keep", "up", "with", "the", "latest", "trends", "in", "te"]}, "caption": {"raw": "Figure 4: Technology experience and attitude responses. All participants have smartphones; more than half are using apps for object recognition; and all are positive about technology.", "tokens": ["Figure", "4", ":", "Technology", "experience", "and", "attitude", "responses", ".", "All", "participants", "have", "smartphones", ";", "more", "than", "half", "are", "using", "apps", "for", "object", "recognition", ";", "and", "all", "are", "positive", "about", "technology", "."]}, "context": {"raw": "Revisiting Blind Photography in the Context of Teachable Object Recognizers Figure 4: Technology experience and attitude responses. All participants have smartphones; more than half are using apps for object recognition; and all are positive about technology.", "tokens": ["Revisiting", "Blind", "Photography", "in", "the", "Context", "of", "Teachable", "Object", "Recognizers", "Figure", "4", ":", "Technology", "experience", "and", "attitude", "responses", ".", "All", "participants", "have", "smartphones", ";", "more", "than", "half", "are", "using", "apps", "for", "object", "recognition", ";", "and", "all", "are", "positive", "about", "technology", "."]}, "filename": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0_Image_021.png", "orig_filename": "854f7ee708fdb78943c7b67dcf8f3b786d94b9b0", "split": "train"}, {"article_id": "UbiBraille: designing and evaluating a vibrotactile Braille-reading device", "description": {"raw": "Bar chart representing word recognition accuracy per condition. 4000ms condition - 93%, 2000ms condition - 89%, 1000ms condition - 64%, 500ms condition - 33%. Accuracy decreases with demand of condition. Clearly there are no statistically significant differences between the first and second conditions.", "tokens": ["Bar", "chart", "representing", "word", "recognition", "accuracy", "per", "condition", ".", "4000ms", "condition", "-", "93", "%", ",", "2000ms", "condition", "-", "89", "%", ",", "1000ms", "condition", "-", "64", "%", ",", "500ms", "condition", "-", "33", "%", ".", "Accuracy", "decreases", "with", "demand", "of", "condition", ".", "Clearly", "there", "are", "no", "statistically", "significant", "differences", "between", "the", "first", "and", "second", "conditions", "."]}, "caption": {"raw": "Figure 5. Word recognition accuracy per duration condition.", "tokens": ["Figure", "5", ".", "Word", "recognition", "accuracy", "per", "duration", "condition", "."]}, "context": {"raw": "UbiBraille: designing and evaluating a vibrotactile Braille-reading device Figure 5. Word recognition accuracy per duration condition.", "tokens": ["UbiBraille", ":", "designing", "and", "evaluating", "a", "vibrotactile", "Braille-reading", "device", "Figure", "5", ".", "Word", "recognition", "accuracy", "per", "duration", "condition", "."]}, "filename": "39dcd4c585a6916a434dffd5cebfd70e97581dea_Image_016.gif", "orig_filename": "39dcd4c585a6916a434dffd5cebfd70e97581dea", "split": "train"}, {"article_id": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "description": {"raw": "A bar graph comparing the motion of the head and controller at 25, 50 and 75 degrees. The controller in all cases moves significantly more than the HMD", "tokens": ["A", "bar", "graph", "comparing", "the", "motion", "of", "the", "head", "and", "controller", "at", "25", ",", "50", "and", "75", "degrees", ".", "The", "controller", "in", "all", "cases", "moves", "significantly", "more", "than", "the", "HMD"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR ", "tokens": ["Head-Coupled", "Kinematic", "Template", "Matching", ":", "A", "Prediction", "Model", "for", "Ray", "Pointing", "in", "VR"]}, "filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_010.gif", "orig_filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "split": "train"}, {"article_id": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery", "description": {"raw": "A graph showing precision-recall curves for overall labeling performance, and performance for each of the four types of labels. Curb ramp performance is best, with performance for missing ramps, obstructions, and surface problems all being somewhat worse.", "tokens": ["A", "graph", "showing", "precision-recall", "curves", "for", "overall", "labeling", "performance", ",", "and", "performance", "for", "each", "of", "the", "four", "types", "of", "labels", ".", "Curb", "ramp", "performance", "is", "best", ",", "with", "performance", "for", "missing", "ramps", ",", "obstructions", ",", "and", "surface", "problems", "all", "being", "somewhat", "worse", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery ", "tokens": ["Deep", "Learning", "for", "Automatically", "Detecting", "Sidewalk", "Accessibility", "Problems", "Using", "Streetscape", "Imagery"]}, "filename": "48fc79059aecbe211fe15aaca6c96d2dff7650fc_Image_012.jpg", "orig_filename": "48fc79059aecbe211fe15aaca6c96d2dff7650fc", "split": "train"}, {"article_id": "'Wow! You're Wearing a Fitbit, You're a Young Boy Now!\": Socio-Technical Aspirations for Children with Autism in India", "description": {"raw": "This is a scatter plot of the Fitbit data for 6 participants with gender (color of point-box), totoal number of days the firbit worn in the x axis, and average steps per day in the y-axis. Data is as follows 1. Female, 4 days, apprx 9000 steps per day 2. Male, 5 days, a little less than 9000 steps a day 3. Male, 6 days, 2500 steps a day 4. Female, 7 days, 7500 steps a day 5. Male, 14 days, a littl emore than 6000 steps a day 6. Female, 16 days, a littl emore than 8000 steps a day", "tokens": ["This", "is", "a", "scatter", "plot", "of", "the", "Fitbit", "data", "for", "6", "participants", "with", "gender", "(", "color", "of", "point-box", ")", ",", "totoal", "number", "of", "days", "the", "firbit", "worn", "in", "the", "x", "axis", ",", "and", "average", "steps", "per", "day", "in", "the", "y-axis", ".", "Data", "is", "as", "follows", "1", ".", "Female", ",", "4", "days", ",", "apprx", "9000", "steps", "per", "day", "2", ".", "Male", ",", "5", "days", ",", "a", "little", "less", "than", "9000", "steps", "a", "day", "3", ".", "Male", ",", "6", "days", ",", "2500", "steps", "a", "day", "4", ".", "Female", ",", "7", "days", ",", "7500", "steps", "a", "day", "5", ".", "Male", ",", "14", "days", ",", "a", "littl", "emore", "than", "6000", "steps", "a", "day", "6", ".", "Female", ",", "16", "days", ",", "a", "littl", "emore", "than", "8000", "steps", "a", "day"]}, "caption": {"raw": "Figure 2: Fitbit data for the six participants", "tokens": ["Figure", "2", ":", "Fitbit", "data", "for", "the", "six", "participants"]}, "context": {"raw": "'Wow! You're Wearing a Fitbit, You're a Young Boy Now!\": Socio-Technical Aspirations for Children with Autism in India Figure 2: Fitbit data for the six participants", "tokens": ["'Wow", "!", "You", "'re", "Wearing", "a", "Fitbit", ",", "You", "'re", "a", "Young", "Boy", "Now", "!", "``", ":", "Socio-Technical", "Aspirations", "for", "Children", "with", "Autism", "in", "India", "Figure", "2", ":", "Fitbit", "data", "for", "the", "six", "participants"]}, "filename": "108a29b3e76f0b2f375ddabb46b44b436bdee7b3_Image_002.png", "orig_filename": "108a29b3e76f0b2f375ddabb46b44b436bdee7b3", "split": "train"}, {"article_id": "Characterizing Human vs. Automated Coaching: Preliminary Results", "description": {"raw": "This violin plot shows a similar side-by-side distribution of the response rate to messages between the \"human coach\" and \"wizard-of-oz\" groups as in Figure 3. On the left, the density for the \"human coach\" group centered on a higher point, the mean of 71%. On the right, the plot for the \"woz\" group is relatively unchanged from how it appeared in Figure 3, with the mass of the distribution centered on the mean response rate of 54%.", "tokens": ["This", "violin", "plot", "shows", "a", "similar", "side-by-side", "distribution", "of", "the", "response", "rate", "to", "messages", "between", "the", "``", "human", "coach", "''", "and", "``", "wizard-of-oz", "''", "groups", "as", "in", "Figure", "3", ".", "On", "the", "left", ",", "the", "density", "for", "the", "``", "human", "coach", "''", "group", "centered", "on", "a", "higher", "point", ",", "the", "mean", "of", "71", "%", ".", "On", "the", "right", ",", "the", "plot", "for", "the", "``", "woz", "''", "group", "is", "relatively", "unchanged", "from", "how", "it", "appeared", "in", "Figure", "3", ",", "with", "the", "mass", "of", "the", "distribution", "centered", "on", "the", "mean", "response", "rate", "of", "54", "%", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Characterizing Human vs. Automated Coaching: Preliminary Results ", "tokens": ["Characterizing", "Human", "vs", ".", "Automated", "Coaching", ":", "Preliminary", "Results"]}, "filename": "3dad3a747463969ccf7aa054e005afa71bb3d2a9_Image_003.jpg", "orig_filename": "3dad3a747463969ccf7aa054e005afa71bb3d2a9", "split": "train"}, {"article_id": "Designing an Adaptive Web Navigation Interface for Users with Variable Pointing Performance", "description": {"raw": "Five images A-E of showing examples of the notification and activation options presented to participants in the study.  Image A displays a browser window with a red bar under the URL bar.  Image B displays a browser window with a bar under the URL that states \"the webpage you are currently browsing seems to have small, hard to click targets!  Would you like the page to be zoomed in for a more enjoyable browsing experience?\".  The bar also includes in the right corner two buttons.  The first has the label \"Adapt my UI\" and the second button has the label \"Dismiss\".  Image C displays a webpage with a pop-up window in the center.  The pop-up window includes text that states  \"the webpage you are currently browsing seems to have small, hard to click targets!  Would you like the page to be zoomed in for a more enjoyable browsing experience?\".  The window also includes in the two buttons.  The first has the label \"Adapt my UI\" and the second button has the label \"Dismiss\".  Image D displays a Wikipedia webpage with text describing the University of Maryland, Baltimore County.  The webpage is displayed at 100% which is the default page viewing size.  Image E displays a Wikipedia webpage with text describing the University of Maryland, Baltimore County.  The webpage is displayed at 150% and has been zoomed in to increase the text size.", "tokens": ["Five", "images", "A-E", "of", "showing", "examples", "of", "the", "notification", "and", "activation", "options", "presented", "to", "participants", "in", "the", "study", ".", "Image", "A", "displays", "a", "browser", "window", "with", "a", "red", "bar", "under", "the", "URL", "bar", ".", "Image", "B", "displays", "a", "browser", "window", "with", "a", "bar", "under", "the", "URL", "that", "states", "``", "the", "webpage", "you", "are", "currently", "browsing", "seems", "to", "have", "small", ",", "hard", "to", "click", "targets", "!", "Would", "you", "like", "the", "page", "to", "be", "zoomed", "in", "for", "a", "more", "enjoyable", "browsing", "experience", "?", "''", ".", "The", "bar", "also", "includes", "in", "the", "right", "corner", "two", "buttons", ".", "The", "first", "has", "the", "label", "``", "Adapt", "my", "UI", "''", "and", "the", "second", "button", "has", "the", "label", "``", "Dismiss", "''", ".", "Image", "C", "displays", "a", "webpage", "with", "a", "pop-up", "window", "in", "the", "center", ".", "The", "pop-up", "window", "includes", "text", "that", "states", "``", "the", "webpage", "you", "are", "currently", "browsing", "seems", "to", "have", "small", ",", "hard", "to", "click", "targets", "!", "Would", "you", "like", "the", "page", "to", "be", "zoomed", "in", "for", "a", "more", "enjoyable", "browsing", "experience", "?", "''", ".", "The", "window", "also", "includes", "in", "the", "two", "buttons", ".", "The", "first", "has", "the", "label", "``", "Adapt", "my", "UI", "''", "and", "the", "second", "button", "has", "the", "label", "``", "Dismiss", "''", ".", "Image", "D", "displays", "a", "Wikipedia", "webpage", "with", "text", "describing", "the", "University", "of", "Maryland", ",", "Baltimore", "County", ".", "The", "webpage", "is", "displayed", "at", "100", "%", "which", "is", "the", "default", "page", "viewing", "size", ".", "Image", "E", "displays", "a", "Wikipedia", "webpage", "with", "text", "describing", "the", "University", "of", "Maryland", ",", "Baltimore", "County", ".", "The", "webpage", "is", "displayed", "at", "150", "%", "and", "has", "been", "zoomed", "in", "to", "increase", "the", "text", "size", "."]}, "caption": {"raw": "Figure 1. Notification and Activation Options: (A) The Bar notification appears as a thin horizontal bar under the URL field. (B) The Bar+ notification appears on as a horizontal box under the URL field with text and interactive buttons. (C) The Dialog Box appears as a popup alert with text and interactive buttons. (D) Wikipedia page zoomed at 100% (default viewing size). (E) Wikipedia page at 150% viewing size after manually zooming in.", "tokens": ["Figure", "1", ".", "Notification", "and", "Activation", "Options", ":", "(", "A", ")", "The", "Bar", "notification", "appears", "as", "a", "thin", "horizontal", "bar", "under", "the", "URL", "field", ".", "(", "B", ")", "The", "Bar+", "notification", "appears", "on", "as", "a", "horizontal", "box", "under", "the", "URL", "field", "with", "text", "and", "interactive", "buttons", ".", "(", "C", ")", "The", "Dialog", "Box", "appears", "as", "a", "popup", "alert", "with", "text", "and", "interactive", "buttons", ".", "(", "D", ")", "Wikipedia", "page", "zoomed", "at", "100", "%", "(", "default", "viewing", "size", ")", ".", "(", "E", ")", "Wikipedia", "page", "at", "150", "%", "viewing", "size", "after", "manually", "zooming", "in", "."]}, "context": {"raw": "Designing an Adaptive Web Navigation Interface for Users with Variable Pointing Performance Figure 1. Notification and Activation Options: (A) The Bar notification appears as a thin horizontal bar under the URL field. (B) The Bar+ notification appears on as a horizontal box under the URL field with text and interactive buttons. (C) The Dialog Box appears as a popup alert with text and interactive buttons. (D) Wikipedia page zoomed at 100% (default viewing size). (E) Wikipedia page at 150% viewing size after manually zooming in.", "tokens": ["Designing", "an", "Adaptive", "Web", "Navigation", "Interface", "for", "Users", "with", "Variable", "Pointing", "Performance", "Figure", "1", ".", "Notification", "and", "Activation", "Options", ":", "(", "A", ")", "The", "Bar", "notification", "appears", "as", "a", "thin", "horizontal", "bar", "under", "the", "URL", "field", ".", "(", "B", ")", "The", "Bar+", "notification", "appears", "on", "as", "a", "horizontal", "box", "under", "the", "URL", "field", "with", "text", "and", "interactive", "buttons", ".", "(", "C", ")", "The", "Dialog", "Box", "appears", "as", "a", "popup", "alert", "with", "text", "and", "interactive", "buttons", ".", "(", "D", ")", "Wikipedia", "page", "zoomed", "at", "100", "%", "(", "default", "viewing", "size", ")", ".", "(", "E", ")", "Wikipedia", "page", "at", "150", "%", "viewing", "size", "after", "manually", "zooming", "in", "."]}, "filename": "673e92340023a4eb42572964ac9343eda8798a1e_Image_002.jpg", "orig_filename": "673e92340023a4eb42572964ac9343eda8798a1e", "split": "train"}, {"article_id": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "description": {"raw": "Quantile dotplots shown on a timeline. Dark vertical bar indicates predicted arrival time; light vertical bar indicates scheduled arrival time.", "tokens": ["Quantile", "dotplots", "shown", "on", "a", "timeline", ".", "Dark", "vertical", "bar", "indicates", "predicted", "arrival", "time", ";", "light", "vertical", "bar", "indicates", "scheduled", "arrival", "time", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making ", "tokens": ["Uncertainty", "Displays", "Using", "Quantile", "Dotplots", "or", "CDFs", "Improve", "Transit", "Decision-Making"]}, "filename": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_005.jpg", "orig_filename": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "split": "train"}, {"article_id": "What Makes Smartphone Use Meaningful or Meaningless?", "description": {"raw": "This figure is a stacked bar chart of the motivation behind use at the start, during, and end timing of app use samples. The percentage share of instrumental motivation declined from the start timing (65.6%) to the during timing (51.4%). There was a slight increase in instrumental use from the during timing to the end timing (55.7%).", "tokens": ["This", "figure", "is", "a", "stacked", "bar", "chart", "of", "the", "motivation", "behind", "use", "at", "the", "start", ",", "during", ",", "and", "end", "timing", "of", "app", "use", "samples", ".", "The", "percentage", "share", "of", "instrumental", "motivation", "declined", "from", "the", "start", "timing", "(", "65.6", "%", ")", "to", "the", "during", "timing", "(", "51.4", "%", ")", ".", "There", "was", "a", "slight", "increase", "in", "instrumental", "use", "from", "the", "during", "timing", "to", "the", "end", "timing", "(", "55.7", "%", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "What Makes Smartphone Use Meaningful or Meaningless? ", "tokens": ["What", "Makes", "Smartphone", "Use", "Meaningful", "or", "Meaningless", "?"]}, "filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_008.jpg", "orig_filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "split": "train"}, {"article_id": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments", "description": {"raw": "It shows four screenshots of AIGuide. The top-left screenshot shows the Selection Interface, which contains a list with three items: Fruit Bar, Lipton Iced Tea and Lucky Charms. The top-right shows the guidance interface, which contains two labels. One indicates that the item was found. Other indicates that the item is two feet away, 15 degrees left and 5 inches below the camera view. Also, it has a guide, confirm , exit and restart button. The bottom-left shows the user settings, which contains a toggle switch for camera access, two toggle switches to control haptic and sound feedback, a scrolling bar for the speaking rate and a submenu for the measuring system. The bottom right shows the tutorial interface, which contains a page number at the top, the description of that explains how the localization phase works, a button to play a demo, and two buttons to switch page.", "tokens": ["It", "shows", "four", "screenshots", "of", "AIGuide", ".", "The", "top-left", "screenshot", "shows", "the", "Selection", "Interface", ",", "which", "contains", "a", "list", "with", "three", "items", ":", "Fruit", "Bar", ",", "Lipton", "Iced", "Tea", "and", "Lucky", "Charms", ".", "The", "top-right", "shows", "the", "guidance", "interface", ",", "which", "contains", "two", "labels", ".", "One", "indicates", "that", "the", "item", "was", "found", ".", "Other", "indicates", "that", "the", "item", "is", "two", "feet", "away", ",", "15", "degrees", "left", "and", "5", "inches", "below", "the", "camera", "view", ".", "Also", ",", "it", "has", "a", "guide", ",", "confirm", ",", "exit", "and", "restart", "button", ".", "The", "bottom-left", "shows", "the", "user", "settings", ",", "which", "contains", "a", "toggle", "switch", "for", "camera", "access", ",", "two", "toggle", "switches", "to", "control", "haptic", "and", "sound", "feedback", ",", "a", "scrolling", "bar", "for", "the", "speaking", "rate", "and", "a", "submenu", "for", "the", "measuring", "system", ".", "The", "bottom", "right", "shows", "the", "tutorial", "interface", ",", "which", "contains", "a", "page", "number", "at", "the", "top", ",", "the", "description", "of", "that", "explains", "how", "the", "localization", "phase", "works", ",", "a", "button", "to", "play", "a", "demo", ",", "and", "two", "buttons", "to", "switch", "page", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "AIGuide: An Augmented Reality Hand Guidance Application for People with Visual Impairments ", "tokens": ["AIGuide", ":", "An", "Augmented", "Reality", "Hand", "Guidance", "Application", "for", "People", "with", "Visual", "Impairments"]}, "filename": "9475d822749fd4754e46de81e21b28445e4f98a7_Image_002.jpg", "orig_filename": "9475d822749fd4754e46de81e21b28445e4f98a7", "split": "train"}, {"article_id": "\"With most of it being pictures now, I rarely use it\": Understanding Twitter's Evolving Accessibility to Blind Users", "description": {"raw": "1.  photograph: 7 people standing posing for picture 2.  drawing:  a drawing of aunicorn with blue hair 3.  picture of text 4.  image with embedded text (screenshort):  shows a baler, and says, \"Enter to win a New Holland baler for one year's use.\" 5.  screenshot of mobile device (includes Instragram photo of a baby) 6.  a graph 7.  an inspirational quote written in cursive over a pastel background, says, \"nothing can dim the light that shines from within\" 8.  baby mafia meme:  says, \"no one hides from me, find this 'waldo' and bring me his head\"", "tokens": ["1.", "photograph", ":", "7", "people", "standing", "posing", "for", "picture", "2.", "drawing", ":", "a", "drawing", "of", "aunicorn", "with", "blue", "hair", "3.", "picture", "of", "text", "4.", "image", "with", "embedded", "text", "(", "screenshort", ")", ":", "shows", "a", "baler", ",", "and", "says", ",", "``", "Enter", "to", "win", "a", "New", "Holland", "baler", "for", "one", "year", "'s", "use", ".", "''", "5.", "screenshot", "of", "mobile", "device", "(", "includes", "Instragram", "photo", "of", "a", "baby", ")", "6.", "a", "graph", "7.", "an", "inspirational", "quote", "written", "in", "cursive", "over", "a", "pastel", "background", ",", "says", ",", "``", "nothing", "can", "dim", "the", "light", "that", "shines", "from", "within", "''", "8.", "baby", "mafia", "meme", ":", "says", ",", "``", "no", "one", "hides", "from", "me", ",", "find", "this", "'waldo", "'", "and", "bring", "me", "his", "head", "''"]}, "caption": {"raw": "Figure 3. Nine types of imagery commonly embedded in tweets. Top row: photograph; drawing; picture of text (“screenshort”); image with embedded text; screenshot. Bottom row: graph; inspirational quote; meme, unofficial retweet.", "tokens": ["Figure", "3", ".", "Nine", "types", "of", "imagery", "commonly", "embedded", "in", "tweets", ".", "Top", "row", ":", "photograph", ";", "drawing", ";", "picture", "of", "text", "(", "“", "screenshort", "”", ")", ";", "image", "with", "embedded", "text", ";", "screenshot", ".", "Bottom", "row", ":", "graph", ";", "inspirational", "quote", ";", "meme", ",", "unofficial", "retweet", "."]}, "context": {"raw": "\"With most of it being pictures now, I rarely use it\": Understanding Twitter's Evolving Accessibility to Blind Users Figure 3. Nine types of imagery commonly embedded in tweets. Top row: photograph; drawing; picture of text (“screenshort”); image with embedded text; screenshot. Bottom row: graph; inspirational quote; meme, unofficial retweet.", "tokens": ["``", "With", "most", "of", "it", "being", "pictures", "now", ",", "I", "rarely", "use", "it", "''", ":", "Understanding", "Twitter", "'s", "Evolving", "Accessibility", "to", "Blind", "Users", "Figure", "3", ".", "Nine", "types", "of", "imagery", "commonly", "embedded", "in", "tweets", ".", "Top", "row", ":", "photograph", ";", "drawing", ";", "picture", "of", "text", "(", "“", "screenshort", "”", ")", ";", "image", "with", "embedded", "text", ";", "screenshot", ".", "Bottom", "row", ":", "graph", ";", "inspirational", "quote", ";", "meme", ",", "unofficial", "retweet", "."]}, "filename": "e4acbc3656424766e39a6fbb0ae758d90554111e_Image_003.jpg", "orig_filename": "e4acbc3656424766e39a6fbb0ae758d90554111e", "split": "train"}, {"article_id": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "description": {"raw": "A line graph showing the accuracy of the four stated conditions. the prediction curves are better for each subsequent curve, going from worst to best: KTM, KTM-7, HC-KTM-1, HC-KTM-7", "tokens": ["A", "line", "graph", "showing", "the", "accuracy", "of", "the", "four", "stated", "conditions", ".", "the", "prediction", "curves", "are", "better", "for", "each", "subsequent", "curve", ",", "going", "from", "worst", "to", "best", ":", "KTM", ",", "KTM-7", ",", "HC-KTM-1", ",", "HC-KTM-7"]}, "caption": {"raw": "Figure 13. Accuracy curves for the Baseline and 4 variants of the model, i.e., HC-KTM-7, HC-KTM-1, KTM-7, KTM.", "tokens": ["Figure", "13", ".", "Accuracy", "curves", "for", "the", "Baseline", "and", "4", "variants", "of", "the", "model", ",", "i.e.", ",", "HC-KTM-7", ",", "HC-KTM-1", ",", "KTM-7", ",", "KTM", "."]}, "context": {"raw": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR Figure 13. Accuracy curves for the Baseline and 4 variants of the model, i.e., HC-KTM-7, HC-KTM-1, KTM-7, KTM.", "tokens": ["Head-Coupled", "Kinematic", "Template", "Matching", ":", "A", "Prediction", "Model", "for", "Ray", "Pointing", "in", "VR", "Figure", "13", ".", "Accuracy", "curves", "for", "the", "Baseline", "and", "4", "variants", "of", "the", "model", ",", "i.e.", ",", "HC-KTM-7", ",", "HC-KTM-1", ",", "KTM-7", ",", "KTM", "."]}, "filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_014.jpg", "orig_filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "split": "train"}, {"article_id": "SoundWatch: Exploring Smartwatch-based Deep Learning Approaches to Support Sound Awareness for Deaf and Hard of Hearing Users", "description": {"raw": "A bar graph comparing the model performance of the four models across the there sound categories. Values given in text.", "tokens": ["A", "bar", "graph", "comparing", "the", "model", "performance", "of", "the", "four", "models", "across", "the", "there", "sound", "categories", ".", "Values", "given", "in", "text", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "SoundWatch: Exploring Smartwatch-based Deep Learning Approaches to Support Sound Awareness for Deaf and Hard of Hearing Users ", "tokens": ["SoundWatch", ":", "Exploring", "Smartwatch-based", "Deep", "Learning", "Approaches", "to", "Support", "Sound", "Awareness", "for", "Deaf", "and", "Hard", "of", "Hearing", "Users"]}, "filename": "243a694be1b17c4018272ecce46685ad24b079a6_Image_007.jpg", "orig_filename": "243a694be1b17c4018272ecce46685ad24b079a6", "split": "train"}, {"article_id": "Assessment of Semantic Taxonomies for Blind Indoor Navigation Based on a Shopping Center Use Case", "description": {"raw": "Bar chart showing mean ratings about usefulness of vocal messages, grouped by mobility aid used and visual condition of participants. Values are explained along subsection 3.3 entitled subjective ratings.", "tokens": ["Bar", "chart", "showing", "mean", "ratings", "about", "usefulness", "of", "vocal", "messages", ",", "grouped", "by", "mobility", "aid", "used", "and", "visual", "condition", "of", "participants", ".", "Values", "are", "explained", "along", "subsection", "3.3", "entitled", "subjective", "ratings", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Assessment of Semantic Taxonomies for Blind Indoor Navigation Based on a Shopping Center Use Case ", "tokens": ["Assessment", "of", "Semantic", "Taxonomies", "for", "Blind", "Indoor", "Navigation", "Based", "on", "a", "Shopping", "Center", "Use", "Case"]}, "filename": "ca084360fd522adf8970c75d12cef70a4865d10f_Image_002.jpg", "orig_filename": "ca084360fd522adf8970c75d12cef70a4865d10f", "split": "train"}, {"article_id": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation", "description": {"raw": "Stacked bar chart showing participants assumed reasons for the message being deleted, separated by group and pairwise chats. The chart is detailed in section 5.1.3 and shows most reported assumption was that it was sent to the wrong recipient.", "tokens": ["Stacked", "bar", "chart", "showing", "participants", "assumed", "reasons", "for", "the", "message", "being", "deleted", ",", "separated", "by", "group", "and", "pairwise", "chats", ".", "The", "chart", "is", "detailed", "in", "section", "5.1.3", "and", "shows", "most", "reported", "assumption", "was", "that", "it", "was", "sent", "to", "the", "wrong", "recipient", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation ", "tokens": ["“", "Oops", "...", "”", ":", "Mobile", "Message", "Deletion", "in", "Conversation", "Error", "and", "Regret", "Remediation"]}, "filename": "fa7c681ffd0ea94a482755623a107fce48740b0d_Image_008.png", "orig_filename": "fa7c681ffd0ea94a482755623a107fce48740b0d", "split": "train"}, {"article_id": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "description": {"raw": "(1) High paying \"video evalution\" HITs push the hourly wage distribution up. (2) The CC distribution is skewed toward left by low hourly wages of \"transcribe data\" and \"transcribe image\" tasks.", "tokens": ["(", "1", ")", "High", "paying", "``", "video", "evalution", "''", "HITs", "push", "the", "hourly", "wage", "distribution", "up", ".", "(", "2", ")", "The", "CC", "distribution", "is", "skewed", "toward", "left", "by", "low", "hourly", "wages", "of", "``", "transcribe", "data", "''", "and", "``", "transcribe", "image", "''", "tasks", "."]}, "caption": {"raw": "Figure 12. (a) Hourly wage distributions of seven HIT categories provided by Gadiraju et al. [25] (with an additional category", "tokens": ["Figure", "12", ".", "(", "a", ")", "Hourly", "wage", "distributions", "of", "seven", "HIT", "categories", "provided", "by", "Gadiraju", "et", "al", ".", "[", "25", "]", "(", "with", "an", "additional", "category"]}, "context": {"raw": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk Figure 12. (a) Hourly wage distributions of seven HIT categories provided by Gadiraju et al. [25] (with an additional category", "tokens": ["A", "Data-Driven", "Analysis", "of", "Workers", "'", "Earnings", "on", "Amazon", "Mechanical", "Turk", "Figure", "12", ".", "(", "a", ")", "Hourly", "wage", "distributions", "of", "seven", "HIT", "categories", "provided", "by", "Gadiraju", "et", "al", ".", "[", "25", "]", "(", "with", "an", "additional", "category"]}, "filename": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b_Image_014.jpg", "orig_filename": "4aa65bbc0da30f1f761722f5db7d3bffd49ea29b", "split": "train"}, {"article_id": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning", "description": {"raw": "The graph mode allows for manipulating the graph of any of the selected groups through moving the buble marks for each of the axes. It also shows the graph for the previous session for comparison. The user can show/hide the graph for the previous session and the graph of the students' self assessment.", "tokens": ["The", "graph", "mode", "allows", "for", "manipulating", "the", "graph", "of", "any", "of", "the", "selected", "groups", "through", "moving", "the", "buble", "marks", "for", "each", "of", "the", "axes", ".", "It", "also", "shows", "the", "graph", "for", "the", "previous", "session", "for", "comparison", ".", "The", "user", "can", "show/hide", "the", "graph", "for", "the", "previous", "session", "and", "the", "graph", "of", "the", "students", "'", "self", "assessment", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Group Spinner : Recognizing & Visualizing Learning in the Classroom for Reflection , Communication & Planning ", "tokens": ["Group", "Spinner", ":", "Recognizing", "&", "Visualizing", "Learning", "in", "the", "Classroom", "for", "Reflection", ",", "Communication", "&", "Planning"]}, "filename": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a_Image_009.jpg", "orig_filename": "e2dfca4f3f079b1ed31d59a68ecce08cbe52c05a", "split": "train"}, {"article_id": "Gaze Guidance for Captioned Videos for DHH Users", "description": {"raw": "Box plot present percent fixation times for videos with vs. without. mean fixation times of \"Without\" is higher than \"With Video\".", "tokens": ["Box", "plot", "present", "percent", "fixation", "times", "for", "videos", "with", "vs.", "without", ".", "mean", "fixation", "times", "of", "``", "Without", "''", "is", "higher", "than", "``", "With", "Video", "''", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Gaze Guidance for Captioned Videos for DHH Users ", "tokens": ["Gaze", "Guidance", "for", "Captioned", "Videos", "for", "DHH", "Users"]}, "filename": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c_Image_007.jpg", "orig_filename": "acc12add91acb0f81c5bbc7c0ac9343d2d4a8a2c", "split": "train"}, {"article_id": "Neo-Noumena", "description": {"raw": "This image illustrates the system architecture of Neo-Noumea. The architecture is represented by a flow chart showing information going from the users EEG, to a laptop computing the signal with OpenBCI, to the support vector machine, to a HoloLens. There is then a circular loop between the signals going to and from the HoloLens and a server, which is being sent via OSC.", "tokens": ["This", "image", "illustrates", "the", "system", "architecture", "of", "Neo-Noumea", ".", "The", "architecture", "is", "represented", "by", "a", "flow", "chart", "showing", "information", "going", "from", "the", "users", "EEG", ",", "to", "a", "laptop", "computing", "the", "signal", "with", "OpenBCI", ",", "to", "the", "support", "vector", "machine", ",", "to", "a", "HoloLens", ".", "There", "is", "then", "a", "circular", "loop", "between", "the", "signals", "going", "to", "and", "from", "the", "HoloLens", "and", "a", "server", ",", "which", "is", "being", "sent", "via", "OSC", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Neo-Noumena ", "tokens": ["Neo-Noumena"]}, "filename": "04080e7d9dd5bb8134505b5092da87b3b6468e16_Image_004.jpg", "orig_filename": "04080e7d9dd5bb8134505b5092da87b3b6468e16", "split": "train"}, {"article_id": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "description": {"raw": "A bar chart showing the average number of actions required for spatail conditon sperated by on and off screen targets as wel as for the phone and for the iPad. Likewise for the touch condition.", "tokens": ["A", "bar", "chart", "showing", "the", "average", "number", "of", "actions", "required", "for", "spatail", "conditon", "sperated", "by", "on", "and", "off", "screen", "targets", "as", "wel", "as", "for", "the", "phone", "and", "for", "the", "iPad", ".", "Likewise", "for", "the", "touch", "condition", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays ", "tokens": ["Pinch-drag-flick", "vs.", "spatial", "input", ":", "rethinking", "zoom", "&", "pan", "on", "mobile", "displays"]}, "filename": "f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_009.jpg", "orig_filename": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "split": "train"}, {"article_id": "Galileo: Citizen-led Experimentation Using a Social Computing System", "description": {"raw": "A screenshot of an experiment designers dashboard showing participant data details and instructions on how to remind participants", "tokens": ["A", "screenshot", "of", "an", "experiment", "designers", "dashboard", "showing", "participant", "data", "details", "and", "instructions", "on", "how", "to", "remind", "participants"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Galileo: Citizen-led Experimentation Using a Social Computing System ", "tokens": ["Galileo", ":", "Citizen-led", "Experimentation", "Using", "a", "Social", "Computing", "System"]}, "filename": "8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_006.jpg", "orig_filename": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "split": "train"}, {"article_id": "Shouldercam: evaluating the user experience of a depth camera system to measure shoulder range of motion", "description": {"raw": "Bar chart displaying that ShoulderCam measurements took less time than (in seconds) than goniomter measurements. P4 and P8 were omitted due to being measured by a member of the research team. The data below:\n\n Gon ShoulderCam\nP1 252 69\nP2 342 95\nP3 330 127\nP5 478 124\nP6 158 77\nP7 124 65\nP9 395 269\nP10 208 117\nP11 218 106", "tokens": ["Bar", "chart", "displaying", "that", "ShoulderCam", "measurements", "took", "less", "time", "than", "(", "in", "seconds", ")", "than", "goniomter", "measurements", ".", "P4", "and", "P8", "were", "omitted", "due", "to", "being", "measured", "by", "a", "member", "of", "the", "research", "team", ".", "The", "data", "below", ":", "Gon", "ShoulderCam", "P1", "252", "69", "P2", "342", "95", "P3", "330", "127", "P5", "478", "124", "P6", "158", "77", "P7", "124", "65", "P9", "395", "269", "P10", "208", "117", "P11", "218", "106"]}, "caption": {"raw": "Figure 6. Average time for goniometer (blue) and Shoulder- Cam (red) in seconds. We omitted P4 and P8 since their measurements were taken by a member of the research team.", "tokens": ["Figure", "6", ".", "Average", "time", "for", "goniometer", "(", "blue", ")", "and", "Shoulder-", "Cam", "(", "red", ")", "in", "seconds", ".", "We", "omitted", "P4", "and", "P8", "since", "their", "measurements", "were", "taken", "by", "a", "member", "of", "the", "research", "team", "."]}, "context": {"raw": "Shouldercam: evaluating the user experience of a depth camera system to measure shoulder range of motion Figure 6. Average time for goniometer (blue) and Shoulder- Cam (red) in seconds. We omitted P4 and P8 since their measurements were taken by a member of the research team.", "tokens": ["Shouldercam", ":", "evaluating", "the", "user", "experience", "of", "a", "depth", "camera", "system", "to", "measure", "shoulder", "range", "of", "motion", "Figure", "6", ".", "Average", "time", "for", "goniometer", "(", "blue", ")", "and", "Shoulder-", "Cam", "(", "red", ")", "in", "seconds", ".", "We", "omitted", "P4", "and", "P8", "since", "their", "measurements", "were", "taken", "by", "a", "member", "of", "the", "research", "team", "."]}, "filename": "1e7672d17103be22a238cadd435f9385b70e86c7_Image_010.jpg", "orig_filename": "1e7672d17103be22a238cadd435f9385b70e86c7", "split": "train"}, {"article_id": "How We Type: Eye and Finger Movement Strategies in Mobile Typing", "description": {"raw": "Heatmap showing finger-to-key mapping in two-thumb touch data of three participants (all sentences aggregated). These patters are representative of a tendency we found in the data for the right thumb to cover more keys than the left. The right hand was the dominant hand for most of the participants, but the same pattern was observed also for the left-handed participants.", "tokens": ["Heatmap", "showing", "finger-to-key", "mapping", "in", "two-thumb", "touch", "data", "of", "three", "participants", "(", "all", "sentences", "aggregated", ")", ".", "These", "patters", "are", "representative", "of", "a", "tendency", "we", "found", "in", "the", "data", "for", "the", "right", "thumb", "to", "cover", "more", "keys", "than", "the", "left", ".", "The", "right", "hand", "was", "the", "dominant", "hand", "for", "most", "of", "the", "participants", ",", "but", "the", "same", "pattern", "was", "observed", "also", "for", "the", "left-handed", "participants", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "How We Type: Eye and Finger Movement Strategies in Mobile Typing ", "tokens": ["How", "We", "Type", ":", "Eye", "and", "Finger", "Movement", "Strategies", "in", "Mobile", "Typing"]}, "filename": "05300590913007eb710cd89f5d373e1ec0833bfa_Image_005.jpg", "orig_filename": "05300590913007eb710cd89f5d373e1ec0833bfa", "split": "train"}, {"article_id": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion", "description": {"raw": "Figure 9 left shows a spider graph with three interaction techniques. This spider graph shows the average response time for each one of 24 segments on the screen. For almost all of the segments PBG is faster than FBG and Shift.", "tokens": ["Figure", "9", "left", "shows", "a", "spider", "graph", "with", "three", "interaction", "techniques", ".", "This", "spider", "graph", "shows", "the", "average", "response", "time", "for", "each", "one", "of", "24", "segments", "on", "the", "screen", ".", "For", "almost", "all", "of", "the", "segments", "PBG", "is", "faster", "than", "FBG", "and", "Shift", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion ", "tokens": ["BezelGlide", ":", "Interacting", "with", "Graphs", "on", "Smartwatches", "with", "Minimal", "Screen", "Occlusion"]}, "filename": "166d91ef569340148a9d77fe64330e4599239c2f_Image_014.jpg", "orig_filename": "166d91ef569340148a9d77fe64330e4599239c2f", "split": "train"}, {"article_id": "Hands Holding Clues for Object Recognition in Teachable Machines", "description": {"raw": "This figure has two graphs that show the average accuracy depending on the background environment of objects: vanilla and wild.  The graph for the vanilla environment is shown on left and the graph for the wild environment is shown on right.  In the vanilla environment, the model achieves 0.754 (std: 0.086), 0.747 (std: 0.102), and 0.704 (std: 0.028) on the HO, CO, and O method with the B data, respectively, and 0.937 (std: 0.011), 0.933 (std: 0.037), and 0.863 (std: 0.028) on the HO, CO, and O method with the S data, respectively.  In the wild environment, the model achieves 0.542 (std: 0.047), 0.667 (std: 0.040), and 0.532 (std: 0.065) on the HO, CO, and O method with the B data, respectively, and 0.796 (std: 0.095), 0.905 (std: 0.053), and 0.853 (std: 0.046) on the HO, CO, and O method with the S data, respectively.", "tokens": ["This", "figure", "has", "two", "graphs", "that", "show", "the", "average", "accuracy", "depending", "on", "the", "background", "environment", "of", "objects", ":", "vanilla", "and", "wild", ".", "The", "graph", "for", "the", "vanilla", "environment", "is", "shown", "on", "left", "and", "the", "graph", "for", "the", "wild", "environment", "is", "shown", "on", "right", ".", "In", "the", "vanilla", "environment", ",", "the", "model", "achieves", "0.754", "(", "std", ":", "0.086", ")", ",", "0.747", "(", "std", ":", "0.102", ")", ",", "and", "0.704", "(", "std", ":", "0.028", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "method", "with", "the", "B", "data", ",", "respectively", ",", "and", "0.937", "(", "std", ":", "0.011", ")", ",", "0.933", "(", "std", ":", "0.037", ")", ",", "and", "0.863", "(", "std", ":", "0.028", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "method", "with", "the", "S", "data", ",", "respectively", ".", "In", "the", "wild", "environment", ",", "the", "model", "achieves", "0.542", "(", "std", ":", "0.047", ")", ",", "0.667", "(", "std", ":", "0.040", ")", ",", "and", "0.532", "(", "std", ":", "0.065", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "method", "with", "the", "B", "data", ",", "respectively", ",", "and", "0.796", "(", "std", ":", "0.095", ")", ",", "0.905", "(", "std", ":", "0.053", ")", ",", "and", "0.853", "(", "std", ":", "0.046", ")", "on", "the", "HO", ",", "CO", ",", "and", "O", "method", "with", "the", "S", "data", ",", "respectively", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Hands Holding Clues for Object Recognition in Teachable Machines ", "tokens": ["Hands", "Holding", "Clues", "for", "Object", "Recognition", "in", "Teachable", "Machines"]}, "filename": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff_Image_087.jpg", "orig_filename": "de6562e7c3736b7e0c94f62f26ea1f77d44e4eff", "split": "train"}, {"article_id": "Synthesized Social Signals: Computationally-Derived Social Signals from Account Histories", "description": {"raw": "Figure 6: This Figure contains a 2D stacked column chart showing the results of the post-study survey (which was conducted after the field study).  In the x-axis, there are 4 questions/sentences which were given to the participants using a 1-5 Likert scale. The y-axis is the average of the rating per each question/sentence. Furthermore, each bar per question/sentence shows the distribution of the ratings. The ratio of each rating (from 1 to 5) is colored differently.  The first sentence is \"1) Sig helped me feel more comfortable when deciding whether to interact with strangers on Twitter\". Participants overall gave an average rating of 3.9/5 for this sentence. 4 people gave a 3, 4 people gave a 4, and 3 people gave a 5.  Next, for \"2) How would you rate your overall experience of using Sig?\" the average rating was 4 out of 5. 2 people gave a 3, 7 people gave a 4, and 2 people gave a 5.  For \"3) Was Sig's visualization of various social signals easy to understand?\" the average score was 4.7 out of 5. 3 people gave a 4 and 8 people gave a 5. Finally, for \"4) Would you be interested in using a refined version of Sig in the future?\", the average rating was 4.7 out of 5. 1 person gave a 2, 2 people gave a 3, 2 people gave a 4, and 6 people gave a 5.", "tokens": ["Figure", "6", ":", "This", "Figure", "contains", "a", "2D", "stacked", "column", "chart", "showing", "the", "results", "of", "the", "post-study", "survey", "(", "which", "was", "conducted", "after", "the", "field", "study", ")", ".", "In", "the", "x-axis", ",", "there", "are", "4", "questions/sentences", "which", "were", "given", "to", "the", "participants", "using", "a", "1-5", "Likert", "scale", ".", "The", "y-axis", "is", "the", "average", "of", "the", "rating", "per", "each", "question/sentence", ".", "Furthermore", ",", "each", "bar", "per", "question/sentence", "shows", "the", "distribution", "of", "the", "ratings", ".", "The", "ratio", "of", "each", "rating", "(", "from", "1", "to", "5", ")", "is", "colored", "differently", ".", "The", "first", "sentence", "is", "``", "1", ")", "Sig", "helped", "me", "feel", "more", "comfortable", "when", "deciding", "whether", "to", "interact", "with", "strangers", "on", "Twitter", "''", ".", "Participants", "overall", "gave", "an", "average", "rating", "of", "3.9/5", "for", "this", "sentence", ".", "4", "people", "gave", "a", "3", ",", "4", "people", "gave", "a", "4", ",", "and", "3", "people", "gave", "a", "5", ".", "Next", ",", "for", "``", "2", ")", "How", "would", "you", "rate", "your", "overall", "experience", "of", "using", "Sig", "?", "''", "the", "average", "rating", "was", "4", "out", "of", "5", ".", "2", "people", "gave", "a", "3", ",", "7", "people", "gave", "a", "4", ",", "and", "2", "people", "gave", "a", "5", ".", "For", "``", "3", ")", "Was", "Sig", "'s", "visualization", "of", "various", "social", "signals", "easy", "to", "understand", "?", "''", "the", "average", "score", "was", "4.7", "out", "of", "5", ".", "3", "people", "gave", "a", "4", "and", "8", "people", "gave", "a", "5", ".", "Finally", ",", "for", "``", "4", ")", "Would", "you", "be", "interested", "in", "using", "a", "refined", "version", "of", "Sig", "in", "the", "future", "?", "``", ",", "the", "average", "rating", "was", "4.7", "out", "of", "5", ".", "1", "person", "gave", "a", "2", ",", "2", "people", "gave", "a", "3", ",", "2", "people", "gave", "a", "4", ",", "and", "6", "people", "gave", "a", "5", "."]}, "caption": {"raw": "Figure 6. Results of the survey taken by participants after the ﬁeld study. Each question appeared on a 1-5 Likert scale.", "tokens": ["Figure", "6", ".", "Results", "of", "the", "survey", "taken", "by", "participants", "after", "the", "ﬁeld", "study", ".", "Each", "question", "appeared", "on", "a", "1-5", "Likert", "scale", "."]}, "context": {"raw": "Synthesized Social Signals: Computationally-Derived Social Signals from Account Histories Figure 6. Results of the survey taken by participants after the ﬁeld study. Each question appeared on a 1-5 Likert scale.", "tokens": ["Synthesized", "Social", "Signals", ":", "Computationally-Derived", "Social", "Signals", "from", "Account", "Histories", "Figure", "6", ".", "Results", "of", "the", "survey", "taken", "by", "participants", "after", "the", "ﬁeld", "study", ".", "Each", "question", "appeared", "on", "a", "1-5", "Likert", "scale", "."]}, "filename": "666ee9fe09ce777ab9e9940ee465c8349d7ac43e_Image_012.jpg", "orig_filename": "666ee9fe09ce777ab9e9940ee465c8349d7ac43e", "split": "train"}, {"article_id": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision", "description": {"raw": "Figure 9: Bar graph showing the rating of users' subjective feelings from 1 to 7 on the Y axis against the six evaluation metrics on the X axis.", "tokens": ["Figure", "9", ":", "Bar", "graph", "showing", "the", "rating", "of", "users", "'", "subjective", "feelings", "from", "1", "to", "7", "on", "the", "Y", "axis", "against", "the", "six", "evaluation", "metrics", "on", "the", "X", "axis", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision ", "tokens": ["FaceSight", ":", "Enabling", "Hand-to-Face", "Gesture", "Interaction", "on", "AR", "Glasses", "with", "a", "Downward-Facing", "Camera", "Vision"]}, "filename": "851f09fb796711c334599647d45759535163d36d_Image_012.jpg", "orig_filename": "851f09fb796711c334599647d45759535163d36d", "split": "train"}, {"article_id": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan", "description": {"raw": "A bubble chart shows a matrix of bubbles, where the rows are countries (U.S., China, Taiwan) and the columns are behavior categories. Each bubble has a number label corresponding to its area, which reflects the average number of a times a parent from the sample (row) performed the specific behavior (column) during the instructional game session.  Taiwan: Intervention: 17.75 Instruction: 16.25 Open-Ended: 12.83 Augmentation: 8.58 Warmth: 12.83  China: Intervention: 7.3 Instruction: 8.2 Open-Ended: 8.0 Augmentation: 7.4 Warmth: 3.8  U.S. Intervention: 2.4 Instruction: 6.26 Open-Ended: 7.6 Augmentation: 9.6 Warmth: 22.8", "tokens": ["A", "bubble", "chart", "shows", "a", "matrix", "of", "bubbles", ",", "where", "the", "rows", "are", "countries", "(", "U.S.", ",", "China", ",", "Taiwan", ")", "and", "the", "columns", "are", "behavior", "categories", ".", "Each", "bubble", "has", "a", "number", "label", "corresponding", "to", "its", "area", ",", "which", "reflects", "the", "average", "number", "of", "a", "times", "a", "parent", "from", "the", "sample", "(", "row", ")", "performed", "the", "specific", "behavior", "(", "column", ")", "during", "the", "instructional", "game", "session", ".", "Taiwan", ":", "Intervention", ":", "17.75", "Instruction", ":", "16.25", "Open-Ended", ":", "12.83", "Augmentation", ":", "8.58", "Warmth", ":", "12.83", "China", ":", "Intervention", ":", "7.3", "Instruction", ":", "8.2", "Open-Ended", ":", "8.0", "Augmentation", ":", "7.4", "Warmth", ":", "3.8", "U.S", ".", "Intervention", ":", "2.4", "Instruction", ":", "6.26", "Open-Ended", ":", "7.6", "Augmentation", ":", "9.6", "Warmth", ":", "22.8"]}, "caption": {"raw": "Figure 10: The average number of times each parent displayed each form of JME during the CMC session.", "tokens": ["Figure", "10", ":", "The", "average", "number", "of", "times", "each", "parent", "displayed", "each", "form", "of", "JME", "during", "the", "CMC", "session", "."]}, "context": {"raw": "Joint Media Engagement between Parents and Preschoolers in the U.S., China, and Taiwan Figure 10: The average number of times each parent displayed each form of JME during the CMC session.", "tokens": ["Joint", "Media", "Engagement", "between", "Parents", "and", "Preschoolers", "in", "the", "U.S.", ",", "China", ",", "and", "Taiwan", "Figure", "10", ":", "The", "average", "number", "of", "times", "each", "parent", "displayed", "each", "form", "of", "JME", "during", "the", "CMC", "session", "."]}, "filename": "c9c4eef7b43440cb866cdd74715e01dfa995f075_Image_019.jpg", "orig_filename": "c9c4eef7b43440cb866cdd74715e01dfa995f075", "split": "train"}, {"article_id": "Capturing Experts' Mental Models to Organize a Collection of Haptic Devices: Affordances Outweigh Attributes", "description": {"raw": "The image has three vertical sections presenting the four stages of the study, the data analysis, and the resulting categories and uber-attributes.", "tokens": ["The", "image", "has", "three", "vertical", "sections", "presenting", "the", "four", "stages", "of", "the", "study", ",", "the", "data", "analysis", ",", "and", "the", "resulting", "categories", "and", "uber-attributes", "."]}, "caption": {"raw": "Figure 3: Overview of the data collected during our study as well as our analytical procedures and results.", "tokens": ["Figure", "3", ":", "Overview", "of", "the", "data", "collected", "during", "our", "study", "as", "well", "as", "our", "analytical", "procedures", "and", "results", "."]}, "context": {"raw": "Capturing Experts' Mental Models to Organize a Collection of Haptic Devices: Affordances Outweigh Attributes Figure 3: Overview of the data collected during our study as well as our analytical procedures and results.", "tokens": ["Capturing", "Experts", "'", "Mental", "Models", "to", "Organize", "a", "Collection", "of", "Haptic", "Devices", ":", "Affordances", "Outweigh", "Attributes", "Figure", "3", ":", "Overview", "of", "the", "data", "collected", "during", "our", "study", "as", "well", "as", "our", "analytical", "procedures", "and", "results", "."]}, "filename": "07ffdba9d5617e70010ec4612616bfb4ab0c07e1_Image_004.jpg", "orig_filename": "07ffdba9d5617e70010ec4612616bfb4ab0c07e1", "split": "train"}, {"article_id": "Exploring Text Revision with Backspace and Caret in Virtual Reality", "description": {"raw": "Bar chart with error bars presents the correction time for four VR text revision techniques when dealing with different types of revision targets. Overall, revising targets that far from the end of the sentence takes more time than those targets near the end of the sentence.", "tokens": ["Bar", "chart", "with", "error", "bars", "presents", "the", "correction", "time", "for", "four", "VR", "text", "revision", "techniques", "when", "dealing", "with", "different", "types", "of", "revision", "targets", ".", "Overall", ",", "revising", "targets", "that", "far", "from", "the", "end", "of", "the", "sentence", "takes", "more", "time", "than", "those", "targets", "near", "the", "end", "of", "the", "sentence", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Exploring Text Revision with Backspace and Caret in Virtual Reality ", "tokens": ["Exploring", "Text", "Revision", "with", "Backspace", "and", "Caret", "in", "Virtual", "Reality"]}, "filename": "5894fd4581a79a7067102891bc3db5738195941f_Image_005.jpg", "orig_filename": "5894fd4581a79a7067102891bc3db5738195941f", "split": "train"}, {"article_id": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies", "description": {"raw": "Plot of Effect of Surfaced Hits on ML-1M. Shows boycotts and data strikes. As size of boycotts/data strikes increases, Surfaced Hits are reduced.", "tokens": ["Plot", "of", "Effect", "of", "Surfaced", "Hits", "on", "ML-1M", ".", "Shows", "boycotts", "and", "data", "strikes", ".", "As", "size", "of", "boycotts/data", "strikes", "increases", ",", "Surfaced", "Hits", "are", "reduced", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "“Data Strikes”: Evaluating the Effectiveness of a New Form of Collective Action Against Technology Companies ", "tokens": ["“", "Data", "Strikes", "”", ":", "Evaluating", "the", "Effectiveness", "of", "a", "New", "Form", "of", "Collective", "Action", "Against", "Technology", "Companies"]}, "filename": "41cbffad975874060d643c36c8bdb5c72637564e_Image_005.jpg", "orig_filename": "41cbffad975874060d643c36c8bdb5c72637564e", "split": "train"}, {"article_id": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer", "description": {"raw": "Change in the magnitude of magnetometer vector data with the change in the ambient magnetic field. The magnetometer vector data is a vector sum of the magnetic field of the magnet ring and the ambient magnetic field. The reference and 6'o clock directions denotes two opposite directions.", "tokens": ["Change", "in", "the", "magnitude", "of", "magnetometer", "vector", "data", "with", "the", "change", "in", "the", "ambient", "magnetic", "field", ".", "The", "magnetometer", "vector", "data", "is", "a", "vector", "sum", "of", "the", "magnetic", "field", "of", "the", "magnet", "ring", "and", "the", "ambient", "magnetic", "field", ".", "The", "reference", "and", "6", "'", "o", "clock", "directions", "denotes", "two", "opposite", "directions", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer ", "tokens": ["MagTouch", ":", "Robust", "Finger", "Identification", "for", "a", "Smartwatch", "Using", "a", "Magnet", "Ring", "and", "a", "Built-in", "Magnetometer"]}, "filename": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4_Image_003.png", "orig_filename": "0ce127ebea937d1730bdb2a4625d6bd94a3bdef4", "split": "train"}, {"article_id": "A General Methodology to Quantify Biases in Natural Language Data", "description": {"raw": "Figure 4: Error bar of user ratings on how samples in different datasets state ideological stereotypes.", "tokens": ["Figure", "4", ":", "Error", "bar", "of", "user", "ratings", "on", "how", "samples", "in", "different", "datasets", "state", "ideological", "stereotypes", "."]}, "caption": {"raw": "Figure 4: Error bar of user ratings on how samples in different datasets state ideological stereotypes. For each data sample, we ask the crowd workers if “the above sentence is from a ideologically neutral or non-neutral stance”. If a sentence is not neutral, we ask them to rate how it states its stereotype on a 5-point scale, from “1: very implicitly” to “5: very explicitly”. We provide background for the crowd workers that explicit stereotypes are overt expressions of judgement with clearly hateful words or phrases; and that implicit stereotypes employ circumlocution, metaphor, or other compositional meanings. We randomize the data sources for all samples, and recruit at least 5 crowd workers for each sample.", "tokens": ["Figure", "4", ":", "Error", "bar", "of", "user", "ratings", "on", "how", "samples", "in", "different", "datasets", "state", "ideological", "stereotypes", ".", "For", "each", "data", "sample", ",", "we", "ask", "the", "crowd", "workers", "if", "“", "the", "above", "sentence", "is", "from", "a", "ideologically", "neutral", "or", "non-neutral", "stance", "”", ".", "If", "a", "sentence", "is", "not", "neutral", ",", "we", "ask", "them", "to", "rate", "how", "it", "states", "its", "stereotype", "on", "a", "5-point", "scale", ",", "from", "“", "1", ":", "very", "implicitly", "”", "to", "“", "5", ":", "very", "explicitly", "”", ".", "We", "provide", "background", "for", "the", "crowd", "workers", "that", "explicit", "stereotypes", "are", "overt", "expressions", "of", "judgement", "with", "clearly", "hateful", "words", "or", "phrases", ";", "and", "that", "implicit", "stereotypes", "employ", "circumlocution", ",", "metaphor", ",", "or", "other", "compositional", "meanings", ".", "We", "randomize", "the", "data", "sources", "for", "all", "samples", ",", "and", "recruit", "at", "least", "5", "crowd", "workers", "for", "each", "sample", "."]}, "context": {"raw": "A General Methodology to Quantify Biases in Natural Language Data Figure 4: Error bar of user ratings on how samples in different datasets state ideological stereotypes. For each data sample, we ask the crowd workers if “the above sentence is from a ideologically neutral or non-neutral stance”. If a sentence is not neutral, we ask them to rate how it states its stereotype on a 5-point scale, from “1: very implicitly” to “5: very explicitly”. We provide background for the crowd workers that explicit stereotypes are overt expressions of judgement with clearly hateful words or phrases; and that implicit stereotypes employ circumlocution, metaphor, or other compositional meanings. We randomize the data sources for all samples, and recruit at least 5 crowd workers for each sample.", "tokens": ["A", "General", "Methodology", "to", "Quantify", "Biases", "in", "Natural", "Language", "Data", "Figure", "4", ":", "Error", "bar", "of", "user", "ratings", "on", "how", "samples", "in", "different", "datasets", "state", "ideological", "stereotypes", ".", "For", "each", "data", "sample", ",", "we", "ask", "the", "crowd", "workers", "if", "“", "the", "above", "sentence", "is", "from", "a", "ideologically", "neutral", "or", "non-neutral", "stance", "”", ".", "If", "a", "sentence", "is", "not", "neutral", ",", "we", "ask", "them", "to", "rate", "how", "it", "states", "its", "stereotype", "on", "a", "5-point", "scale", ",", "from", "“", "1", ":", "very", "implicitly", "”", "to", "“", "5", ":", "very", "explicitly", "”", ".", "We", "provide", "background", "for", "the", "crowd", "workers", "that", "explicit", "stereotypes", "are", "overt", "expressions", "of", "judgement", "with", "clearly", "hateful", "words", "or", "phrases", ";", "and", "that", "implicit", "stereotypes", "employ", "circumlocution", ",", "metaphor", ",", "or", "other", "compositional", "meanings", ".", "We", "randomize", "the", "data", "sources", "for", "all", "samples", ",", "and", "recruit", "at", "least", "5", "crowd", "workers", "for", "each", "sample", "."]}, "filename": "ce4b730f4eaddd98be36ce4ba95548a95a6d69fb_Image_006.jpg", "orig_filename": "ce4b730f4eaddd98be36ce4ba95548a95a6d69fb", "split": "train"}, {"article_id": "Passively-sensed Behavioral Correlates of Discrimination Events in College Students", "description": {"raw": "bar chart for level of significance from the day before to two days after the discrimination events", "tokens": ["bar", "chart", "for", "level", "of", "significance", "from", "the", "day", "before", "to", "two", "days", "after", "the", "discrimination", "events"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Passively-sensed Behavioral Correlates of Discrimination Events in College Students ", "tokens": ["Passively-sensed", "Behavioral", "Correlates", "of", "Discrimination", "Events", "in", "College", "Students"]}, "filename": "fa83fb659bbf9efb4b4a56e6a933c155c87c201e_Image_015.png", "orig_filename": "fa83fb659bbf9efb4b4a56e6a933c155c87c201e", "split": "train"}, {"article_id": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students", "description": {"raw": "Retention scores in a bar chart. For both user groups, differences between conditons are small and non-significant.", "tokens": ["Retention", "scores", "in", "a", "bar", "chart", ".", "For", "both", "user", "groups", ",", "differences", "between", "conditons", "are", "small", "and", "non-significant", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "SlidePacer: A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students ", "tokens": ["SlidePacer", ":", "A", "Presentation", "Delivery", "Tool", "for", "Instructors", "of", "Deaf", "and", "Hard", "of", "Hearing", "Students"]}, "filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738_Image_006.png", "orig_filename": "505187170911c92ea3c8b2f2eeb0fe0a1fb26738", "split": "test"}, {"article_id": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making", "description": {"raw": "No-uncertainty visualization, showing a predicted arrival time bar at 9 minutes and the scheduled arrival time bar at 7 minutes.", "tokens": ["No-uncertainty", "visualization", ",", "showing", "a", "predicted", "arrival", "time", "bar", "at", "9", "minutes", "and", "the", "scheduled", "arrival", "time", "bar", "at", "7", "minutes", "."]}, "caption": {"raw": "This display type represents the status quo: it is informationally similar to the existing OneBusAway app, except we did not include annota­", "tokens": ["This", "display", "type", "represents", "the", "status", "quo", ":", "it", "is", "informationally", "similar", "to", "the", "existing", "OneBusAway", "app", ",", "except", "we", "did", "not", "include", "annota­"]}, "context": {"raw": "Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making This display type represents the status quo: it is informationally similar to the existing OneBusAway app, except we did not include annota­", "tokens": ["Uncertainty", "Displays", "Using", "Quantile", "Dotplots", "or", "CDFs", "Improve", "Transit", "Decision-Making", "This", "display", "type", "represents", "the", "status", "quo", ":", "it", "is", "informationally", "similar", "to", "the", "existing", "OneBusAway", "app", ",", "except", "we", "did", "not", "include", "annota­"]}, "filename": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9_Image_002.jpg", "orig_filename": "825ae04b67cd58951cdb115978e9fba6a1d5f5d9", "split": "test"}, {"article_id": "User Experiences with Online Status Indicators", "description": {"raw": "A hand-drawn image showing a representation of a mobile phone screen. There is a drawing of a person's face, meant to look like a profile picture and a green dot icon but that looks like a pie chart with only a small section shown in green and the rest in white. Next to this icon is a small triangle pointing down, like those used for expanding menus. An `explanation' in the app says `Starting App. You will appear online in 5 ... 4 ... 3 ...' and an annotation pointing to the triangle next to the green pie chart icon says `click here to avoid appearing as online.'", "tokens": ["A", "hand-drawn", "image", "showing", "a", "representation", "of", "a", "mobile", "phone", "screen", ".", "There", "is", "a", "drawing", "of", "a", "person", "'s", "face", ",", "meant", "to", "look", "like", "a", "profile", "picture", "and", "a", "green", "dot", "icon", "but", "that", "looks", "like", "a", "pie", "chart", "with", "only", "a", "small", "section", "shown", "in", "green", "and", "the", "rest", "in", "white", ".", "Next", "to", "this", "icon", "is", "a", "small", "triangle", "pointing", "down", ",", "like", "those", "used", "for", "expanding", "menus", ".", "An", "`", "explanation", "'", "in", "the", "app", "says", "`", "Starting", "App", ".", "You", "will", "appear", "online", "in", "5", "...", "4", "...", "3", "...", "'", "and", "an", "annotation", "pointing", "to", "the", "triangle", "next", "to", "the", "green", "pie", "chart", "icon", "says", "`", "click", "here", "to", "avoid", "appearing", "as", "online", ".", "'"]}, "caption": {"raw": "Figure 6. Illustration of a design recommendations to let users turn off their OSI as they open an app.", "tokens": ["Figure", "6", ".", "Illustration", "of", "a", "design", "recommendations", "to", "let", "users", "turn", "off", "their", "OSI", "as", "they", "open", "an", "app", "."]}, "context": {"raw": "User Experiences with Online Status Indicators Figure 6. Illustration of a design recommendations to let users turn off their OSI as they open an app.", "tokens": ["User", "Experiences", "with", "Online", "Status", "Indicators", "Figure", "6", ".", "Illustration", "of", "a", "design", "recommendations", "to", "let", "users", "turn", "off", "their", "OSI", "as", "they", "open", "an", "app", "."]}, "filename": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b_Image_008.jpg", "orig_filename": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b", "split": "test"}, {"article_id": "Galileo: Citizen-led Experimentation Using a Social Computing System", "description": {"raw": "Three bar plots showing how three communities -- kombucha, open humans, beer -- signed up, participated, and adhered to the instructions", "tokens": ["Three", "bar", "plots", "showing", "how", "three", "communities", "--", "kombucha", ",", "open", "humans", ",", "beer", "--", "signed", "up", ",", "participated", ",", "and", "adhered", "to", "the", "instructions"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Galileo: Citizen-led Experimentation Using a Social Computing System ", "tokens": ["Galileo", ":", "Citizen-led", "Experimentation", "Using", "a", "Social", "Computing", "System"]}, "filename": "8dfa5d035b93b2987ac0997f10851a11f6f90865_Image_013.jpg", "orig_filename": "8dfa5d035b93b2987ac0997f10851a11f6f90865", "split": "test"}, {"article_id": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards", "description": {"raw": "A horizontal bar graph which lists tasks on the Y-axis and percent success from 0.0%-100.0% on the X-axis. The list of tasks, with percentages, are as follows: 1) Identify objects on the pre-made slide; 91.7%. 2) Identify rectangle color; 25.0%. 3) Identify rectangle width; 58.3%. 4) Determine rectangle's and oval's positions on the slide; 50.0%. 5) Determine rectangle's and oval's positions in relation to each other; 36.4%. 6) Determine the size of the rectangle relative to the oval; 54.5%. 7) Determine the arrow's position on the slide; 45.5%. 8) Determine the direction the arrow is pointing; 0.0%. 9) Determine the text box's position on the slide; 45.5%. 10) Identify the text in the text box; 90.9%. 11) Determine the text box's position relative to the other objects; 36.4%", "tokens": ["A", "horizontal", "bar", "graph", "which", "lists", "tasks", "on", "the", "Y-axis", "and", "percent", "success", "from", "0.0", "%", "-100.0", "%", "on", "the", "X-axis", ".", "The", "list", "of", "tasks", ",", "with", "percentages", ",", "are", "as", "follows", ":", "1", ")", "Identify", "objects", "on", "the", "pre-made", "slide", ";", "91.7", "%", ".", "2", ")", "Identify", "rectangle", "color", ";", "25.0", "%", ".", "3", ")", "Identify", "rectangle", "width", ";", "58.3", "%", ".", "4", ")", "Determine", "rectangle", "'s", "and", "oval", "'s", "positions", "on", "the", "slide", ";", "50.0", "%", ".", "5", ")", "Determine", "rectangle", "'s", "and", "oval", "'s", "positions", "in", "relation", "to", "each", "other", ";", "36.4", "%", ".", "6", ")", "Determine", "the", "size", "of", "the", "rectangle", "relative", "to", "the", "oval", ";", "54.5", "%", ".", "7", ")", "Determine", "the", "arrow", "'s", "position", "on", "the", "slide", ";", "45.5", "%", ".", "8", ")", "Determine", "the", "direction", "the", "arrow", "is", "pointing", ";", "0.0", "%", ".", "9", ")", "Determine", "the", "text", "box", "'s", "position", "on", "the", "slide", ";", "45.5", "%", ".", "10", ")", "Identify", "the", "text", "in", "the", "text", "box", ";", "90.9", "%", ".", "11", ")", "Determine", "the", "text", "box", "'s", "position", "relative", "to", "the", "other", "objects", ";", "36.4", "%"]}, "caption": {"raw": "The interpretive tasks (#1-11) for the task-based usability test, including task number, description, and the percentage of participants who were able to par- tially or fully succeed in completing the task.", "tokens": ["The", "interpretive", "tasks", "(", "#", "1-11", ")", "for", "the", "task-based", "usability", "test", ",", "including", "task", "number", ",", "description", ",", "and", "the", "percentage", "of", "participants", "who", "were", "able", "to", "par-", "tially", "or", "fully", "succeed", "in", "completing", "the", "task", "."]}, "context": {"raw": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards The interpretive tasks (#1-11) for the task-based usability test, including task number, description, and the percentage of participants who were able to par- tially or fully succeed in completing the task.", "tokens": ["Understanding", "Blind", "Screen-Reader", "Users", "’", "Experiences", "of", "Digital", "Artboards", "The", "interpretive", "tasks", "(", "#", "1-11", ")", "for", "the", "task-based", "usability", "test", ",", "including", "task", "number", ",", "description", ",", "and", "the", "percentage", "of", "participants", "who", "were", "able", "to", "par-", "tially", "or", "fully", "succeed", "in", "completing", "the", "task", "."]}, "filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_007.jpg", "orig_filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "split": "test"}, {"article_id": "Predicting Cognitive Load in Future Code Puzzles", "description": {"raw": "This graph displays the three most important features overall and for each model. Both transparency and height of the blocks represent the feature importance value.", "tokens": ["This", "graph", "displays", "the", "three", "most", "important", "features", "overall", "and", "for", "each", "model", ".", "Both", "transparency", "and", "height", "of", "the", "blocks", "represent", "the", "feature", "importance", "value", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Predicting Cognitive Load in Future Code Puzzles ", "tokens": ["Predicting", "Cognitive", "Load", "in", "Future", "Code", "Puzzles"]}, "filename": "7ce152841cc96d0d4c20e8a67c43692e1b742ddb_Image_012.png", "orig_filename": "7ce152841cc96d0d4c20e8a67c43692e1b742ddb", "split": "test"}, {"article_id": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR", "description": {"raw": "A line graph showing 3 velocity curves for 25, 50 and 75 degrees. This graph shows the velocities for the controller angle. In the first 200 milliseconds, there is almost no spread between the 3 curves", "tokens": ["A", "line", "graph", "showing", "3", "velocity", "curves", "for", "25", ",", "50", "and", "75", "degrees", ".", "This", "graph", "shows", "the", "velocities", "for", "the", "controller", "angle", ".", "In", "the", "first", "200", "milliseconds", ",", "there", "is", "almost", "no", "spread", "between", "the", "3", "curves"]}, "caption": {"raw": "Figure 11. Representative angular velocity profiles for the HMD and Controller by movement angle. The highlighted regions illustrate that in the first 150 ms of movement, profiles only diverge for the HMD, not the controller.", "tokens": ["Figure", "11", ".", "Representative", "angular", "velocity", "profiles", "for", "the", "HMD", "and", "Controller", "by", "movement", "angle", ".", "The", "highlighted", "regions", "illustrate", "that", "in", "the", "first", "150", "ms", "of", "movement", ",", "profiles", "only", "diverge", "for", "the", "HMD", ",", "not", "the", "controller", "."]}, "context": {"raw": "Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR Figure 11. Representative angular velocity profiles for the HMD and Controller by movement angle. The highlighted regions illustrate that in the first 150 ms of movement, profiles only diverge for the HMD, not the controller.", "tokens": ["Head-Coupled", "Kinematic", "Template", "Matching", ":", "A", "Prediction", "Model", "for", "Ray", "Pointing", "in", "VR", "Figure", "11", ".", "Representative", "angular", "velocity", "profiles", "for", "the", "HMD", "and", "Controller", "by", "movement", "angle", ".", "The", "highlighted", "regions", "illustrate", "that", "in", "the", "first", "150", "ms", "of", "movement", ",", "profiles", "only", "diverge", "for", "the", "HMD", ",", "not", "the", "controller", "."]}, "filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd_Image_012.jpg", "orig_filename": "be49605108a4c7b2cc78a9cf112a749a38a7b8fd", "split": "test"}, {"article_id": "Evaluating Smartwatch-based Sound Feedback for Deaf and Hard-of-hearing Users Across Contexts", "description": {"raw": "Three column charts are shown horizontally. The first chart is labelled \"Complexity of Haptic w/ Visual\" and has shows decreasing utility moving right from Simple, to Tacton, to none.    The second chart is labelled Sound characteristics and shows decreasing utility moving right, from Identity, to Direction, to Loudness.     The last chart, labelled filtering options, shows decreasing utility moving from Identity to Direction to Loudness.", "tokens": ["Three", "column", "charts", "are", "shown", "horizontally", ".", "The", "first", "chart", "is", "labelled", "``", "Complexity", "of", "Haptic", "w/", "Visual", "''", "and", "has", "shows", "decreasing", "utility", "moving", "right", "from", "Simple", ",", "to", "Tacton", ",", "to", "none", ".", "The", "second", "chart", "is", "labelled", "Sound", "characteristics", "and", "shows", "decreasing", "utility", "moving", "right", ",", "from", "Identity", ",", "to", "Direction", ",", "to", "Loudness", ".", "The", "last", "chart", ",", "labelled", "filtering", "options", ",", "shows", "decreasing", "utility", "moving", "from", "Identity", "to", "Direction", "to", "Loudness", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Evaluating Smartwatch-based Sound Feedback for Deaf and Hard-of-hearing Users Across Contexts ", "tokens": ["Evaluating", "Smartwatch-based", "Sound", "Feedback", "for", "Deaf", "and", "Hard-of-hearing", "Users", "Across", "Contexts"]}, "filename": "fdc69738f68745f1b11b269d30f9482fb428480b_Image_008.jpg", "orig_filename": "fdc69738f68745f1b11b269d30f9482fb428480b", "split": "test"}, {"article_id": "Interacting with Literary Style through Computational Tools", "description": {"raw": "Figure 1: Two photographs and two screenshots show examples of style analysis.  From left to right: a library bookshelf holds seven books with visualizations of style as small rectangular patches on their spines.  A close-up of a hand holding an e-reader, where the thumb rests on a textured bar along the side of the case. An Amazon search results page shows three science-fiction books, with style visualizations inserted below the covers and above the title and book metadata.  A screenshot shows several paragraphs of text, to the left of which a visualization shows a gradient of color, most of which is blue, but one chunk in the middle is magenta.", "tokens": ["Figure", "1", ":", "Two", "photographs", "and", "two", "screenshots", "show", "examples", "of", "style", "analysis", ".", "From", "left", "to", "right", ":", "a", "library", "bookshelf", "holds", "seven", "books", "with", "visualizations", "of", "style", "as", "small", "rectangular", "patches", "on", "their", "spines", ".", "A", "close-up", "of", "a", "hand", "holding", "an", "e-reader", ",", "where", "the", "thumb", "rests", "on", "a", "textured", "bar", "along", "the", "side", "of", "the", "case", ".", "An", "Amazon", "search", "results", "page", "shows", "three", "science-fiction", "books", ",", "with", "style", "visualizations", "inserted", "below", "the", "covers", "and", "above", "the", "title", "and", "book", "metadata", ".", "A", "screenshot", "shows", "several", "paragraphs", "of", "text", ",", "to", "the", "left", "of", "which", "a", "visualization", "shows", "a", "gradient", "of", "color", ",", "most", "of", "which", "is", "blue", ",", "but", "one", "chunk", "in", "the", "middle", "is", "magenta", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Interacting with Literary Style through Computational Tools ", "tokens": ["Interacting", "with", "Literary", "Style", "through", "Computational", "Tools"]}, "filename": "574211ab6e038ddee2e964502fb0b8a07930805a_Image_001.jpg", "orig_filename": "574211ab6e038ddee2e964502fb0b8a07930805a", "split": "test"}, {"article_id": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard", "description": {"raw": "Trend of the AdoptRate of Auto-switch in Experiment 2. The error bars in the graph indicate standard deviation.", "tokens": ["Trend", "of", "the", "AdoptRate", "of", "Auto-switch", "in", "Experiment", "2", ".", "The", "error", "bars", "in", "the", "graph", "indicate", "standard", "deviation", "."]}, "caption": {"raw": "Figure 7: Trend of the AdoptRate of Auto-switch in Experi- ment 2. The error bars in the graph indicate standard devia- tion.", "tokens": ["Figure", "7", ":", "Trend", "of", "the", "AdoptRate", "of", "Auto-switch", "in", "Experi-", "ment", "2", ".", "The", "error", "bars", "in", "the", "graph", "indicate", "standard", "devia-", "tion", "."]}, "context": {"raw": "Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard Figure 7: Trend of the AdoptRate of Auto-switch in Experi- ment 2. The error bars in the graph indicate standard devia- tion.", "tokens": ["Diagnosing", "and", "Coping", "with", "Mode", "Errors", "in", "Korean-English", "Dual-language", "Keyboard", "Figure", "7", ":", "Trend", "of", "the", "AdoptRate", "of", "Auto-switch", "in", "Experi-", "ment", "2", ".", "The", "error", "bars", "in", "the", "graph", "indicate", "standard", "devia-", "tion", "."]}, "filename": "408a505662902bbaf20ef32ac6deaf7a78e52650_Image_012.jpg", "orig_filename": "408a505662902bbaf20ef32ac6deaf7a78e52650", "split": "test"}, {"article_id": "Which robot am I thinking about? The impact of action and appearance on people's evaluations of a moral robot", "description": {"raw": "Column graph shows results of Experiment 3: people's blame judgments for each of the four agents, for both action and inaction.", "tokens": ["Column", "graph", "shows", "results", "of", "Experiment", "3", ":", "people", "'s", "blame", "judgments", "for", "each", "of", "the", "four", "agents", ",", "for", "both", "action", "and", "inaction", "."]}, "caption": {"raw": "DISCUSSION", "tokens": ["DISCUSSION"]}, "context": {"raw": "Which robot am I thinking about? The impact of action and appearance on people's evaluations of a moral robot DISCUSSION", "tokens": ["Which", "robot", "am", "I", "thinking", "about", "?", "The", "impact", "of", "action", "and", "appearance", "on", "people", "'s", "evaluations", "of", "a", "moral", "robot", "DISCUSSION"]}, "filename": "bc2d3dfd2555c7590021db88d60d729dde30be82_Image_007.jpg", "orig_filename": "bc2d3dfd2555c7590021db88d60d729dde30be82", "split": "test"}, {"article_id": "What Makes Smartphone Use Meaningful or Meaningless?", "description": {"raw": "This bar graph shows desired frequency on the y-axis (1-5 scale). In order from highest desired frequency to lowest, the 5 U&G types are: productivity, information, communication, entertainment, and social media.", "tokens": ["This", "bar", "graph", "shows", "desired", "frequency", "on", "the", "y-axis", "(", "1-5", "scale", ")", ".", "In", "order", "from", "highest", "desired", "frequency", "to", "lowest", ",", "the", "5", "U", "&", "G", "types", "are", ":", "productivity", ",", "information", ",", "communication", ",", "entertainment", ",", "and", "social", "media", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "What Makes Smartphone Use Meaningful or Meaningless? ", "tokens": ["What", "Makes", "Smartphone", "Use", "Meaningful", "or", "Meaningless", "?"]}, "filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac_Image_007.jpg", "orig_filename": "b685aec04e942f0b78e729407fd49eb9e0f84fac", "split": "test"}, {"article_id": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions", "description": {"raw": "Prior and posterior distributions for the likelihood ratio of H0 and H+, and corresponding Bayes Factors, based on performance improvement on the transfer test and learnt vocabulary after one week compared to performance before the study. Relative likelihoods are illustrated by the pie-chart. Created with JASP", "tokens": ["Prior", "and", "posterior", "distributions", "for", "the", "likelihood", "ratio", "of", "H0", "and", "H+", ",", "and", "corresponding", "Bayes", "Factors", ",", "based", "on", "performance", "improvement", "on", "the", "transfer", "test", "and", "learnt", "vocabulary", "after", "one", "week", "compared", "to", "performance", "before", "the", "study", ".", "Relative", "likelihoods", "are", "illustrated", "by", "the", "pie-chart", ".", "Created", "with", "JASP"]}, "caption": {"raw": "(a) Transfer Improvement (b) Additional Words Recalled", "tokens": ["(", "a", ")", "Transfer", "Improvement", "(", "b", ")", "Additional", "Words", "Recalled"]}, "context": {"raw": "Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions (a) Transfer Improvement (b) Additional Words Recalled", "tokens": ["Augmented", "Reality", "to", "Enable", "Users", "in", "Learning", "Case", "Grammar", "from", "Their", "Real-World", "Interactions", "(", "a", ")", "Transfer", "Improvement", "(", "b", ")", "Additional", "Words", "Recalled"]}, "filename": "80183519a1e9c67b6996bea274cd5e6c251e6683_Image_009.jpg", "orig_filename": "80183519a1e9c67b6996bea274cd5e6c251e6683", "split": "test"}, {"article_id": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "description": {"raw": "This figure presents a line chart with the average WPM for each method, for three spatial ability groups (<=4.75, 4.76-7.0 and >=7.01). The difference in performance of the group with the best spatial ability and the other two is big for QWERTY and MultiTap. Performance in NavTouch and BrailleTouch is similar among groups.", "tokens": ["This", "figure", "presents", "a", "line", "chart", "with", "the", "average", "WPM", "for", "each", "method", ",", "for", "three", "spatial", "ability", "groups", "(", "<", "=4.75", ",", "4.76-7.0", "and", ">", "=7.01", ")", ".", "The", "difference", "in", "performance", "of", "the", "group", "with", "the", "best", "spatial", "ability", "and", "the", "other", "two", "is", "big", "for", "QWERTY", "and", "MultiTap", ".", "Performance", "in", "NavTouch", "and", "BrailleTouch", "is", "similar", "among", "groups", "."]}, "caption": {"raw": "Figure 8. Spatial ability impact on WPM.", "tokens": ["Figure", "8", ".", "Spatial", "ability", "impact", "on", "WPM", "."]}, "context": {"raw": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors Figure 8. Spatial ability impact on WPM.", "tokens": ["Blind", "people", "and", "mobile", "touch-based", "text-entry", ":", "acknowledging", "the", "need", "for", "different", "flavors", "Figure", "8", ".", "Spatial", "ability", "impact", "on", "WPM", "."]}, "filename": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_009.jpg", "orig_filename": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9", "split": "test"}, {"article_id": "How We Type: Eye and Finger Movement Strategies in Mobile Typing", "description": {"raw": "Heatmap showing finger-to-key mapping in two-thumb touch data of three participants (all sentences aggregated). These patters are representative of a tendency we found in the data for the right thumb to cover more keys than the left. The right hand was the dominant hand for most of the participants, but the same pattern was observed also for the left-handed participants.", "tokens": ["Heatmap", "showing", "finger-to-key", "mapping", "in", "two-thumb", "touch", "data", "of", "three", "participants", "(", "all", "sentences", "aggregated", ")", ".", "These", "patters", "are", "representative", "of", "a", "tendency", "we", "found", "in", "the", "data", "for", "the", "right", "thumb", "to", "cover", "more", "keys", "than", "the", "left", ".", "The", "right", "hand", "was", "the", "dominant", "hand", "for", "most", "of", "the", "participants", ",", "but", "the", "same", "pattern", "was", "observed", "also", "for", "the", "left-handed", "participants", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "How We Type: Eye and Finger Movement Strategies in Mobile Typing ", "tokens": ["How", "We", "Type", ":", "Eye", "and", "Finger", "Movement", "Strategies", "in", "Mobile", "Typing"]}, "filename": "05300590913007eb710cd89f5d373e1ec0833bfa_Image_007.jpg", "orig_filename": "05300590913007eb710cd89f5d373e1ec0833bfa", "split": "test"}, {"article_id": "Towards Understanding the Design of Positive Pre-sleep Through a Neurofeedback Artistic Experience", "description": {"raw": "In this graph, all frequency bandwidth decrease in activity across the time sample. Frequencies in order of highest to lowest activity is delta, alpha, beta, theta, ad gamma.", "tokens": ["In", "this", "graph", ",", "all", "frequency", "bandwidth", "decrease", "in", "activity", "across", "the", "time", "sample", ".", "Frequencies", "in", "order", "of", "highest", "to", "lowest", "activity", "is", "delta", ",", "alpha", ",", "beta", ",", "theta", ",", "ad", "gamma", "."]}, "caption": {"raw": "Figure 7: Time series of between-subjects (N=11) mean absolute power for frequency bandwidths delta [red], theta [purple], alpha [blue], beta [green] and gamma [orange]", "tokens": ["Figure", "7", ":", "Time", "series", "of", "between-subjects", "(", "N=11", ")", "mean", "absolute", "power", "for", "frequency", "bandwidths", "delta", "[", "red", "]", ",", "theta", "[", "purple", "]", ",", "alpha", "[", "blue", "]", ",", "beta", "[", "green", "]", "and", "gamma", "[", "orange", "]"]}, "context": {"raw": "Towards Understanding the Design of Positive Pre-sleep Through a Neurofeedback Artistic Experience Figure 7: Time series of between-subjects (N=11) mean absolute power for frequency bandwidths delta [red], theta [purple], alpha [blue], beta [green] and gamma [orange]", "tokens": ["Towards", "Understanding", "the", "Design", "of", "Positive", "Pre-sleep", "Through", "a", "Neurofeedback", "Artistic", "Experience", "Figure", "7", ":", "Time", "series", "of", "between-subjects", "(", "N=11", ")", "mean", "absolute", "power", "for", "frequency", "bandwidths", "delta", "[", "red", "]", ",", "theta", "[", "purple", "]", ",", "alpha", "[", "blue", "]", ",", "beta", "[", "green", "]", "and", "gamma", "[", "orange", "]"]}, "filename": "9bd21b56644843a04823a05de227853db6ee5f07_Image_006.jpg", "orig_filename": "9bd21b56644843a04823a05de227853db6ee5f07", "split": "test"}, {"article_id": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use", "description": {"raw": "Figure 11: A collection of three images, with each image showing a patient performing specific movements with SoPhy socks on and the SoPhy visualisation showing the data accordingly.", "tokens": ["Figure", "11", ":", "A", "collection", "of", "three", "images", ",", "with", "each", "image", "showing", "a", "patient", "performing", "specific", "movements", "with", "SoPhy", "socks", "on", "and", "the", "SoPhy", "visualisation", "showing", "the", "data", "accordingly", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Lessons Learnt from Designing a Smart Clothing Telehealth System for Hospital Use ", "tokens": ["Lessons", "Learnt", "from", "Designing", "a", "Smart", "Clothing", "Telehealth", "System", "for", "Hospital", "Use"]}, "filename": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8_Image_014.jpg", "orig_filename": "28a67ae8f0df5405a91c205dc1ce5b771058f6c8", "split": "test"}, {"article_id": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality", "description": {"raw": "The first subfigure shows bar charts of Latitude levels wrist, forearm, and elbow combined with awareness schemes known and unknown.       The height of the bar chart represents the throughput result of each combination:       wrist x known: 3.047, wrist x unknown: 2.697, forearm x known: 3.020, forearm x unknown: 2.584, elbow x known: 2.822, elbow x unknown: 2.265.       Gray lines indicate significant pairs between wrist x known vs. elbow x known, wrist x unknown vs. forearm x unknown, wrist x unknown vs. elbow x unknown, forearm x known vs. elbow x known, and forearm x unknown vs. elbow x unknown.      The second subfigure shows bar charts of Height levels close, medium, and far combined with awareness schemes known and unknown.       The height of the bar chart represents the throughput result of each combination:       close x known: 3.059, close x unknown: 2.660, medium x known: 3.014, medium x unknown: 2.565, far x known: 2.815, far x unknown: 2.321.      Gray lines indicate significant pairs between close x known vs. far x known, close x unknown vs. medium x unknown, close x unknown vs. far x unknown, medium x known vs. far x known, and medium x unknown vs. far x unknown.", "tokens": ["The", "first", "subfigure", "shows", "bar", "charts", "of", "Latitude", "levels", "wrist", ",", "forearm", ",", "and", "elbow", "combined", "with", "awareness", "schemes", "known", "and", "unknown", ".", "The", "height", "of", "the", "bar", "chart", "represents", "the", "throughput", "result", "of", "each", "combination", ":", "wrist", "x", "known", ":", "3.047", ",", "wrist", "x", "unknown", ":", "2.697", ",", "forearm", "x", "known", ":", "3.020", ",", "forearm", "x", "unknown", ":", "2.584", ",", "elbow", "x", "known", ":", "2.822", ",", "elbow", "x", "unknown", ":", "2.265", ".", "Gray", "lines", "indicate", "significant", "pairs", "between", "wrist", "x", "known", "vs.", "elbow", "x", "known", ",", "wrist", "x", "unknown", "vs.", "forearm", "x", "unknown", ",", "wrist", "x", "unknown", "vs.", "elbow", "x", "unknown", ",", "forearm", "x", "known", "vs.", "elbow", "x", "known", ",", "and", "forearm", "x", "unknown", "vs.", "elbow", "x", "unknown", ".", "The", "second", "subfigure", "shows", "bar", "charts", "of", "Height", "levels", "close", ",", "medium", ",", "and", "far", "combined", "with", "awareness", "schemes", "known", "and", "unknown", ".", "The", "height", "of", "the", "bar", "chart", "represents", "the", "throughput", "result", "of", "each", "combination", ":", "close", "x", "known", ":", "3.059", ",", "close", "x", "unknown", ":", "2.660", ",", "medium", "x", "known", ":", "3.014", ",", "medium", "x", "unknown", ":", "2.565", ",", "far", "x", "known", ":", "2.815", ",", "far", "x", "unknown", ":", "2.321", ".", "Gray", "lines", "indicate", "significant", "pairs", "between", "close", "x", "known", "vs.", "far", "x", "known", ",", "close", "x", "unknown", "vs.", "medium", "x", "unknown", ",", "close", "x", "unknown", "vs.", "far", "x", "unknown", ",", "medium", "x", "known", "vs.", "far", "x", "known", ",", "and", "medium", "x", "unknown", "vs.", "far", "x", "unknown", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality ", "tokens": ["Armstrong", ":", "An", "Empirical", "Examination", "of", "Pointing", "at", "Non-Dominant", "Arm-Anchored", "UIs", "in", "Virtual", "Reality"]}, "filename": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09_Image_008.jpg", "orig_filename": "a2c4f7c19d32134841d8fe6f90ee44a8fefe7b09", "split": "test"}, {"article_id": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "description": {"raw": "On the far left is a bar graph of the number of perceivable pins for each participant. All participants were able to perceive more pins in the lower area of the palm. On the center left is a diagram of the pin display showing the locations of pins that participants could recognize. The perceivable pins produce a shape similar to the left palm. Pins on the edges of the shape could only be perceived by participants with large hands while pins in the inside could be recognized by most (more than seven) participants. On the center right is a bar graph of the distance errors for each participant. P5 had the highest error of approximately 19~mm while P3 had the lowest error of approximately 10~mm. On the far right is a diagram of the pin display showing the average error for each perceivable pin. The error is lower towards the fingertips while it is highest at the bottom of the palm.", "tokens": ["On", "the", "far", "left", "is", "a", "bar", "graph", "of", "the", "number", "of", "perceivable", "pins", "for", "each", "participant", ".", "All", "participants", "were", "able", "to", "perceive", "more", "pins", "in", "the", "lower", "area", "of", "the", "palm", ".", "On", "the", "center", "left", "is", "a", "diagram", "of", "the", "pin", "display", "showing", "the", "locations", "of", "pins", "that", "participants", "could", "recognize", ".", "The", "perceivable", "pins", "produce", "a", "shape", "similar", "to", "the", "left", "palm", ".", "Pins", "on", "the", "edges", "of", "the", "shape", "could", "only", "be", "perceived", "by", "participants", "with", "large", "hands", "while", "pins", "in", "the", "inside", "could", "be", "recognized", "by", "most", "(", "more", "than", "seven", ")", "participants", ".", "On", "the", "center", "right", "is", "a", "bar", "graph", "of", "the", "distance", "errors", "for", "each", "participant", ".", "P5", "had", "the", "highest", "error", "of", "approximately", "19~mm", "while", "P3", "had", "the", "lowest", "error", "of", "approximately", "10~mm", ".", "On", "the", "far", "right", "is", "a", "diagram", "of", "the", "pin", "display", "showing", "the", "average", "error", "for", "each", "perceivable", "pin", ".", "The", "error", "is", "lower", "towards", "the", "fingertips", "while", "it", "is", "highest", "at", "the", "bottom", "of", "the", "palm", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects ", "tokens": ["ThroughHand", ":", "2D", "Tactile", "Interaction", "to", "Simultaneously", "Recognize", "and", "Touch", "Multiple", "Objects"]}, "filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_011.png", "orig_filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "split": "test"}, {"article_id": "REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR", "description": {"raw": "Figure 14A shows a plot of visuo-haptic latency in seconds versus device speed in cm/s. The mean latency for no redirection, limited REACH+, and REACH+ conditions are shown for each speed. The data is as follows. For 20 cm/s device speed: No Redirection = 0.22 s, Limited REACH+ = 0.13 s, REACH+ = 0.07 s. For 25 cm/s device speed: No Redirection = 0.06 s, Limited REACH+ = 0.04 s, REACH+ = 0.075 s. For 30 cm/s device speed: No Redirection = 0.02 s, Limited REACH+ = 0.03 s, REACH+ = 0.05 s. For 35 cm/s device speed: No Redirection = 0.04 s, Limited REACH+ = 0.03 s, REACH+ = 0.035 s. For 20 cm/s device speed, significant differences were found between no redirection and Limited REACH+ (p<0.001) and no redirection and REACH+ (p<0.001). Figure 14B shows a plot of on time arrival rate versus device speed in cm/s. The on time arrival rate of the EHD for no redirection, limited REACH+, and REACH+ conditions are shown for each speed. The data is as follows. For 20 cm/s device speed: No Redirection = 0.43, Limited REACH+ = 0.60, REACH+ = 0.71. For 25 cm/s device speed: No Redirection = 0.79, Limited REACH+ = 0.87, REACH+ = 0.77. For 30 cm/s device speed: No Redirection = 0.87, Limited REACH+ = 0.90, REACH+ = 0.80. For 35 cm/s device speed: No Redirection = 0.85, Limited REACH+ = 0.86, REACH+ = 0.85. For 20 cm/s device speed, significant differences were found between no redirection and Limited REACH+ (p<0.01) and no redirection and REACH+ (p<0.001).", "tokens": ["Figure", "14A", "shows", "a", "plot", "of", "visuo-haptic", "latency", "in", "seconds", "versus", "device", "speed", "in", "cm/s", ".", "The", "mean", "latency", "for", "no", "redirection", ",", "limited", "REACH+", ",", "and", "REACH+", "conditions", "are", "shown", "for", "each", "speed", ".", "The", "data", "is", "as", "follows", ".", "For", "20", "cm/s", "device", "speed", ":", "No", "Redirection", "=", "0.22", "s", ",", "Limited", "REACH+", "=", "0.13", "s", ",", "REACH+", "=", "0.07", "s.", "For", "25", "cm/s", "device", "speed", ":", "No", "Redirection", "=", "0.06", "s", ",", "Limited", "REACH+", "=", "0.04", "s", ",", "REACH+", "=", "0.075", "s.", "For", "30", "cm/s", "device", "speed", ":", "No", "Redirection", "=", "0.02", "s", ",", "Limited", "REACH+", "=", "0.03", "s", ",", "REACH+", "=", "0.05", "s.", "For", "35", "cm/s", "device", "speed", ":", "No", "Redirection", "=", "0.04", "s", ",", "Limited", "REACH+", "=", "0.03", "s", ",", "REACH+", "=", "0.035", "s.", "For", "20", "cm/s", "device", "speed", ",", "significant", "differences", "were", "found", "between", "no", "redirection", "and", "Limited", "REACH+", "(", "p", "<", "0.001", ")", "and", "no", "redirection", "and", "REACH+", "(", "p", "<", "0.001", ")", ".", "Figure", "14B", "shows", "a", "plot", "of", "on", "time", "arrival", "rate", "versus", "device", "speed", "in", "cm/s", ".", "The", "on", "time", "arrival", "rate", "of", "the", "EHD", "for", "no", "redirection", ",", "limited", "REACH+", ",", "and", "REACH+", "conditions", "are", "shown", "for", "each", "speed", ".", "The", "data", "is", "as", "follows", ".", "For", "20", "cm/s", "device", "speed", ":", "No", "Redirection", "=", "0.43", ",", "Limited", "REACH+", "=", "0.60", ",", "REACH+", "=", "0.71", ".", "For", "25", "cm/s", "device", "speed", ":", "No", "Redirection", "=", "0.79", ",", "Limited", "REACH+", "=", "0.87", ",", "REACH+", "=", "0.77", ".", "For", "30", "cm/s", "device", "speed", ":", "No", "Redirection", "=", "0.87", ",", "Limited", "REACH+", "=", "0.90", ",", "REACH+", "=", "0.80", ".", "For", "35", "cm/s", "device", "speed", ":", "No", "Redirection", "=", "0.85", ",", "Limited", "REACH+", "=", "0.86", ",", "REACH+", "=", "0.85", ".", "For", "20", "cm/s", "device", "speed", ",", "significant", "differences", "were", "found", "between", "no", "redirection", "and", "Limited", "REACH+", "(", "p", "<", "0.01", ")", "and", "no", "redirection", "and", "REACH+", "(", "p", "<", "0.001", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR ", "tokens": ["REACH+", ":", "Extending", "the", "Reachability", "of", "Encountered-type", "Haptics", "Devices", "through", "Dynamic", "Redirection", "in", "VR"]}, "filename": "91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f_Image_013.jpg", "orig_filename": "91bc2ec316eb38527582f1cf9fc35ec4df4f9f0f", "split": "test"}, {"article_id": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy", "description": {"raw": "This figure is divided into two parts: (a) and (b). \n\nPart (a) of the figure shows a stacked bar plot showing percentages of 6-point scale responses for different categories of User Initiative. The title is \"User Initiative\", the y-axis has 5 categories; \"Toggle Suggestions\", \"No Suggestions\", \"Suggestions\", \"Replace All\", and \"Automatic\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The scale responses for 1-3 are on the left side, and 4-6 are on the right side. It can be seen that for “Toggle Suggestions”, “No Suggestions”, and “Suggestions”, the majority of responses are on the right side, while for the “Replace All” and “Automatic” conditions, the majority of responses are on the left side.\n\nFor \"Toggle Suggestions\" 8.33% responded 1, 0% for 2, 0% for 3, 33.33% for 4, 33.33% for 5, and 25% for 6.\nFor \"No Suggestions\" 0% responded 1, 8.33% for 2, 8.33% for 3, 33.33% for 4, 16.67% for 5, and 33.33% for 6.\nFor \"Suggestions\" 0% responded 1, 8.33% for 2, 16.67% for 3, 16.67% for 4, 41.67% for 5, and 16.67% for 6.\nFor \"Replace All\" 8.33% responded 1, 33.33% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 0% for 6.\nFor \"Automatic\" 16.67% responded 1, 33.33% for 2, 33.33% for 3, 8.33% for 4, 8.33% for 5, and 0% for 6.\n\nPart (b) of the figure shows a stacked bar plot showing percentages of 6-point scale responses for different categories of Change Visibility. The title is \"Change Visibility\", the y-axis has 4 categories; \"Trace\", \"Pop-up\", \"No Trace\", and \"Sidebar\". The x-axis shows percentage, centered at 0, and going up to 100 percent left and right. The scale responses for 1-3 are on the left side, and 4-6 are on the right side. For “Trace” and “Pop-up”, the majority of responses are on the right side. For “No Trace” and “Sidebar” the majority of responses are on the left side.\n\nFor \"Trace\" 0% responded 1, 0% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 41.67% for 6. \nFor \"Pop-up\" 8.33% responded 1, 25% for 2, 8.33% for 3, 8.33% for 4, 16.67% for 5, and 33.33% for 6. \nFor \"No Trace\" 8.33% responded 1, 33.33% for 2, 16.67% for 3, 16.67% for 4, 25% for 5, and 41.67% for 6. \nFor \"Sidebar\" 50% responded 1, 10% for 2, 10% for 3, 20% for 4, 10% for 5, and 0% for 6.", "tokens": ["This", "figure", "is", "divided", "into", "two", "parts", ":", "(", "a", ")", "and", "(", "b", ")", ".", "Part", "(", "a", ")", "of", "the", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "6-point", "scale", "responses", "for", "different", "categories", "of", "User", "Initiative", ".", "The", "title", "is", "``", "User", "Initiative", "''", ",", "the", "y-axis", "has", "5", "categories", ";", "``", "Toggle", "Suggestions", "''", ",", "``", "No", "Suggestions", "''", ",", "``", "Suggestions", "''", ",", "``", "Replace", "All", "''", ",", "and", "``", "Automatic", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "The", "scale", "responses", "for", "1-3", "are", "on", "the", "left", "side", ",", "and", "4-6", "are", "on", "the", "right", "side", ".", "It", "can", "be", "seen", "that", "for", "“", "Toggle", "Suggestions", "”", ",", "“", "No", "Suggestions", "”", ",", "and", "“", "Suggestions", "”", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ",", "while", "for", "the", "“", "Replace", "All", "”", "and", "“", "Automatic", "”", "conditions", ",", "the", "majority", "of", "responses", "are", "on", "the", "left", "side", ".", "For", "``", "Toggle", "Suggestions", "''", "8.33", "%", "responded", "1", ",", "0", "%", "for", "2", ",", "0", "%", "for", "3", ",", "33.33", "%", "for", "4", ",", "33.33", "%", "for", "5", ",", "and", "25", "%", "for", "6", ".", "For", "``", "No", "Suggestions", "''", "0", "%", "responded", "1", ",", "8.33", "%", "for", "2", ",", "8.33", "%", "for", "3", ",", "33.33", "%", "for", "4", ",", "16.67", "%", "for", "5", ",", "and", "33.33", "%", "for", "6", ".", "For", "``", "Suggestions", "''", "0", "%", "responded", "1", ",", "8.33", "%", "for", "2", ",", "16.67", "%", "for", "3", ",", "16.67", "%", "for", "4", ",", "41.67", "%", "for", "5", ",", "and", "16.67", "%", "for", "6", ".", "For", "``", "Replace", "All", "''", "8.33", "%", "responded", "1", ",", "33.33", "%", "for", "2", ",", "16.67", "%", "for", "3", ",", "16.67", "%", "for", "4", ",", "25", "%", "for", "5", ",", "and", "0", "%", "for", "6", ".", "For", "``", "Automatic", "''", "16.67", "%", "responded", "1", ",", "33.33", "%", "for", "2", ",", "33.33", "%", "for", "3", ",", "8.33", "%", "for", "4", ",", "8.33", "%", "for", "5", ",", "and", "0", "%", "for", "6", ".", "Part", "(", "b", ")", "of", "the", "figure", "shows", "a", "stacked", "bar", "plot", "showing", "percentages", "of", "6-point", "scale", "responses", "for", "different", "categories", "of", "Change", "Visibility", ".", "The", "title", "is", "``", "Change", "Visibility", "''", ",", "the", "y-axis", "has", "4", "categories", ";", "``", "Trace", "''", ",", "``", "Pop-up", "''", ",", "``", "No", "Trace", "''", ",", "and", "``", "Sidebar", "''", ".", "The", "x-axis", "shows", "percentage", ",", "centered", "at", "0", ",", "and", "going", "up", "to", "100", "percent", "left", "and", "right", ".", "The", "scale", "responses", "for", "1-3", "are", "on", "the", "left", "side", ",", "and", "4-6", "are", "on", "the", "right", "side", ".", "For", "“", "Trace", "”", "and", "“", "Pop-up", "”", ",", "the", "majority", "of", "responses", "are", "on", "the", "right", "side", ".", "For", "“", "No", "Trace", "”", "and", "“", "Sidebar", "”", "the", "majority", "of", "responses", "are", "on", "the", "left", "side", ".", "For", "``", "Trace", "''", "0", "%", "responded", "1", ",", "0", "%", "for", "2", ",", "16.67", "%", "for", "3", ",", "16.67", "%", "for", "4", ",", "25", "%", "for", "5", ",", "and", "41.67", "%", "for", "6", ".", "For", "``", "Pop-up", "''", "8.33", "%", "responded", "1", ",", "25", "%", "for", "2", ",", "8.33", "%", "for", "3", ",", "8.33", "%", "for", "4", ",", "16.67", "%", "for", "5", ",", "and", "33.33", "%", "for", "6", ".", "For", "``", "No", "Trace", "''", "8.33", "%", "responded", "1", ",", "33.33", "%", "for", "2", ",", "16.67", "%", "for", "3", ",", "16.67", "%", "for", "4", ",", "25", "%", "for", "5", ",", "and", "41.67", "%", "for", "6", ".", "For", "``", "Sidebar", "''", "50", "%", "responded", "1", ",", "10", "%", "for", "2", ",", "10", "%", "for", "3", ",", "20", "%", "for", "4", ",", "10", "%", "for", "5", ",", "and", "0", "%", "for", "6", "."]}, "caption": {"raw": "(b)", "tokens": ["(", "b", ")"]}, "context": {"raw": "Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy (b)", "tokens": ["Automatic", "Text", "Simplification", "Tools", "for", "Deaf", "and", "Hard", "of", "Hearing", "Adults", ":", "Benefits", "of", "Lexical", "Simplification", "and", "Providing", "Users", "with", "Autonomy", "(", "b", ")"]}, "filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8_Image_003.jpg", "orig_filename": "15e11b0e326f7122d819cb95c0ad84d2a8d581c8", "split": "test"}, {"article_id": "Tilt-Responsive Techniques for Digital Drawing Boards", "description": {"raw": "Figure 1 shows the main concept of our work, using a tilt sensor on a digital drawing board to support continuous sensor-driven transitions. These include interface dualities such as reading/writing, public/private, and person-space/task-space based on the angle of the display. A \"Tilt Transfer Function\" transforms the raw input data for these applications, and a \"Tilt Side-Channel menu\" surfaces generic cross-application commands such as clutching to temporarily disengage tilt sensing from application responses, if desired.", "tokens": ["Figure", "1", "shows", "the", "main", "concept", "of", "our", "work", ",", "using", "a", "tilt", "sensor", "on", "a", "digital", "drawing", "board", "to", "support", "continuous", "sensor-driven", "transitions", ".", "These", "include", "interface", "dualities", "such", "as", "reading/writing", ",", "public/private", ",", "and", "person-space/task-space", "based", "on", "the", "angle", "of", "the", "display", ".", "A", "``", "Tilt", "Transfer", "Function", "''", "transforms", "the", "raw", "input", "data", "for", "these", "applications", ",", "and", "a", "``", "Tilt", "Side-Channel", "menu", "''", "surfaces", "generic", "cross-application", "commands", "such", "as", "clutching", "to", "temporarily", "disengage", "tilt", "sensing", "from", "application", "responses", ",", "if", "desired", "."]}, "caption": {"raw": "Figure 1. Tilting a digital drawing board from vertical to a low- angle posture transforms the current app’s user experience via continuous, interactive, sensor-driven transitions.", "tokens": ["Figure", "1", ".", "Tilting", "a", "digital", "drawing", "board", "from", "vertical", "to", "a", "low-", "angle", "posture", "transforms", "the", "current", "app", "’", "s", "user", "experience", "via", "continuous", ",", "interactive", ",", "sensor-driven", "transitions", "."]}, "context": {"raw": "Tilt-Responsive Techniques for Digital Drawing Boards Figure 1. Tilting a digital drawing board from vertical to a low- angle posture transforms the current app’s user experience via continuous, interactive, sensor-driven transitions.", "tokens": ["Tilt-Responsive", "Techniques", "for", "Digital", "Drawing", "Boards", "Figure", "1", ".", "Tilting", "a", "digital", "drawing", "board", "from", "vertical", "to", "a", "low-", "angle", "posture", "transforms", "the", "current", "app", "’", "s", "user", "experience", "via", "continuous", ",", "interactive", ",", "sensor-driven", "transitions", "."]}, "filename": "a6ee8d3cb7469494d485a718807d501b22e48be3_Image_001.jpg", "orig_filename": "a6ee8d3cb7469494d485a718807d501b22e48be3", "split": "test"}, {"article_id": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality", "description": {"raw": "Bar chart of mean score on each NASA TLX attribute for 7 pointing conditions in smartphone study. The y-axis indicates mean score and the x-axis indicates TLX attributes. For each attribute, each bar represents each pointing condition in the following sequence: VC, VBP2TR, VBP2TA, VBP1RR, WDP2TR, WDP2TA and WDP1RR. The error bar of each pointing conditions and the significant difference between each other are also visualised.", "tokens": ["Bar", "chart", "of", "mean", "score", "on", "each", "NASA", "TLX", "attribute", "for", "7", "pointing", "conditions", "in", "smartphone", "study", ".", "The", "y-axis", "indicates", "mean", "score", "and", "the", "x-axis", "indicates", "TLX", "attributes", ".", "For", "each", "attribute", ",", "each", "bar", "represents", "each", "pointing", "condition", "in", "the", "following", "sequence", ":", "VC", ",", "VBP2TR", ",", "VBP2TA", ",", "VBP1RR", ",", "WDP2TR", ",", "WDP2TA", "and", "WDP1RR", ".", "The", "error", "bar", "of", "each", "pointing", "conditions", "and", "the", "significant", "difference", "between", "each", "other", "are", "also", "visualised", "."]}, "caption": {"raw": "Figure 8. The mean responses for the attributes of NASA TLX questionnaire. The standard error bar is visualized. The statistical signiﬁcances evaluated by Wilcoxon rank sum test are marked with stars (∗∗ = p < 0.01 and ∗ = p < 0.05).", "tokens": ["Figure", "8", ".", "The", "mean", "responses", "for", "the", "attributes", "of", "NASA", "TLX", "questionnaire", ".", "The", "standard", "error", "bar", "is", "visualized", ".", "The", "statistical", "signiﬁcances", "evaluated", "by", "Wilcoxon", "rank", "sum", "test", "are", "marked", "with", "stars", "(", "∗∗", "=", "p", "<", "0.01", "and", "∗", "=", "p", "<", "0.05", ")", "."]}, "context": {"raw": "Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality Figure 8. The mean responses for the attributes of NASA TLX questionnaire. The standard error bar is visualized. The statistical signiﬁcances evaluated by Wilcoxon rank sum test are marked with stars (∗∗ = p < 0.01 and ∗ = p < 0.05).", "tokens": ["Understanding", "Viewport-", "and", "World-based", "Pointing", "with", "Everyday", "Smart", "Devices", "in", "Immersive", "Augmented", "Reality", "Figure", "8", ".", "The", "mean", "responses", "for", "the", "attributes", "of", "NASA", "TLX", "questionnaire", ".", "The", "standard", "error", "bar", "is", "visualized", ".", "The", "statistical", "signiﬁcances", "evaluated", "by", "Wilcoxon", "rank", "sum", "test", "are", "marked", "with", "stars", "(", "∗∗", "=", "p", "<", "0.01", "and", "∗", "=", "p", "<", "0.05", ")", "."]}, "filename": "7fa3a268386650d6940c6450eb41de2f28ff0a6a_Image_008.jpg", "orig_filename": "7fa3a268386650d6940c6450eb41de2f28ff0a6a", "split": "test"}, {"article_id": "In the blink of an eye: investigating latency perception during stylus interaction", "description": {"raw": "Bar graph of the JND latency thresholds gathered from each participant during the scribbling task.", "tokens": ["Bar", "graph", "of", "the", "JND", "latency", "thresholds", "gathered", "from", "each", "participant", "during", "the", "scribbling", "task", "."]}, "caption": {"raw": "Figure 11. Minimum latency perceived by participants while performing the scribbling task. Results ranged from 10 to 70 milliseconds with a median latency of 40 milliseconds.", "tokens": ["Figure", "11", ".", "Minimum", "latency", "perceived", "by", "participants", "while", "performing", "the", "scribbling", "task", ".", "Results", "ranged", "from", "10", "to", "70", "milliseconds", "with", "a", "median", "latency", "of", "40", "milliseconds", "."]}, "context": {"raw": "In the blink of an eye: investigating latency perception during stylus interaction Figure 11. Minimum latency perceived by participants while performing the scribbling task. Results ranged from 10 to 70 milliseconds with a median latency of 40 milliseconds.", "tokens": ["In", "the", "blink", "of", "an", "eye", ":", "investigating", "latency", "perception", "during", "stylus", "interaction", "Figure", "11", ".", "Minimum", "latency", "perceived", "by", "participants", "while", "performing", "the", "scribbling", "task", ".", "Results", "ranged", "from", "10", "to", "70", "milliseconds", "with", "a", "median", "latency", "of", "40", "milliseconds", "."]}, "filename": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee_Image_011.jpg", "orig_filename": "386a15fd85c162b8e4ebb6023acdce9df2bd43ee", "split": "test"}, {"article_id": "Tactile graphics with a voice: using QR codes to access text in tactile graphics", "description": {"raw": "A line chart showing the average time it took participants on all tasks. The y-axis of the chart is time in seconds (ranges from 0 to 60), the x-axis of the chart is session number (ranges from 1 to 6). There is a line for the three modes: Silent, Verbal and Finger Pointing. They all appear to be going down, but there is a big spike in the Verbal mode line at session 4. In general, the Finger Pointing mode is the highest (takes the most time), the Silent mode is next and the Verbal takes the least amount of time, although in the fourth and sixth sessions, the Verbal line is above the Silent one. There is a dot corresponding to the Braille mode at Session 6, it is between the Verbal and Silent modes.", "tokens": ["A", "line", "chart", "showing", "the", "average", "time", "it", "took", "participants", "on", "all", "tasks", ".", "The", "y-axis", "of", "the", "chart", "is", "time", "in", "seconds", "(", "ranges", "from", "0", "to", "60", ")", ",", "the", "x-axis", "of", "the", "chart", "is", "session", "number", "(", "ranges", "from", "1", "to", "6", ")", ".", "There", "is", "a", "line", "for", "the", "three", "modes", ":", "Silent", ",", "Verbal", "and", "Finger", "Pointing", ".", "They", "all", "appear", "to", "be", "going", "down", ",", "but", "there", "is", "a", "big", "spike", "in", "the", "Verbal", "mode", "line", "at", "session", "4", ".", "In", "general", ",", "the", "Finger", "Pointing", "mode", "is", "the", "highest", "(", "takes", "the", "most", "time", ")", ",", "the", "Silent", "mode", "is", "next", "and", "the", "Verbal", "takes", "the", "least", "amount", "of", "time", ",", "although", "in", "the", "fourth", "and", "sixth", "sessions", ",", "the", "Verbal", "line", "is", "above", "the", "Silent", "one", ".", "There", "is", "a", "dot", "corresponding", "to", "the", "Braille", "mode", "at", "Session", "6", ",", "it", "is", "between", "the", "Verbal", "and", "Silent", "modes", "."]}, "caption": {"raw": "Figure 4. A comparison of the average time it took for each participant to give the answer for a task for the three modes (n=10) across the six sessions as well as for Braille (n=6) on the final session.", "tokens": ["Figure", "4", ".", "A", "comparison", "of", "the", "average", "time", "it", "took", "for", "each", "participant", "to", "give", "the", "answer", "for", "a", "task", "for", "the", "three", "modes", "(", "n=10", ")", "across", "the", "six", "sessions", "as", "well", "as", "for", "Braille", "(", "n=6", ")", "on", "the", "final", "session", "."]}, "context": {"raw": "Tactile graphics with a voice: using QR codes to access text in tactile graphics Figure 4. A comparison of the average time it took for each participant to give the answer for a task for the three modes (n=10) across the six sessions as well as for Braille (n=6) on the final session.", "tokens": ["Tactile", "graphics", "with", "a", "voice", ":", "using", "QR", "codes", "to", "access", "text", "in", "tactile", "graphics", "Figure", "4", ".", "A", "comparison", "of", "the", "average", "time", "it", "took", "for", "each", "participant", "to", "give", "the", "answer", "for", "a", "task", "for", "the", "three", "modes", "(", "n=10", ")", "across", "the", "six", "sessions", "as", "well", "as", "for", "Braille", "(", "n=6", ")", "on", "the", "final", "session", "."]}, "filename": "fd420c53c56d844ccd48efd6a261907b2bc53de9_Image_009.jpg", "orig_filename": "fd420c53c56d844ccd48efd6a261907b2bc53de9", "split": "test"}, {"article_id": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input", "description": {"raw": "Perceived accuracy bar chart of phraseflow in day 2,4,and 6 of the deployment study. Results are reported in section 7.4", "tokens": ["Perceived", "accuracy", "bar", "chart", "of", "phraseflow", "in", "day", "2,4", ",", "and", "6", "of", "the", "deployment", "study", ".", "Results", "are", "reported", "in", "section", "7.4"]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "PhraseFlow: Designs and Empirical Studies of Phrase-Level Input ", "tokens": ["PhraseFlow", ":", "Designs", "and", "Empirical", "Studies", "of", "Phrase-Level", "Input"]}, "filename": "1b2d659346931daf853e0640d874557a86b4eb2a_Image_020.png", "orig_filename": "1b2d659346931daf853e0640d874557a86b4eb2a", "split": "test"}, {"article_id": "From Tactile to NavTile: Opportunities and Challenges with Multi-Modal Feedback for Guiding Surfaces during Non-Visual Navigation", "description": {"raw": "Figure 2. has four images. The first image shows a pattern of truncated domes in tactile surfaces are placed equally distant from each other. The second image shows the truncated domes in each row offset from each other. The third image shows a tactile surface with rectangular long bars parallel to walking direction, these are used for guiding. The fourth image shows a rectangular bar that are perpendicular to walking direction.", "tokens": ["Figure", "2.", "has", "four", "images", ".", "The", "first", "image", "shows", "a", "pattern", "of", "truncated", "domes", "in", "tactile", "surfaces", "are", "placed", "equally", "distant", "from", "each", "other", ".", "The", "second", "image", "shows", "the", "truncated", "domes", "in", "each", "row", "offset", "from", "each", "other", ".", "The", "third", "image", "shows", "a", "tactile", "surface", "with", "rectangular", "long", "bars", "parallel", "to", "walking", "direction", ",", "these", "are", "used", "for", "guiding", ".", "The", "fourth", "image", "shows", "a", "rectangular", "bar", "that", "are", "perpendicular", "to", "walking", "direction", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "From Tactile to NavTile: Opportunities and Challenges with Multi-Modal Feedback for Guiding Surfaces during Non-Visual Navigation ", "tokens": ["From", "Tactile", "to", "NavTile", ":", "Opportunities", "and", "Challenges", "with", "Multi-Modal", "Feedback", "for", "Guiding", "Surfaces", "during", "Non-Visual", "Navigation"]}, "filename": "276ca9ad8056f5104e65167e5ec855ecee3c4aca_Image_005.jpg", "orig_filename": "276ca9ad8056f5104e65167e5ec855ecee3c4aca", "split": "test"}, {"article_id": "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG", "description": {"raw": "Recognition accuracy continues to improve with data from additional sessions. Collecting data in different conditions is more valuable than collecting larger volumes of data in similar conditions.", "tokens": ["Recognition", "accuracy", "continues", "to", "improve", "with", "data", "from", "additional", "sessions", ".", "Collecting", "data", "in", "different", "conditions", "is", "more", "valuable", "than", "collecting", "larger", "volumes", "of", "data", "in", "similar", "conditions", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Leveraging Dual-Observable Input for Fine-Grained Thumb Interaction Using Forearm EMG ", "tokens": ["Leveraging", "Dual-Observable", "Input", "for", "Fine-Grained", "Thumb", "Interaction", "Using", "Forearm", "EMG"]}, "filename": "f007aa8f60e02b4b917f2e161733b5d2b21c56b8_Image_006.jpg", "orig_filename": "f007aa8f60e02b4b917f2e161733b5d2b21c56b8", "split": "test"}, {"article_id": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation", "description": {"raw": "Stacked bar chart showing how likely reasons for deleting messages. This graph is detailed in section 4.1. Sent by mistake, and sent to wrong recipient are the most likely reasons reported for deletion, while increase storage capacity is the least likely reason reported.", "tokens": ["Stacked", "bar", "chart", "showing", "how", "likely", "reasons", "for", "deleting", "messages", ".", "This", "graph", "is", "detailed", "in", "section", "4.1", ".", "Sent", "by", "mistake", ",", "and", "sent", "to", "wrong", "recipient", "are", "the", "most", "likely", "reasons", "reported", "for", "deletion", ",", "while", "increase", "storage", "capacity", "is", "the", "least", "likely", "reason", "reported", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation ", "tokens": ["“", "Oops", "...", "”", ":", "Mobile", "Message", "Deletion", "in", "Conversation", "Error", "and", "Regret", "Remediation"]}, "filename": "fa7c681ffd0ea94a482755623a107fce48740b0d_Image_003.jpg", "orig_filename": "fa7c681ffd0ea94a482755623a107fce48740b0d", "split": "test"}, {"article_id": "Anchored Audio Sampling: A Seamless Method for Exploring Children's Thoughts During Deployment Studies", "description": {"raw": "On the left is a list view that shows multiple file names. On the right, a dialog box is overlaid on top of the same list view, with a single file name that has been selected, a seek bar, and a play button.", "tokens": ["On", "the", "left", "is", "a", "list", "view", "that", "shows", "multiple", "file", "names", ".", "On", "the", "right", ",", "a", "dialog", "box", "is", "overlaid", "on", "top", "of", "the", "same", "list", "view", ",", "with", "a", "single", "file", "name", "that", "has", "been", "selected", ",", "a", "seek", "bar", ",", "and", "a", "play", "button", "."]}, "caption": {"raw": "Figure 3: Default UI for reviewing recordings. Left: the par- ticipant can view their complete data set. Right: Each record- ing can be reviewed.", "tokens": ["Figure", "3", ":", "Default", "UI", "for", "reviewing", "recordings", ".", "Left", ":", "the", "par-", "ticipant", "can", "view", "their", "complete", "data", "set", ".", "Right", ":", "Each", "record-", "ing", "can", "be", "reviewed", "."]}, "context": {"raw": "Anchored Audio Sampling: A Seamless Method for Exploring Children's Thoughts During Deployment Studies Figure 3: Default UI for reviewing recordings. Left: the par- ticipant can view their complete data set. Right: Each record- ing can be reviewed.", "tokens": ["Anchored", "Audio", "Sampling", ":", "A", "Seamless", "Method", "for", "Exploring", "Children", "'s", "Thoughts", "During", "Deployment", "Studies", "Figure", "3", ":", "Default", "UI", "for", "reviewing", "recordings", ".", "Left", ":", "the", "par-", "ticipant", "can", "view", "their", "complete", "data", "set", ".", "Right", ":", "Each", "record-", "ing", "can", "be", "reviewed", "."]}, "filename": "abfd76ebf841802ffc6c26cb28e9106211a658f3_Image_004.png", "orig_filename": "abfd76ebf841802ffc6c26cb28e9106211a658f3", "split": "test"}, {"article_id": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "description": {"raw": "Figure 7: An origin chart and follow-up authoring paths. The origin plot is a tick plot, displaying \"IMDB Rating\" on the x axis and \"Major Genre\" on the y axis (`genre_vs_rating = Chart(movies).field('IMDB_Rating', 'Major_Genre')` then a newline and `genre_vs_rating`). On the left, the cold path. The first query adds the \"IMDB Votes\" field (`plus_votes = genre_vs_ratings.field('IMDB_Votes')` then a newline and `plus_votes`), resulting in a binned bubble plot with \"IMDB Votes\" replacing \"IMDB Rating\" on the x axis, and \"IMDB Rating\" moving to the size channel. The second query requests \"IMDB Rating\" back on the x channel (`plus_votes2 = plus_votes.field('IMDB_Rating', channel='x')`, newline then `plus_votes2`). The result is a bubble plot with \"IMDB Rating\" and \"IMDB Votes\" switching channels. The third query requests \"IMDB Rating\" to be unbinned (`plus_votes3 = plus_votes2.field('IMDB_Rating', bin=False)`, newline then `plus_votes3`), resulting in a tick plot with color encoding \"IMDB Votes\" instead of size. The final query requests \"IMDB Votes\" to be placed on the size channel (`plus_votes3.field('IMDB_Votes', channel='size')`), resulting in a bubble plot (unbinned) with \"IMDB Rating\" on the x channel, \"Major Genre\" on y, and \"IMDB Votes\" on size. On the right, the anchored path. A single anchored query requesting an additional \"IMDB Votes\" field be added to the original visualization (`genre_vs_rating.anchor().field('IMDB_Votes')`. The result is a bubble plot identical to the final visualization in the cold query lineage.", "tokens": ["Figure", "7", ":", "An", "origin", "chart", "and", "follow-up", "authoring", "paths", ".", "The", "origin", "plot", "is", "a", "tick", "plot", ",", "displaying", "``", "IMDB", "Rating", "''", "on", "the", "x", "axis", "and", "``", "Major", "Genre", "''", "on", "the", "y", "axis", "(", "`", "genre_vs_rating", "=", "Chart", "(", "movies", ")", ".field", "(", "'IMDB_Rating", "'", ",", "'Major_Genre", "'", ")", "`", "then", "a", "newline", "and", "`", "genre_vs_rating", "`", ")", ".", "On", "the", "left", ",", "the", "cold", "path", ".", "The", "first", "query", "adds", "the", "``", "IMDB", "Votes", "''", "field", "(", "`", "plus_votes", "=", "genre_vs_ratings.field", "(", "'IMDB_Votes", "'", ")", "`", "then", "a", "newline", "and", "`", "plus_votes", "`", ")", ",", "resulting", "in", "a", "binned", "bubble", "plot", "with", "``", "IMDB", "Votes", "''", "replacing", "``", "IMDB", "Rating", "''", "on", "the", "x", "axis", ",", "and", "``", "IMDB", "Rating", "''", "moving", "to", "the", "size", "channel", ".", "The", "second", "query", "requests", "``", "IMDB", "Rating", "''", "back", "on", "the", "x", "channel", "(", "`", "plus_votes2", "=", "plus_votes.field", "(", "'IMDB_Rating", "'", ",", "channel=", "'", "x", "'", ")", "`", ",", "newline", "then", "`", "plus_votes2", "`", ")", ".", "The", "result", "is", "a", "bubble", "plot", "with", "``", "IMDB", "Rating", "''", "and", "``", "IMDB", "Votes", "''", "switching", "channels", ".", "The", "third", "query", "requests", "``", "IMDB", "Rating", "''", "to", "be", "unbinned", "(", "`", "plus_votes3", "=", "plus_votes2.field", "(", "'IMDB_Rating", "'", ",", "bin=False", ")", "`", ",", "newline", "then", "`", "plus_votes3", "`", ")", ",", "resulting", "in", "a", "tick", "plot", "with", "color", "encoding", "``", "IMDB", "Votes", "''", "instead", "of", "size", ".", "The", "final", "query", "requests", "``", "IMDB", "Votes", "''", "to", "be", "placed", "on", "the", "size", "channel", "(", "`", "plus_votes3.field", "(", "'IMDB_Votes", "'", ",", "channel='size", "'", ")", "`", ")", ",", "resulting", "in", "a", "bubble", "plot", "(", "unbinned", ")", "with", "``", "IMDB", "Rating", "''", "on", "the", "x", "channel", ",", "``", "Major", "Genre", "''", "on", "y", ",", "and", "``", "IMDB", "Votes", "''", "on", "size", ".", "On", "the", "right", ",", "the", "anchored", "path", ".", "A", "single", "anchored", "query", "requesting", "an", "additional", "``", "IMDB", "Votes", "''", "field", "be", "added", "to", "the", "original", "visualization", "(", "`", "genre_vs_rating.anchor", "(", ")", ".field", "(", "'IMDB_Votes", "'", ")", "`", ".", "The", "result", "is", "a", "bubble", "plot", "identical", "to", "the", "final", "visualization", "in", "the", "cold", "query", "lineage", "."]}, "caption": {"raw": "Figure 7. Trying to coerce cold recommendations towards a speciﬁc goal can result in a frustrating experience in which effectiveness is optimized at the expense of consistency. In this example, an anchored recommenda- tion maintains the unaggregated view of movies plotted by their IMDB Rating and Genre. On the other hand, a cold recommendation initially swaps channel assignments (removing the original visualized relation- ship) and aggregates ﬁelds (removing sample awareness). Cold queries that attempt to correct these changes can result in further deviations (in this case, changing of mark and channels) that require further adjust- ments. Anchored recommendations ease this process by reducing the number of changes made to the prior.", "tokens": ["Figure", "7", ".", "Trying", "to", "coerce", "cold", "recommendations", "towards", "a", "speciﬁc", "goal", "can", "result", "in", "a", "frustrating", "experience", "in", "which", "effectiveness", "is", "optimized", "at", "the", "expense", "of", "consistency", ".", "In", "this", "example", ",", "an", "anchored", "recommenda-", "tion", "maintains", "the", "unaggregated", "view", "of", "movies", "plotted", "by", "their", "IMDB", "Rating", "and", "Genre", ".", "On", "the", "other", "hand", ",", "a", "cold", "recommendation", "initially", "swaps", "channel", "assignments", "(", "removing", "the", "original", "visualized", "relation-", "ship", ")", "and", "aggregates", "ﬁelds", "(", "removing", "sample", "awareness", ")", ".", "Cold", "queries", "that", "attempt", "to", "correct", "these", "changes", "can", "result", "in", "further", "deviations", "(", "in", "this", "case", ",", "changing", "of", "mark", "and", "channels", ")", "that", "require", "further", "adjust-", "ments", ".", "Anchored", "recommendations", "ease", "this", "process", "by", "reducing", "the", "number", "of", "changes", "made", "to", "the", "prior", "."]}, "context": {"raw": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations Figure 7. Trying to coerce cold recommendations towards a speciﬁc goal can result in a frustrating experience in which effectiveness is optimized at the expense of consistency. In this example, an anchored recommenda- tion maintains the unaggregated view of movies plotted by their IMDB Rating and Genre. On the other hand, a cold recommendation initially swaps channel assignments (removing the original visualized relation- ship) and aggregates ﬁelds (removing sample awareness). Cold queries that attempt to correct these changes can result in further deviations (in this case, changing of mark and channels) that require further adjust- ments. Anchored recommendations ease this process by reducing the number of changes made to the prior.", "tokens": ["Dziban", ":", "Balancing", "Agency", "&", "Automation", "in", "Visualization", "Design", "via", "Anchored", "Recommendations", "Figure", "7", ".", "Trying", "to", "coerce", "cold", "recommendations", "towards", "a", "speciﬁc", "goal", "can", "result", "in", "a", "frustrating", "experience", "in", "which", "effectiveness", "is", "optimized", "at", "the", "expense", "of", "consistency", ".", "In", "this", "example", ",", "an", "anchored", "recommenda-", "tion", "maintains", "the", "unaggregated", "view", "of", "movies", "plotted", "by", "their", "IMDB", "Rating", "and", "Genre", ".", "On", "the", "other", "hand", ",", "a", "cold", "recommendation", "initially", "swaps", "channel", "assignments", "(", "removing", "the", "original", "visualized", "relation-", "ship", ")", "and", "aggregates", "ﬁelds", "(", "removing", "sample", "awareness", ")", ".", "Cold", "queries", "that", "attempt", "to", "correct", "these", "changes", "can", "result", "in", "further", "deviations", "(", "in", "this", "case", ",", "changing", "of", "mark", "and", "channels", ")", "that", "require", "further", "adjust-", "ments", ".", "Anchored", "recommendations", "ease", "this", "process", "by", "reducing", "the", "number", "of", "changes", "made", "to", "the", "prior", "."]}, "filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_031.jpg", "orig_filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "split": "test"}, {"article_id": "Designing and Evaluating Livefonts", "description": {"raw": "Box-plot of Time Ration (script/Latin). Scripts plotted, in order: Matilda, Version 1, Version 2, Tricolor Braille, Armenian, Hebrew, Arabic, Devangari, Chinese. Low-vision and sighted groups are plotted separately for each. Medians range from about 1 for Matilda to about 3 for Chinese. The spread for Chinese is the largest by far.", "tokens": ["Box-plot", "of", "Time", "Ration", "(", "script/Latin", ")", ".", "Scripts", "plotted", ",", "in", "order", ":", "Matilda", ",", "Version", "1", ",", "Version", "2", ",", "Tricolor", "Braille", ",", "Armenian", ",", "Hebrew", ",", "Arabic", ",", "Devangari", ",", "Chinese", ".", "Low-vision", "and", "sighted", "groups", "are", "plotted", "separately", "for", "each", ".", "Medians", "range", "from", "about", "1", "for", "Matilda", "to", "about", "3", "for", "Chinese", ".", "The", "spread", "for", "Chinese", "is", "the", "largest", "by", "far", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Designing and Evaluating Livefonts ", "tokens": ["Designing", "and", "Evaluating", "Livefonts"]}, "filename": "0bc157f953ea87ab91323fd70ff95af28332e818_Image_012.jpg", "orig_filename": "0bc157f953ea87ab91323fd70ff95af28332e818", "split": "test"}, {"article_id": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People", "description": {"raw": "The bar chart shows the average selection time (y-axis, in milliseconds) for all feedback conditions, and contains 10 vertical bars, one for each condition. The x-axis groups bars by the feedback designs, from left: Geiger, Pitch, Constant and Control. There are three coloured bars in the Geiger, Pitch and Constant, one for each of the Wrist, Object and Both locations. The Control Design only has a single bar (for Object). The values, from left to right, are as follows: Geiger-Wrist = 4881ms, Geiger-Object = 4686ms, Geiger-Both = 4020ms; Pitch-Wrist = 3938ms, Pitch-Object = 3107ms, Pitch-Both = 4268ms; Constant-Wrist = 3195%, Constant-Object = 3568ms, Constant-Both = 3972ms; Control-Object = 2666ms.", "tokens": ["The", "bar", "chart", "shows", "the", "average", "selection", "time", "(", "y-axis", ",", "in", "milliseconds", ")", "for", "all", "feedback", "conditions", ",", "and", "contains", "10", "vertical", "bars", ",", "one", "for", "each", "condition", ".", "The", "x-axis", "groups", "bars", "by", "the", "feedback", "designs", ",", "from", "left", ":", "Geiger", ",", "Pitch", ",", "Constant", "and", "Control", ".", "There", "are", "three", "coloured", "bars", "in", "the", "Geiger", ",", "Pitch", "and", "Constant", ",", "one", "for", "each", "of", "the", "Wrist", ",", "Object", "and", "Both", "locations", ".", "The", "Control", "Design", "only", "has", "a", "single", "bar", "(", "for", "Object", ")", ".", "The", "values", ",", "from", "left", "to", "right", ",", "are", "as", "follows", ":", "Geiger-Wrist", "=", "4881ms", ",", "Geiger-Object", "=", "4686ms", ",", "Geiger-Both", "=", "4020ms", ";", "Pitch-Wrist", "=", "3938ms", ",", "Pitch-Object", "=", "3107ms", ",", "Pitch-Both", "=", "4268ms", ";", "Constant-Wrist", "=", "3195", "%", ",", "Constant-Object", "=", "3568ms", ",", "Constant-Both", "=", "3972ms", ";", "Control-Object", "=", "2666ms", "."]}, "caption": {"raw": "Figure 6: Mean Study 1 target selection times for all Feedback Designs and Speaker Locations. Error bars = 95% CI.", "tokens": ["Figure", "6", ":", "Mean", "Study", "1", "target", "selection", "times", "for", "all", "Feedback", "Designs", "and", "Speaker", "Locations", ".", "Error", "bars", "=", "95", "%", "CI", "."]}, "context": {"raw": "Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People Figure 6: Mean Study 1 target selection times for all Feedback Designs and Speaker Locations. Error bars = 95% CI.", "tokens": ["Using", "Dynamic", "Audio", "Feedback", "to", "Support", "Peripersonal", "Reaching", "in", "Young", "Visually", "Impaired", "People", "Figure", "6", ":", "Mean", "Study", "1", "target", "selection", "times", "for", "all", "Feedback", "Designs", "and", "Speaker", "Locations", ".", "Error", "bars", "=", "95", "%", "CI", "."]}, "filename": "09f444597b50aebe550b5efd1368a55953d8ddd1_Image_006.jpg", "orig_filename": "09f444597b50aebe550b5efd1368a55953d8ddd1", "split": "test"}, {"article_id": "Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints", "description": {"raw": "The planning process includes 5 steps: 1) Exercise guidelines overview (amount, balance, pattern, progression, compatibility), 2) Feature tour for making agood plan (e.g. a bar showing blaance of cardio and strength and explanation of it's meaning to the user), 3) Review client profile (e.g. goals for next week, long term goals, constraints, access, activities they like, dislike, or wnat to try), 4) schedule activities (a calendar showing the client schedule and exercise appointments, and information about calories burned if plan is followed, and strength and cardio distribution), 5) Plan is ready for client (overview of a plan by day, with activity name, time, duration, distribution of strength and cario, reasoning for why the activity matches the client, how to prepare for the activity, alternatives for the activity)", "tokens": ["The", "planning", "process", "includes", "5", "steps", ":", "1", ")", "Exercise", "guidelines", "overview", "(", "amount", ",", "balance", ",", "pattern", ",", "progression", ",", "compatibility", ")", ",", "2", ")", "Feature", "tour", "for", "making", "agood", "plan", "(", "e.g", ".", "a", "bar", "showing", "blaance", "of", "cardio", "and", "strength", "and", "explanation", "of", "it", "'s", "meaning", "to", "the", "user", ")", ",", "3", ")", "Review", "client", "profile", "(", "e.g", ".", "goals", "for", "next", "week", ",", "long", "term", "goals", ",", "constraints", ",", "access", ",", "activities", "they", "like", ",", "dislike", ",", "or", "wnat", "to", "try", ")", ",", "4", ")", "schedule", "activities", "(", "a", "calendar", "showing", "the", "client", "schedule", "and", "exercise", "appointments", ",", "and", "information", "about", "calories", "burned", "if", "plan", "is", "followed", ",", "and", "strength", "and", "cardio", "distribution", ")", ",", "5", ")", "Plan", "is", "ready", "for", "client", "(", "overview", "of", "a", "plan", "by", "day", ",", "with", "activity", "name", ",", "time", ",", "duration", ",", "distribution", "of", "strength", "and", "cario", ",", "reasoning", "for", "why", "the", "activity", "matches", "the", "client", ",", "how", "to", "prepare", "for", "the", "activity", ",", "alternatives", "for", "the", "activity", ")"]}, "caption": {"raw": "Figure 1. In CrowdFit, planners schedule exercise activities following expert guidelines. Planners are first given a tour of the interactive system and presented with the guidelines, then review the client’s profile, and schedule activities which fit within the client’s schedule and match their preferences.", "tokens": ["Figure", "1", ".", "In", "CrowdFit", ",", "planners", "schedule", "exercise", "activities", "following", "expert", "guidelines", ".", "Planners", "are", "first", "given", "a", "tour", "of", "the", "interactive", "system", "and", "presented", "with", "the", "guidelines", ",", "then", "review", "the", "client", "’", "s", "profile", ",", "and", "schedule", "activities", "which", "fit", "within", "the", "client", "’", "s", "schedule", "and", "match", "their", "preferences", "."]}, "context": {"raw": "Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints Figure 1. In CrowdFit, planners schedule exercise activities following expert guidelines. Planners are first given a tour of the interactive system and presented with the guidelines, then review the client’s profile, and schedule activities which fit within the client’s schedule and match their preferences.", "tokens": ["Crowdsourcing", "Exercise", "Plans", "Aligned", "with", "Expert", "Guidelines", "and", "Everyday", "Constraints", "Figure", "1", ".", "In", "CrowdFit", ",", "planners", "schedule", "exercise", "activities", "following", "expert", "guidelines", ".", "Planners", "are", "first", "given", "a", "tour", "of", "the", "interactive", "system", "and", "presented", "with", "the", "guidelines", ",", "then", "review", "the", "client", "’", "s", "profile", ",", "and", "schedule", "activities", "which", "fit", "within", "the", "client", "’", "s", "schedule", "and", "match", "their", "preferences", "."]}, "filename": "87f5eb86ba8eb8ccd444326ff265e883f7c99948_Image_001.jpg", "orig_filename": "87f5eb86ba8eb8ccd444326ff265e883f7c99948", "split": "test"}, {"article_id": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards", "description": {"raw": "Box chart of durations for tasks 1-11. Time in minutes on the Y-axis, and task numbers on the X-axis. There is a higher variance in duration for task 4 than for the rest of the tasks, and it ranges from 0.25 to just over 4 minutes, with the majority between 1 and 2.5 minutes. Tasks 8 and 10 have noticeably short durations, close to zero, with little variance. 1, 2, and 11 range between 0 and 0.5 minutes. 3, 5, and 7 range between 0.25 and 1.25 minutes, and 6 and 9 reach nearly 1.5 minutes.    Box chart of durations for tasks 2-23. Half of the tasks (12, 13, 14, 15, 16, and 17) have an extremely high variance. The boxes for 12, 16, and 17 range from around 1.25 minutes to 5 minutes. 13 has whiskers from 1 to 5, and the box is between 2 and 4. 14 has whiskers from 0 to 5 (the entire Y axis) and a box between 1.25 and 4.5. 15 is lower, the box ranging between 0.5 and 3 minutes. Tasks 19, and 21 have noticeably short durations, between nearly zero and 0.25 minutes, with little variance. 18, 20, 22, and 23 have boxes which span 1.5 minutes; 18 ranges from almost 0 up to 1.25, 22 ranges from 1.5 to 3 with a whisker extending below 0.5. 22 and 23 are nearly identical, ranging from about 0.75 to 2.5 minutes.", "tokens": ["Box", "chart", "of", "durations", "for", "tasks", "1-11", ".", "Time", "in", "minutes", "on", "the", "Y-axis", ",", "and", "task", "numbers", "on", "the", "X-axis", ".", "There", "is", "a", "higher", "variance", "in", "duration", "for", "task", "4", "than", "for", "the", "rest", "of", "the", "tasks", ",", "and", "it", "ranges", "from", "0.25", "to", "just", "over", "4", "minutes", ",", "with", "the", "majority", "between", "1", "and", "2.5", "minutes", ".", "Tasks", "8", "and", "10", "have", "noticeably", "short", "durations", ",", "close", "to", "zero", ",", "with", "little", "variance", ".", "1", ",", "2", ",", "and", "11", "range", "between", "0", "and", "0.5", "minutes", ".", "3", ",", "5", ",", "and", "7", "range", "between", "0.25", "and", "1.25", "minutes", ",", "and", "6", "and", "9", "reach", "nearly", "1.5", "minutes", ".", "Box", "chart", "of", "durations", "for", "tasks", "2-23", ".", "Half", "of", "the", "tasks", "(", "12", ",", "13", ",", "14", ",", "15", ",", "16", ",", "and", "17", ")", "have", "an", "extremely", "high", "variance", ".", "The", "boxes", "for", "12", ",", "16", ",", "and", "17", "range", "from", "around", "1.25", "minutes", "to", "5", "minutes", ".", "13", "has", "whiskers", "from", "1", "to", "5", ",", "and", "the", "box", "is", "between", "2", "and", "4", ".", "14", "has", "whiskers", "from", "0", "to", "5", "(", "the", "entire", "Y", "axis", ")", "and", "a", "box", "between", "1.25", "and", "4.5", ".", "15", "is", "lower", ",", "the", "box", "ranging", "between", "0.5", "and", "3", "minutes", ".", "Tasks", "19", ",", "and", "21", "have", "noticeably", "short", "durations", ",", "between", "nearly", "zero", "and", "0.25", "minutes", ",", "with", "little", "variance", ".", "18", ",", "20", ",", "22", ",", "and", "23", "have", "boxes", "which", "span", "1.5", "minutes", ";", "18", "ranges", "from", "almost", "0", "up", "to", "1.25", ",", "22", "ranges", "from", "1.5", "to", "3", "with", "a", "whisker", "extending", "below", "0.5", ".", "22", "and", "23", "are", "nearly", "identical", ",", "ranging", "from", "about", "0.75", "to", "2.5", "minutes", "."]}, "caption": {"raw": "Task durations for the interpretive tasks, #1-11.Task durations for the generative tasks, #12-23.", "tokens": ["Task", "durations", "for", "the", "interpretive", "tasks", ",", "#", "1-11.Task", "durations", "for", "the", "generative", "tasks", ",", "#", "12-23", "."]}, "context": {"raw": "Understanding Blind Screen-Reader Users’ Experiences of Digital Artboards Task durations for the interpretive tasks, #1-11.Task durations for the generative tasks, #12-23.", "tokens": ["Understanding", "Blind", "Screen-Reader", "Users", "’", "Experiences", "of", "Digital", "Artboards", "Task", "durations", "for", "the", "interpretive", "tasks", ",", "#", "1-11.Task", "durations", "for", "the", "generative", "tasks", ",", "#", "12-23", "."]}, "filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4_Image_011.jpg", "orig_filename": "88b5f0341afb94f658ade33d8cef8a30bde358b4", "split": "test"}, {"article_id": "CymaSense: A Novel Audio-Visual Therapeutic Tool for People on the Autism Spectrum", "description": {"raw": "This line graph shows the trends for musical, non-musical and combined mean scores for all participants over the eight therapy sessions.", "tokens": ["This", "line", "graph", "shows", "the", "trends", "for", "musical", ",", "non-musical", "and", "combined", "mean", "scores", "for", "all", "participants", "over", "the", "eight", "therapy", "sessions", "."]}, "caption": {"raw": "Figure 7: One-to-one and group mean scores over 8 sessions without G1 included. Musical scores ranged from 23 to 77 including G1 data, 23 to 62 without, with mean scores of 35", "tokens": ["Figure", "7", ":", "One-to-one", "and", "group", "mean", "scores", "over", "8", "sessions", "without", "G1", "included", ".", "Musical", "scores", "ranged", "from", "23", "to", "77", "including", "G1", "data", ",", "23", "to", "62", "without", ",", "with", "mean", "scores", "of", "35"]}, "context": {"raw": "CymaSense: A Novel Audio-Visual Therapeutic Tool for People on the Autism Spectrum Figure 7: One-to-one and group mean scores over 8 sessions without G1 included. Musical scores ranged from 23 to 77 including G1 data, 23 to 62 without, with mean scores of 35", "tokens": ["CymaSense", ":", "A", "Novel", "Audio-Visual", "Therapeutic", "Tool", "for", "People", "on", "the", "Autism", "Spectrum", "Figure", "7", ":", "One-to-one", "and", "group", "mean", "scores", "over", "8", "sessions", "without", "G1", "included", ".", "Musical", "scores", "ranged", "from", "23", "to", "77", "including", "G1", "data", ",", "23", "to", "62", "without", ",", "with", "mean", "scores", "of", "35"]}, "filename": "d1d8b26974a672a00ac297c4910d279cb8f13a7c_Image_007.jpg", "orig_filename": "d1d8b26974a672a00ac297c4910d279cb8f13a7c", "split": "test"}, {"article_id": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation", "description": {"raw": "Stacked bar chart showing participants (senders) reasons for the message being deleted, separated by group and pairwise chats. The chart is detailed in section 5.1 and shows most reported reason was that the content was incorrect.", "tokens": ["Stacked", "bar", "chart", "showing", "participants", "(", "senders", ")", "reasons", "for", "the", "message", "being", "deleted", ",", "separated", "by", "group", "and", "pairwise", "chats", ".", "The", "chart", "is", "detailed", "in", "section", "5.1", "and", "shows", "most", "reported", "reason", "was", "that", "the", "content", "was", "incorrect", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation ", "tokens": ["“", "Oops", "...", "”", ":", "Mobile", "Message", "Deletion", "in", "Conversation", "Error", "and", "Regret", "Remediation"]}, "filename": "fa7c681ffd0ea94a482755623a107fce48740b0d_Image_009.png", "orig_filename": "fa7c681ffd0ea94a482755623a107fce48740b0d", "split": "test"}, {"article_id": "Opportunities for In-Home Augmented Reality Guidance", "description": {"raw": "A box and whisker plot showing NASA Task Load Index score in each of six categories and the average.", "tokens": ["A", "box", "and", "whisker", "plot", "showing", "NASA", "Task", "Load", "Index", "score", "in", "each", "of", "six", "categories", "and", "the", "average", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Opportunities for In-Home Augmented Reality Guidance ", "tokens": ["Opportunities", "for", "In-Home", "Augmented", "Reality", "Guidance"]}, "filename": "1ac54dd26a3dfd9af0a0d9276a8f4af4a710004d_Image_005.png", "orig_filename": "1ac54dd26a3dfd9af0a0d9276a8f4af4a710004d", "split": "test"}, {"article_id": "Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems", "description": {"raw": "Example plot showcasing the waveforms for facing and not facing, the FFT with HLBR features and the cross correlation between the wave forms.", "tokens": ["Example", "plot", "showcasing", "the", "waveforms", "for", "facing", "and", "not", "facing", ",", "the", "FFT", "with", "HLBR", "features", "and", "the", "cross", "correlation", "between", "the", "wave", "forms", "."]}, "caption": {"raw": "Figure 3. Example waveform, FFT, and cross correlation for an example “facing” and “not facing” trial (all other factors same).", "tokens": ["Figure", "3", ".", "Example", "waveform", ",", "FFT", ",", "and", "cross", "correlation", "for", "an", "example", "“", "facing", "”", "and", "“", "not", "facing", "”", "trial", "(", "all", "other", "factors", "same", ")", "."]}, "context": {"raw": "Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems Figure 3. Example waveform, FFT, and cross correlation for an example “facing” and “not facing” trial (all other factors same).", "tokens": ["Direction-of-Voice", "(", "DoV", ")", "Estimation", "for", "Intuitive", "Speech", "Interaction", "with", "Smart", "Devices", "Ecosystems", "Figure", "3", ".", "Example", "waveform", ",", "FFT", ",", "and", "cross", "correlation", "for", "an", "example", "“", "facing", "”", "and", "“", "not", "facing", "”", "trial", "(", "all", "other", "factors", "same", ")", "."]}, "filename": "b5cff6a8e605b87e828405a2bb7cbdd0275470d0_Image_004.jpg", "orig_filename": "b5cff6a8e605b87e828405a2bb7cbdd0275470d0", "split": "test"}, {"article_id": "Multimodal Deep Learning using Images and Text for Information Graphic Classification", "description": {"raw": "This figure contains two line graphs, a and b.  The a line graph represents the vision testing and training accuracy of the network over 80 epochs.  The training data rises consistently, but the testing accuracy maxes out at around 25 epochs.  In graph b, the line graph shows the training and testing accuracy of the bag of words model.  The training accuracy rises consistently, but the testing accuracy maxes out at round 45% at epoch 30 and slightly falls afterward.", "tokens": ["This", "figure", "contains", "two", "line", "graphs", ",", "a", "and", "b", ".", "The", "a", "line", "graph", "represents", "the", "vision", "testing", "and", "training", "accuracy", "of", "the", "network", "over", "80", "epochs", ".", "The", "training", "data", "rises", "consistently", ",", "but", "the", "testing", "accuracy", "maxes", "out", "at", "around", "25", "epochs", ".", "In", "graph", "b", ",", "the", "line", "graph", "shows", "the", "training", "and", "testing", "accuracy", "of", "the", "bag", "of", "words", "model", ".", "The", "training", "accuracy", "rises", "consistently", ",", "but", "the", "testing", "accuracy", "maxes", "out", "at", "round", "45", "%", "at", "epoch", "30", "and", "slightly", "falls", "afterward", "."]}, "caption": {"raw": "Vision CNN Accuracy          (b) Bag of WordsAccuracyFigure 5. Training accuracy of the neural network using (a) just the vision part of the network, and (b) just the text part of the network. The maximum accuracy of the vision network is 69.67% obtained after 30 epochs. The maximum of the text network is 45% obtain after 32 epochs. Even though the training accuracy continues to trend upwards, the network begins to overfit to the training data as the network fails to generalize and improve on the testing data.Multimodal Accuracy", "tokens": ["Vision", "CNN", "Accuracy", "(", "b", ")", "Bag", "of", "WordsAccuracyFigure", "5", ".", "Training", "accuracy", "of", "the", "neural", "network", "using", "(", "a", ")", "just", "the", "vision", "part", "of", "the", "network", ",", "and", "(", "b", ")", "just", "the", "text", "part", "of", "the", "network", ".", "The", "maximum", "accuracy", "of", "the", "vision", "network", "is", "69.67", "%", "obtained", "after", "30", "epochs", ".", "The", "maximum", "of", "the", "text", "network", "is", "45", "%", "obtain", "after", "32", "epochs", ".", "Even", "though", "the", "training", "accuracy", "continues", "to", "trend", "upwards", ",", "the", "network", "begins", "to", "overfit", "to", "the", "training", "data", "as", "the", "network", "fails", "to", "generalize", "and", "improve", "on", "the", "testing", "data.Multimodal", "Accuracy"]}, "context": {"raw": "Multimodal Deep Learning using Images and Text for Information Graphic Classification Vision CNN Accuracy          (b) Bag of WordsAccuracyFigure 5. Training accuracy of the neural network using (a) just the vision part of the network, and (b) just the text part of the network. The maximum accuracy of the vision network is 69.67% obtained after 30 epochs. The maximum of the text network is 45% obtain after 32 epochs. Even though the training accuracy continues to trend upwards, the network begins to overfit to the training data as the network fails to generalize and improve on the testing data.Multimodal Accuracy", "tokens": ["Multimodal", "Deep", "Learning", "using", "Images", "and", "Text", "for", "Information", "Graphic", "Classification", "Vision", "CNN", "Accuracy", "(", "b", ")", "Bag", "of", "WordsAccuracyFigure", "5", ".", "Training", "accuracy", "of", "the", "neural", "network", "using", "(", "a", ")", "just", "the", "vision", "part", "of", "the", "network", ",", "and", "(", "b", ")", "just", "the", "text", "part", "of", "the", "network", ".", "The", "maximum", "accuracy", "of", "the", "vision", "network", "is", "69.67", "%", "obtained", "after", "30", "epochs", ".", "The", "maximum", "of", "the", "text", "network", "is", "45", "%", "obtain", "after", "32", "epochs", ".", "Even", "though", "the", "training", "accuracy", "continues", "to", "trend", "upwards", ",", "the", "network", "begins", "to", "overfit", "to", "the", "training", "data", "as", "the", "network", "fails", "to", "generalize", "and", "improve", "on", "the", "testing", "data.Multimodal", "Accuracy"]}, "filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d_Image_007.jpg", "orig_filename": "29ce15f6520d7427cf1c0cce62e49fca0f40c19d", "split": "test"}, {"article_id": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics", "description": {"raw": "Figure 3. Plots with feature vectors for the different gestures and participants. The plot shows data from one repetition (out of 9) for the 12 participants (horizontal axis) for the 8 gestures (vertical axis). Each sub-image shows a plot of 16 overlaid feature vectors, which has been interpolated to 80 observations over time. Participants performed gestures without feedback and in their own style, which required user-dependent classification. Some potential issues can be seen in the time series:", "tokens": ["Figure", "3", ".", "Plots", "with", "feature", "vectors", "for", "the", "different", "gestures", "and", "participants", ".", "The", "plot", "shows", "data", "from", "one", "repetition", "(", "out", "of", "9", ")", "for", "the", "12", "participants", "(", "horizontal", "axis", ")", "for", "the", "8", "gestures", "(", "vertical", "axis", ")", ".", "Each", "sub-image", "shows", "a", "plot", "of", "16", "overlaid", "feature", "vectors", ",", "which", "has", "been", "interpolated", "to", "80", "observations", "over", "time", ".", "Participants", "performed", "gestures", "without", "feedback", "and", "in", "their", "own", "style", ",", "which", "required", "user-dependent", "classification", ".", "Some", "potential", "issues", "can", "be", "seen", "in", "the", "time", "series", ":"]}, "caption": {"raw": "Figure 3: Based on the gesture elicitation, we choose to support three gesture classes (Flick, Slide, Grasp), which represent 83.3% of the elicited gestures. The plot shows data from one repetition (out of nine) for the 12 participants (horizontal axis) for the eight gestures (vertical axis). Each sub-image shows a plot of 16 overlaid feature vectors, which has been interpolated to 80 observations over time. Participants performed gestures without feedback and in their own style, which required user-dependent classification. Some potential issues can be seen in the time series:", "tokens": ["Figure", "3", ":", "Based", "on", "the", "gesture", "elicitation", ",", "we", "choose", "to", "support", "three", "gesture", "classes", "(", "Flick", ",", "Slide", ",", "Grasp", ")", ",", "which", "represent", "83.3", "%", "of", "the", "elicited", "gestures", ".", "The", "plot", "shows", "data", "from", "one", "repetition", "(", "out", "of", "nine", ")", "for", "the", "12", "participants", "(", "horizontal", "axis", ")", "for", "the", "eight", "gestures", "(", "vertical", "axis", ")", ".", "Each", "sub-image", "shows", "a", "plot", "of", "16", "overlaid", "feature", "vectors", ",", "which", "has", "been", "interpolated", "to", "80", "observations", "over", "time", ".", "Participants", "performed", "gestures", "without", "feedback", "and", "in", "their", "own", "style", ",", "which", "required", "user-dependent", "classification", ".", "Some", "potential", "issues", "can", "be", "seen", "in", "the", "time", "series", ":"]}, "context": {"raw": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics Figure 3: Based on the gesture elicitation, we choose to support three gesture classes (Flick, Slide, Grasp), which represent 83.3% of the elicited gestures. The plot shows data from one repetition (out of nine) for the 12 participants (horizontal axis) for the eight gestures (vertical axis). Each sub-image shows a plot of 16 overlaid feature vectors, which has been interpolated to 80 observations over time. Participants performed gestures without feedback and in their own style, which required user-dependent classification. Some potential issues can be seen in the time series:", "tokens": ["E-Textile", "Microinteractions", ":", "Augmenting", "Twist", "with", "Flick", ",", "Slide", "and", "Grasp", "Gestures", "for", "Soft", "Electronics", "Figure", "3", ":", "Based", "on", "the", "gesture", "elicitation", ",", "we", "choose", "to", "support", "three", "gesture", "classes", "(", "Flick", ",", "Slide", ",", "Grasp", ")", ",", "which", "represent", "83.3", "%", "of", "the", "elicited", "gestures", ".", "The", "plot", "shows", "data", "from", "one", "repetition", "(", "out", "of", "nine", ")", "for", "the", "12", "participants", "(", "horizontal", "axis", ")", "for", "the", "eight", "gestures", "(", "vertical", "axis", ")", ".", "Each", "sub-image", "shows", "a", "plot", "of", "16", "overlaid", "feature", "vectors", ",", "which", "has", "been", "interpolated", "to", "80", "observations", "over", "time", ".", "Participants", "performed", "gestures", "without", "feedback", "and", "in", "their", "own", "style", ",", "which", "required", "user-dependent", "classification", ".", "Some", "potential", "issues", "can", "be", "seen", "in", "the", "time", "series", ":"]}, "filename": "f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_005.jpg", "orig_filename": "f06a381ef1ff7d02a6a1876548e83db20e71442f", "split": "val"}, {"article_id": "Faster Command Selection on Touchscreen Watches", "description": {"raw": "Photos of two demo applications, each with a WristTap menu open. The music player application's menu has controls for play/pause, next track, previous track, favorite, volume up, and volume down. The home screen application's menu has toggles for wifi, cell data, notifications, gps, airplane mode, and a button to open settings.", "tokens": ["Photos", "of", "two", "demo", "applications", ",", "each", "with", "a", "WristTap", "menu", "open", ".", "The", "music", "player", "application", "'s", "menu", "has", "controls", "for", "play/pause", ",", "next", "track", ",", "previous", "track", ",", "favorite", ",", "volume", "up", ",", "and", "volume", "down", ".", "The", "home", "screen", "application", "'s", "menu", "has", "toggles", "for", "wifi", ",", "cell", "data", ",", "notifications", ",", "gps", ",", "airplane", "mode", ",", "and", "a", "button", "to", "open", "settings", "."]}, "caption": {"raw": "Figure 4. Demo applications: a music player (left), and a home screen application over the native watch face (right).", "tokens": ["Figure", "4", ".", "Demo", "applications", ":", "a", "music", "player", "(", "left", ")", ",", "and", "a", "home", "screen", "application", "over", "the", "native", "watch", "face", "(", "right", ")", "."]}, "context": {"raw": "Faster Command Selection on Touchscreen Watches Figure 4. Demo applications: a music player (left), and a home screen application over the native watch face (right).", "tokens": ["Faster", "Command", "Selection", "on", "Touchscreen", "Watches", "Figure", "4", ".", "Demo", "applications", ":", "a", "music", "player", "(", "left", ")", ",", "and", "a", "home", "screen", "application", "over", "the", "native", "watch", "face", "(", "right", ")", "."]}, "filename": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4_Image_004.jpg", "orig_filename": "6adc7c33566402e7aec97e9b7039ba4921fcf8d4", "split": "val"}, {"article_id": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students", "description": {"raw": "Fig.2  A complex flowchart of the detailed pipeline as described in Section 4.3. The upper part of the flow chart described the procedure of feature selection, rule mining and rule selection (applied on common rules and unique rules respectively.) of the RuleGenerateSet. Then, the output rules are applied on the TrainTestSet to extract contextually filtered features for machine learning training.  The rule selection and the contextually filtered features are the key contributions of this paper.", "tokens": ["Fig.2", "A", "complex", "flowchart", "of", "the", "detailed", "pipeline", "as", "described", "in", "Section", "4.3", ".", "The", "upper", "part", "of", "the", "flow", "chart", "described", "the", "procedure", "of", "feature", "selection", ",", "rule", "mining", "and", "rule", "selection", "(", "applied", "on", "common", "rules", "and", "unique", "rules", "respectively", ".", ")", "of", "the", "RuleGenerateSet", ".", "Then", ",", "the", "output", "rules", "are", "applied", "on", "the", "TrainTestSet", "to", "extract", "contextually", "filtered", "features", "for", "machine", "learning", "training", ".", "The", "rule", "selection", "and", "the", "contextually", "filtered", "features", "are", "the", "key", "contributions", "of", "this", "paper", "."]}, "caption": {"raw": "Fig. 2. The detailed pipeline of rule mining, pairing, selecting and models training. The dashed frame highlights the novel procedures in the pipeline.", "tokens": ["Fig", ".", "2", ".", "The", "detailed", "pipeline", "of", "rule", "mining", ",", "pairing", ",", "selecting", "and", "models", "training", ".", "The", "dashed", "frame", "highlights", "the", "novel", "procedures", "in", "the", "pipeline", "."]}, "context": {"raw": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students Fig. 2. The detailed pipeline of rule mining, pairing, selecting and models training. The dashed frame highlights the novel procedures in the pipeline.", "tokens": ["Leveraging", "Routine", "Behavior", "and", "Contextually-Filtered", "Features", "for", "Depression", "Detection", "among", "College", "Students", "Fig", ".", "2", ".", "The", "detailed", "pipeline", "of", "rule", "mining", ",", "pairing", ",", "selecting", "and", "models", "training", ".", "The", "dashed", "frame", "highlights", "the", "novel", "procedures", "in", "the", "pipeline", "."]}, "filename": "fbd915aa1143821e150396f6e9ac20f2134c400d_Image_020.gif", "orig_filename": "fbd915aa1143821e150396f6e9ac20f2134c400d", "split": "val"}, {"article_id": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics", "description": {"raw": "Figure 7a. Time on task Boxplot with task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).   Figure 7b. Excess motion: Total trial vs last 1 second while locking on target. I/O Braid had more excess motion compared to Buttons and Scroll. The boxplots show median values with quartiles and min/max extent.   Figure 7c. Weighted average subjective feedback.  We mapped the 7-point Likert scale to a score in the range [-3, 3] for Ease of Use, Perceived Accuracy and Tactile Feel. We multiplied the score by the number of times the technique received that rating and computed an average for all the scores. The chart show favorable scores for I/O Braid and Scroll, whereas Buttons was the least popular.", "tokens": ["Figure", "7a", ".", "Time", "on", "task", "Boxplot", "with", "task", "completion", "times", "for", "the", "three", "input", "devices", ".", "I/O", "Braid", "was", "faster", "than", "Buttons", "(", "statistical", "significance", ")", ".", "Figure", "7b", ".", "Excess", "motion", ":", "Total", "trial", "vs", "last", "1", "second", "while", "locking", "on", "target", ".", "I/O", "Braid", "had", "more", "excess", "motion", "compared", "to", "Buttons", "and", "Scroll", ".", "The", "boxplots", "show", "median", "values", "with", "quartiles", "and", "min/max", "extent", ".", "Figure", "7c", ".", "Weighted", "average", "subjective", "feedback", ".", "We", "mapped", "the", "7-point", "Likert", "scale", "to", "a", "score", "in", "the", "range", "[", "-3", ",", "3", "]", "for", "Ease", "of", "Use", ",", "Perceived", "Accuracy", "and", "Tactile", "Feel", ".", "We", "multiplied", "the", "score", "by", "the", "number", "of", "times", "the", "technique", "received", "that", "rating", "and", "computed", "an", "average", "for", "all", "the", "scores", ".", "The", "chart", "show", "favorable", "scores", "for", "I/O", "Braid", "and", "Scroll", ",", "whereas", "Buttons", "was", "the", "least", "popular", "."]}, "caption": {"raw": "Figure 7. a) Task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).", "tokens": ["Figure", "7.", "a", ")", "Task", "completion", "times", "for", "the", "three", "input", "devices", ".", "I/O", "Braid", "was", "faster", "than", "Buttons", "(", "statistical", "significance", ")", "."]}, "context": {"raw": "E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics Figure 7. a) Task completion times for the three input devices. I/O Braid was faster than Buttons (statistical significance).", "tokens": ["E-Textile", "Microinteractions", ":", "Augmenting", "Twist", "with", "Flick", ",", "Slide", "and", "Grasp", "Gestures", "for", "Soft", "Electronics", "Figure", "7.", "a", ")", "Task", "completion", "times", "for", "the", "three", "input", "devices", ".", "I/O", "Braid", "was", "faster", "than", "Buttons", "(", "statistical", "significance", ")", "."]}, "filename": "f06a381ef1ff7d02a6a1876548e83db20e71442f_Image_010.jpg", "orig_filename": "f06a381ef1ff7d02a6a1876548e83db20e71442f", "split": "val"}, {"article_id": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "description": {"raw": "This figure presents a collum chart with the average WPM for each method. QWERTY ist the fastest followed by MultiTap, NavTouch and BrailleTouch.", "tokens": ["This", "figure", "presents", "a", "collum", "chart", "with", "the", "average", "WPM", "for", "each", "method", ".", "QWERTY", "ist", "the", "fastest", "followed", "by", "MultiTap", ",", "NavTouch", "and", "BrailleTouch", "."]}, "caption": {"raw": "Figure 4. WPM (average) across the different methods. Error bars denote 95% CI.", "tokens": ["Figure", "4", ".", "WPM", "(", "average", ")", "across", "the", "different", "methods", ".", "Error", "bars", "denote", "95", "%", "CI", "."]}, "context": {"raw": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors Figure 4. WPM (average) across the different methods. Error bars denote 95% CI.", "tokens": ["Blind", "people", "and", "mobile", "touch-based", "text-entry", ":", "acknowledging", "the", "need", "for", "different", "flavors", "Figure", "4", ".", "WPM", "(", "average", ")", "across", "the", "different", "methods", ".", "Error", "bars", "denote", "95", "%", "CI", "."]}, "filename": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_005.jpg", "orig_filename": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9", "split": "val"}, {"article_id": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects", "description": {"raw": "Time error graph of the moving target selection task. Most participants produced shorter time errors for faster target speeds while P2 produced a constant error of approximately 200~ms regardless of the target speed.", "tokens": ["Time", "error", "graph", "of", "the", "moving", "target", "selection", "task", ".", "Most", "participants", "produced", "shorter", "time", "errors", "for", "faster", "target", "speeds", "while", "P2", "produced", "a", "constant", "error", "of", "approximately", "200~ms", "regardless", "of", "the", "target", "speed", "."]}, "caption": {"raw": "Figure 8: Results of the moving target selection task (error bars represent standard error).", "tokens": ["Figure", "8", ":", "Results", "of", "the", "moving", "target", "selection", "task", "(", "error", "bars", "represent", "standard", "error", ")", "."]}, "context": {"raw": "ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects Figure 8: Results of the moving target selection task (error bars represent standard error).", "tokens": ["ThroughHand", ":", "2D", "Tactile", "Interaction", "to", "Simultaneously", "Recognize", "and", "Touch", "Multiple", "Objects", "Figure", "8", ":", "Results", "of", "the", "moving", "target", "selection", "task", "(", "error", "bars", "represent", "standard", "error", ")", "."]}, "filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3_Image_015.png", "orig_filename": "3b7a594a4bde4a45ded0293c0b2699ccb41e4dc3", "split": "val"}, {"article_id": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation", "description": {"raw": "The left bar chart is showing reasons participants reported for why delete indicators might stop them from using the delete function. The chart is detailed in section 4.3 and shows most reported reason was to avoid negative assumptions, whilst fewest reported the reason was to avoid showing vulnerability.  The right bar chart is showing reasons participants reported for why delete indicators would not stop them from using the delete function. The chart is detailed in section 4.3.2 and shows most reported reason was the cost of it being read.", "tokens": ["The", "left", "bar", "chart", "is", "showing", "reasons", "participants", "reported", "for", "why", "delete", "indicators", "might", "stop", "them", "from", "using", "the", "delete", "function", ".", "The", "chart", "is", "detailed", "in", "section", "4.3", "and", "shows", "most", "reported", "reason", "was", "to", "avoid", "negative", "assumptions", ",", "whilst", "fewest", "reported", "the", "reason", "was", "to", "avoid", "showing", "vulnerability", ".", "The", "right", "bar", "chart", "is", "showing", "reasons", "participants", "reported", "for", "why", "delete", "indicators", "would", "not", "stop", "them", "from", "using", "the", "delete", "function", ".", "The", "chart", "is", "detailed", "in", "section", "4.3.2", "and", "shows", "most", "reported", "reason", "was", "the", "cost", "of", "it", "being", "read", "."]}, "caption": {"raw": "Figure 6: Reasons participants reported for why delete in- dicators would not stop them from using the deletion func- tion.", "tokens": ["Figure", "6", ":", "Reasons", "participants", "reported", "for", "why", "delete", "in-", "dicators", "would", "not", "stop", "them", "from", "using", "the", "deletion", "func-", "tion", "."]}, "context": {"raw": "“Oops...”: Mobile Message Deletion in Conversation Error and Regret Remediation Figure 6: Reasons participants reported for why delete in- dicators would not stop them from using the deletion func- tion.", "tokens": ["“", "Oops", "...", "”", ":", "Mobile", "Message", "Deletion", "in", "Conversation", "Error", "and", "Regret", "Remediation", "Figure", "6", ":", "Reasons", "participants", "reported", "for", "why", "delete", "in-", "dicators", "would", "not", "stop", "them", "from", "using", "the", "deletion", "func-", "tion", "."]}, "filename": "fa7c681ffd0ea94a482755623a107fce48740b0d_Image_007.png", "orig_filename": "fa7c681ffd0ea94a482755623a107fce48740b0d", "split": "val"}, {"article_id": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together", "description": {"raw": "Figure 8. Heatmap of sleep data captured. Dark green represents days children interacted, light green represents days children did not interact, dark red represents days parents interacted, light orange represents days parents did not interact.", "tokens": ["Figure", "8", ".", "Heatmap", "of", "sleep", "data", "captured", ".", "Dark", "green", "represents", "days", "children", "interacted", ",", "light", "green", "represents", "days", "children", "did", "not", "interact", ",", "dark", "red", "represents", "days", "parents", "interacted", ",", "light", "orange", "represents", "days", "parents", "did", "not", "interact", "."]}, "caption": {"raw": "Figure 8. Heatmap of sleep data captured. Dark green represents days when children interacted, light green represents days children did not interact, dark red represents days parents interacted, and light red represents days parents did not interact.", "tokens": ["Figure", "8", ".", "Heatmap", "of", "sleep", "data", "captured", ".", "Dark", "green", "represents", "days", "when", "children", "interacted", ",", "light", "green", "represents", "days", "children", "did", "not", "interact", ",", "dark", "red", "represents", "days", "parents", "interacted", ",", "and", "light", "red", "represents", "days", "parents", "did", "not", "interact", "."]}, "context": {"raw": "DreamCatcher: Exploring How Parents and School-Age Children can Track and Review Sleep Information Together Figure 8. Heatmap of sleep data captured. Dark green represents days when children interacted, light green represents days children did not interact, dark red represents days parents interacted, and light red represents days parents did not interact.", "tokens": ["DreamCatcher", ":", "Exploring", "How", "Parents", "and", "School-Age", "Children", "can", "Track", "and", "Review", "Sleep", "Information", "Together", "Figure", "8", ".", "Heatmap", "of", "sleep", "data", "captured", ".", "Dark", "green", "represents", "days", "when", "children", "interacted", ",", "light", "green", "represents", "days", "children", "did", "not", "interact", ",", "dark", "red", "represents", "days", "parents", "interacted", ",", "and", "light", "red", "represents", "days", "parents", "did", "not", "interact", "."]}, "filename": "373b3ed1b4ea52cc733b8296301221e3fb00ef42_Image_017.jpg", "orig_filename": "373b3ed1b4ea52cc733b8296301221e3fb00ef42", "split": "val"}, {"article_id": "The Mimesis Effect: The Effect of Roles on Player Choice in Interactive Narrative Role-Playing Games", "description": {"raw": "A graph showing the relationship between the roles Figher, Mage, and Rogue and their attributes. Fighters have the main attribute \"Strong\" and secondary attributes \"Non-stealthy/open\" and \"Non-magical\". Mages have primary attribute \"Magical\" and secondary attributes \"Frail\" and \"Non-stealthy\". Rogues have primary attribute \"Stealthy\" and secondary attributes \"Frail\" and \"Non-magical\".", "tokens": ["A", "graph", "showing", "the", "relationship", "between", "the", "roles", "Figher", ",", "Mage", ",", "and", "Rogue", "and", "their", "attributes", ".", "Fighters", "have", "the", "main", "attribute", "``", "Strong", "''", "and", "secondary", "attributes", "``", "Non-stealthy/open", "''", "and", "``", "Non-magical", "''", ".", "Mages", "have", "primary", "attribute", "``", "Magical", "''", "and", "secondary", "attributes", "``", "Frail", "''", "and", "``", "Non-stealthy", "''", ".", "Rogues", "have", "primary", "attribute", "``", "Stealthy", "''", "and", "secondary", "attributes", "``", "Frail", "''", "and", "``", "Non-magical", "''", "."]}, "caption": {"raw": "Figure 2. Our triad of role-attribute mappings. We selected three attributes and identiﬁed three corresponding roles we felt best represented the attributes. Nodes represent role-attribute mappings, and edges are attributes shared between the connected role-attribute mappings. The edge opposite a node is the antonymic attribute to the node’s role-attribute.", "tokens": ["Figure", "2", ".", "Our", "triad", "of", "role-attribute", "mappings", ".", "We", "selected", "three", "attributes", "and", "identiﬁed", "three", "corresponding", "roles", "we", "felt", "best", "represented", "the", "attributes", ".", "Nodes", "represent", "role-attribute", "mappings", ",", "and", "edges", "are", "attributes", "shared", "between", "the", "connected", "role-attribute", "mappings", ".", "The", "edge", "opposite", "a", "node", "is", "the", "antonymic", "attribute", "to", "the", "node", "’", "s", "role-attribute", "."]}, "context": {"raw": "The Mimesis Effect: The Effect of Roles on Player Choice in Interactive Narrative Role-Playing Games Figure 2. Our triad of role-attribute mappings. We selected three attributes and identiﬁed three corresponding roles we felt best represented the attributes. Nodes represent role-attribute mappings, and edges are attributes shared between the connected role-attribute mappings. The edge opposite a node is the antonymic attribute to the node’s role-attribute.", "tokens": ["The", "Mimesis", "Effect", ":", "The", "Effect", "of", "Roles", "on", "Player", "Choice", "in", "Interactive", "Narrative", "Role-Playing", "Games", "Figure", "2", ".", "Our", "triad", "of", "role-attribute", "mappings", ".", "We", "selected", "three", "attributes", "and", "identiﬁed", "three", "corresponding", "roles", "we", "felt", "best", "represented", "the", "attributes", ".", "Nodes", "represent", "role-attribute", "mappings", ",", "and", "edges", "are", "attributes", "shared", "between", "the", "connected", "role-attribute", "mappings", ".", "The", "edge", "opposite", "a", "node", "is", "the", "antonymic", "attribute", "to", "the", "node", "’", "s", "role-attribute", "."]}, "filename": "747015237c3a1d786d88d2a6f03f1297354f2077_Image_004.png", "orig_filename": "747015237c3a1d786d88d2a6f03f1297354f2077", "split": "val"}, {"article_id": "TouchCam: Realtime Recognition of Location-Specific On-Body Gestures to Support Users with Visual Impairments", "description": {"raw": "Study 1 data collection. Left: a participant follows on-screen instructions to complete our data collection protocol while wearing the prototype. Right: Example skin images collected during study 1 across the 15 fine-grained classes.", "tokens": ["Study", "1", "data", "collection", ".", "Left", ":", "a", "participant", "follows", "on-screen", "instructions", "to", "complete", "our", "data", "collection", "protocol", "while", "wearing", "the", "prototype", ".", "Right", ":", "Example", "skin", "images", "collected", "during", "study", "1", "across", "the", "15", "fine-grained", "classes", "."]}, "caption": {"raw": "Participant following on-screen data collection protocol                                       (b) Example skin images from Study IFig. 3. (a) Data collection setup showing our prototype, location and gesture instructions, and camera video feed. (b) Example skin-surface images recorded by  our finger-mounted camera (fingerprint images  omitted to  protect  our participants’ privacy).XX:10 • L. Stearns et al.Procedure. The procedure lasted up to 90 minutes. After a brief demographic questionnaire and setup period (i.e., selecting rings, putting on the prototype), participants completed the following tasks, in order:Location-specific touches. Participants touched and held their finger in place at 15 locations (Figure 2b) with each location prompted visually on a monitor (Figure 3a). After confirming the location and image quality, the experimenter logged the current location (e.g., timestamp, location label) and triggered the start of the next trial. Participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 15 locations (150 trials in total). In total, this dataset includes 3600 location-specific touches across all participants. Example images are shown in Figure 3b.Location-specific gestures. Participants performed the eight basic gestures: tap, swipe up, swipe down, swipe left, swipe right, circle, triangle, and square (Figure 2c) at three body locations: the palm, wrist, and thigh. These locations were selected from the 15 locations in the first task because they are easy to access, unobtrusive, and have a relatively large input area thus allowing for more complex gestures. As with the first task, participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 24 gesture and location combinations (240 trials in total). This dataset includes 5,760 location-specific gestures across all participants.", "tokens": ["Participant", "following", "on-screen", "data", "collection", "protocol", "(", "b", ")", "Example", "skin", "images", "from", "Study", "IFig", ".", "3", ".", "(", "a", ")", "Data", "collection", "setup", "showing", "our", "prototype", ",", "location", "and", "gesture", "instructions", ",", "and", "camera", "video", "feed", ".", "(", "b", ")", "Example", "skin-surface", "images", "recorded", "by", "our", "finger-mounted", "camera", "(", "fingerprint", "images", "omitted", "to", "protect", "our", "participants", "’", "privacy", ")", ".XX:10", "•", "L.", "Stearns", "et", "al.Procedure", ".", "The", "procedure", "lasted", "up", "to", "90", "minutes", ".", "After", "a", "brief", "demographic", "questionnaire", "and", "setup", "period", "(", "i.e.", ",", "selecting", "rings", ",", "putting", "on", "the", "prototype", ")", ",", "participants", "completed", "the", "following", "tasks", ",", "in", "order", ":", "Location-specific", "touches", ".", "Participants", "touched", "and", "held", "their", "finger", "in", "place", "at", "15", "locations", "(", "Figure", "2b", ")", "with", "each", "location", "prompted", "visually", "on", "a", "monitor", "(", "Figure", "3a", ")", ".", "After", "confirming", "the", "location", "and", "image", "quality", ",", "the", "experimenter", "logged", "the", "current", "location", "(", "e.g.", ",", "timestamp", ",", "location", "label", ")", "and", "triggered", "the", "start", "of", "the", "next", "trial", ".", "Participants", "completed", "10", "blocks", "of", "trials", ",", "where", "each", "block", "consisted", "of", "a", "different", "random", "permutation", "of", "the", "15", "locations", "(", "150", "trials", "in", "total", ")", ".", "In", "total", ",", "this", "dataset", "includes", "3600", "location-specific", "touches", "across", "all", "participants", ".", "Example", "images", "are", "shown", "in", "Figure", "3b.Location-specific", "gestures", ".", "Participants", "performed", "the", "eight", "basic", "gestures", ":", "tap", ",", "swipe", "up", ",", "swipe", "down", ",", "swipe", "left", ",", "swipe", "right", ",", "circle", ",", "triangle", ",", "and", "square", "(", "Figure", "2c", ")", "at", "three", "body", "locations", ":", "the", "palm", ",", "wrist", ",", "and", "thigh", ".", "These", "locations", "were", "selected", "from", "the", "15", "locations", "in", "the", "first", "task", "because", "they", "are", "easy", "to", "access", ",", "unobtrusive", ",", "and", "have", "a", "relatively", "large", "input", "area", "thus", "allowing", "for", "more", "complex", "gestures", ".", "As", "with", "the", "first", "task", ",", "participants", "completed", "10", "blocks", "of", "trials", ",", "where", "each", "block", "consisted", "of", "a", "different", "random", "permutation", "of", "the", "24", "gesture", "and", "location", "combinations", "(", "240", "trials", "in", "total", ")", ".", "This", "dataset", "includes", "5,760", "location-specific", "gestures", "across", "all", "participants", "."]}, "context": {"raw": "TouchCam: Realtime Recognition of Location-Specific On-Body Gestures to Support Users with Visual Impairments Participant following on-screen data collection protocol                                       (b) Example skin images from Study IFig. 3. (a) Data collection setup showing our prototype, location and gesture instructions, and camera video feed. (b) Example skin-surface images recorded by  our finger-mounted camera (fingerprint images  omitted to  protect  our participants’ privacy).XX:10 • L. Stearns et al.Procedure. The procedure lasted up to 90 minutes. After a brief demographic questionnaire and setup period (i.e., selecting rings, putting on the prototype), participants completed the following tasks, in order:Location-specific touches. Participants touched and held their finger in place at 15 locations (Figure 2b) with each location prompted visually on a monitor (Figure 3a). After confirming the location and image quality, the experimenter logged the current location (e.g., timestamp, location label) and triggered the start of the next trial. Participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 15 locations (150 trials in total). In total, this dataset includes 3600 location-specific touches across all participants. Example images are shown in Figure 3b.Location-specific gestures. Participants performed the eight basic gestures: tap, swipe up, swipe down, swipe left, swipe right, circle, triangle, and square (Figure 2c) at three body locations: the palm, wrist, and thigh. These locations were selected from the 15 locations in the first task because they are easy to access, unobtrusive, and have a relatively large input area thus allowing for more complex gestures. As with the first task, participants completed 10 blocks of trials, where each block consisted of a different random permutation of the 24 gesture and location combinations (240 trials in total). This dataset includes 5,760 location-specific gestures across all participants.", "tokens": ["TouchCam", ":", "Realtime", "Recognition", "of", "Location-Specific", "On-Body", "Gestures", "to", "Support", "Users", "with", "Visual", "Impairments", "Participant", "following", "on-screen", "data", "collection", "protocol", "(", "b", ")", "Example", "skin", "images", "from", "Study", "IFig", ".", "3", ".", "(", "a", ")", "Data", "collection", "setup", "showing", "our", "prototype", ",", "location", "and", "gesture", "instructions", ",", "and", "camera", "video", "feed", ".", "(", "b", ")", "Example", "skin-surface", "images", "recorded", "by", "our", "finger-mounted", "camera", "(", "fingerprint", "images", "omitted", "to", "protect", "our", "participants", "’", "privacy", ")", ".XX:10", "•", "L.", "Stearns", "et", "al.Procedure", ".", "The", "procedure", "lasted", "up", "to", "90", "minutes", ".", "After", "a", "brief", "demographic", "questionnaire", "and", "setup", "period", "(", "i.e.", ",", "selecting", "rings", ",", "putting", "on", "the", "prototype", ")", ",", "participants", "completed", "the", "following", "tasks", ",", "in", "order", ":", "Location-specific", "touches", ".", "Participants", "touched", "and", "held", "their", "finger", "in", "place", "at", "15", "locations", "(", "Figure", "2b", ")", "with", "each", "location", "prompted", "visually", "on", "a", "monitor", "(", "Figure", "3a", ")", ".", "After", "confirming", "the", "location", "and", "image", "quality", ",", "the", "experimenter", "logged", "the", "current", "location", "(", "e.g.", ",", "timestamp", ",", "location", "label", ")", "and", "triggered", "the", "start", "of", "the", "next", "trial", ".", "Participants", "completed", "10", "blocks", "of", "trials", ",", "where", "each", "block", "consisted", "of", "a", "different", "random", "permutation", "of", "the", "15", "locations", "(", "150", "trials", "in", "total", ")", ".", "In", "total", ",", "this", "dataset", "includes", "3600", "location-specific", "touches", "across", "all", "participants", ".", "Example", "images", "are", "shown", "in", "Figure", "3b.Location-specific", "gestures", ".", "Participants", "performed", "the", "eight", "basic", "gestures", ":", "tap", ",", "swipe", "up", ",", "swipe", "down", ",", "swipe", "left", ",", "swipe", "right", ",", "circle", ",", "triangle", ",", "and", "square", "(", "Figure", "2c", ")", "at", "three", "body", "locations", ":", "the", "palm", ",", "wrist", ",", "and", "thigh", ".", "These", "locations", "were", "selected", "from", "the", "15", "locations", "in", "the", "first", "task", "because", "they", "are", "easy", "to", "access", ",", "unobtrusive", ",", "and", "have", "a", "relatively", "large", "input", "area", "thus", "allowing", "for", "more", "complex", "gestures", ".", "As", "with", "the", "first", "task", ",", "participants", "completed", "10", "blocks", "of", "trials", ",", "where", "each", "block", "consisted", "of", "a", "different", "random", "permutation", "of", "the", "24", "gesture", "and", "location", "combinations", "(", "240", "trials", "in", "total", ")", ".", "This", "dataset", "includes", "5,760", "location-specific", "gestures", "across", "all", "participants", "."]}, "filename": "a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c_Image_012.jpg", "orig_filename": "a03fc1faa8d49a0de40d44d53bf551bfcbb93c5c", "split": "val"}, {"article_id": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults", "description": {"raw": "Description: image of a chart that has automatic adaptation and notification selected", "tokens": ["Description", ":", "image", "of", "a", "chart", "that", "has", "automatic", "adaptation", "and", "notification", "selected"]}, "caption": {"raw": "Participants: YA4, YA7, YA8, YA11, YA28, YA30, OA6, OA8", "tokens": ["Participants", ":", "YA4", ",", "YA7", ",", "YA8", ",", "YA11", ",", "YA28", ",", "YA30", ",", "OA6", ",", "OA8"]}, "context": {"raw": "Understanding design considerations for adaptive user interfaces for accessible pointing with older and younger adults Participants: YA4, YA7, YA8, YA11, YA28, YA30, OA6, OA8", "tokens": ["Understanding", "design", "considerations", "for", "adaptive", "user", "interfaces", "for", "accessible", "pointing", "with", "older", "and", "younger", "adults", "Participants", ":", "YA4", ",", "YA7", ",", "YA8", ",", "YA11", ",", "YA28", ",", "YA30", ",", "OA6", ",", "OA8"]}, "filename": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9_Image_011.png", "orig_filename": "3f0f4f61812103e1ecdbd1a2ec01b4d923c147c9", "split": "val"}, {"article_id": "A General Methodology to Quantify Biases in Natural Language Data", "description": {"raw": "Warning!  Tagging errors. Text on Page 3 of this paper is not tagged. Figure 1: Workflow of the proposed general methodology to quantify and mitigate biases in natural language data.", "tokens": ["Warning", "!", "Tagging", "errors", ".", "Text", "on", "Page", "3", "of", "this", "paper", "is", "not", "tagged", ".", "Figure", "1", ":", "Workflow", "of", "the", "proposed", "general", "methodology", "to", "quantify", "and", "mitigate", "biases", "in", "natural", "language", "data", "."]}, "caption": {"raw": "Figure 1: We propose a general methodology to quantify and mitigate biases in natural language data. 1) Feature extraction: extract observational space and latent space features to capture explicit and implicit biases. 2) Quantify distribution discrepancy: based on extracted features, use Maximum Mean Discrepancy to measure the difference between the potentially biased data and the reference data. 3) Mitigate Biases: if biases are detected, reduce the level of biases leveraging conﬁdence ranking or Deep Generative Models.", "tokens": ["Figure", "1", ":", "We", "propose", "a", "general", "methodology", "to", "quantify", "and", "mitigate", "biases", "in", "natural", "language", "data", ".", "1", ")", "Feature", "extraction", ":", "extract", "observational", "space", "and", "latent", "space", "features", "to", "capture", "explicit", "and", "implicit", "biases", ".", "2", ")", "Quantify", "distribution", "discrepancy", ":", "based", "on", "extracted", "features", ",", "use", "Maximum", "Mean", "Discrepancy", "to", "measure", "the", "difference", "between", "the", "potentially", "biased", "data", "and", "the", "reference", "data", ".", "3", ")", "Mitigate", "Biases", ":", "if", "biases", "are", "detected", ",", "reduce", "the", "level", "of", "biases", "leveraging", "conﬁdence", "ranking", "or", "Deep", "Generative", "Models", "."]}, "context": {"raw": "A General Methodology to Quantify Biases in Natural Language Data Figure 1: We propose a general methodology to quantify and mitigate biases in natural language data. 1) Feature extraction: extract observational space and latent space features to capture explicit and implicit biases. 2) Quantify distribution discrepancy: based on extracted features, use Maximum Mean Discrepancy to measure the difference between the potentially biased data and the reference data. 3) Mitigate Biases: if biases are detected, reduce the level of biases leveraging conﬁdence ranking or Deep Generative Models.", "tokens": ["A", "General", "Methodology", "to", "Quantify", "Biases", "in", "Natural", "Language", "Data", "Figure", "1", ":", "We", "propose", "a", "general", "methodology", "to", "quantify", "and", "mitigate", "biases", "in", "natural", "language", "data", ".", "1", ")", "Feature", "extraction", ":", "extract", "observational", "space", "and", "latent", "space", "features", "to", "capture", "explicit", "and", "implicit", "biases", ".", "2", ")", "Quantify", "distribution", "discrepancy", ":", "based", "on", "extracted", "features", ",", "use", "Maximum", "Mean", "Discrepancy", "to", "measure", "the", "difference", "between", "the", "potentially", "biased", "data", "and", "the", "reference", "data", ".", "3", ")", "Mitigate", "Biases", ":", "if", "biases", "are", "detected", ",", "reduce", "the", "level", "of", "biases", "leveraging", "conﬁdence", "ranking", "or", "Deep", "Generative", "Models", "."]}, "filename": "ce4b730f4eaddd98be36ce4ba95548a95a6d69fb_Image_003.jpg", "orig_filename": "ce4b730f4eaddd98be36ce4ba95548a95a6d69fb", "split": "val"}, {"article_id": "Identifying Speech Input Errors Through Audio-Only Interaction", "description": {"raw": "There is a progress bar at the top of the screen which shows the current step in a study. Below the progress bar, there are set number, the reference phrase participants should read, a speaker icon to play the audio clip, and boxes of words to mark the recognition errors with instructions for each.", "tokens": ["There", "is", "a", "progress", "bar", "at", "the", "top", "of", "the", "screen", "which", "shows", "the", "current", "step", "in", "a", "study", ".", "Below", "the", "progress", "bar", ",", "there", "are", "set", "number", ",", "the", "reference", "phrase", "participants", "should", "read", ",", "a", "speaker", "icon", "to", "play", "the", "audio", "clip", ",", "and", "boxes", "of", "words", "to", "mark", "the", "recognition", "errors", "with", "instructions", "for", "each", "."]}, "caption": {"raw": "Figure 3. Screenshot of the online testbed used for Studies 2, 3, and 4, showing a single trial. A trial consisted of reading a presented phrase, listening to an audio clip of what a speech recognition engine had heard, and marking errors in therecognized version (i.e., discrepancies between text and audio).", "tokens": ["Figure", "3", ".", "Screenshot", "of", "the", "online", "testbed", "used", "for", "Studies", "2", ",", "3", ",", "and", "4", ",", "showing", "a", "single", "trial", ".", "A", "trial", "consisted", "of", "reading", "a", "presented", "phrase", ",", "listening", "to", "an", "audio", "clip", "of", "what", "a", "speech", "recognition", "engine", "had", "heard", ",", "and", "marking", "errors", "in", "therecognized", "version", "(", "i.e.", ",", "discrepancies", "between", "text", "and", "audio", ")", "."]}, "context": {"raw": "Identifying Speech Input Errors Through Audio-Only Interaction Figure 3. Screenshot of the online testbed used for Studies 2, 3, and 4, showing a single trial. A trial consisted of reading a presented phrase, listening to an audio clip of what a speech recognition engine had heard, and marking errors in therecognized version (i.e., discrepancies between text and audio).", "tokens": ["Identifying", "Speech", "Input", "Errors", "Through", "Audio-Only", "Interaction", "Figure", "3", ".", "Screenshot", "of", "the", "online", "testbed", "used", "for", "Studies", "2", ",", "3", ",", "and", "4", ",", "showing", "a", "single", "trial", ".", "A", "trial", "consisted", "of", "reading", "a", "presented", "phrase", ",", "listening", "to", "an", "audio", "clip", "of", "what", "a", "speech", "recognition", "engine", "had", "heard", ",", "and", "marking", "errors", "in", "therecognized", "version", "(", "i.e.", ",", "discrepancies", "between", "text", "and", "audio", ")", "."]}, "filename": "343623ed81f67a3fd7b8183ad10a520817a0356b_Image_019.jpg", "orig_filename": "343623ed81f67a3fd7b8183ad10a520817a0356b", "split": "val"}, {"article_id": "Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites", "description": {"raw": "The graph shows that the F1 score hovers around 71% once at least 10 frames are selected from a 60 second video segment while it takes more frames to achieve a lower performance (65%) when analyzing 30 second video segments.", "tokens": ["The", "graph", "shows", "that", "the", "F1", "score", "hovers", "around", "71", "%", "once", "at", "least", "10", "frames", "are", "selected", "from", "a", "60", "second", "video", "segment", "while", "it", "takes", "more", "frames", "to", "achieve", "a", "lower", "performance", "(", "65", "%", ")", "when", "analyzing", "30", "second", "video", "segments", "."]}, "caption": {"raw": "Figure 5. SL vs. Non-SL F1 score for 30 and 60 second videos.", "tokens": ["Figure", "5", ".", "SL", "vs.", "Non-SL", "F1", "score", "for", "30", "and", "60", "second", "videos", "."]}, "context": {"raw": "Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites Figure 5. SL vs. Non-SL F1 score for 30 and 60 second videos.", "tokens": ["Speed-Accuracy", "Tradeoffs", "for", "Detecting", "Sign", "Language", "Content", "in", "Video", "Sharing", "Sites", "Figure", "5", ".", "SL", "vs.", "Non-SL", "F1", "score", "for", "30", "and", "60", "second", "videos", "."]}, "filename": "c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4_Image_008.jpg", "orig_filename": "c769c84174b7a6b2f1083aa5bcf5ef45e8e527f4", "split": "val"}, {"article_id": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "description": {"raw": "A bar diagram showing the factors navigation technique, gender and display size. The y axis ranges from 0 to 1000. The bar for the spatial navigation technique is about two thirds the size of the touch techiques . The bar of the male participants is about 4/5 the size of the female participants. The bar of the tablet is about the same hight of the male participants and similarly the bar of the phone is about the same hight as the bar of the female participants.", "tokens": ["A", "bar", "diagram", "showing", "the", "factors", "navigation", "technique", ",", "gender", "and", "display", "size", ".", "The", "y", "axis", "ranges", "from", "0", "to", "1000", ".", "The", "bar", "for", "the", "spatial", "navigation", "technique", "is", "about", "two", "thirds", "the", "size", "of", "the", "touch", "techiques", ".", "The", "bar", "of", "the", "male", "participants", "is", "about", "4/5", "the", "size", "of", "the", "female", "participants", ".", "The", "bar", "of", "the", "tablet", "is", "about", "the", "same", "hight", "of", "the", "male", "participants", "and", "similarly", "the", "bar", "of", "the", "phone", "is", "about", "the", "same", "hight", "as", "the", "bar", "of", "the", "female", "participants", "."]}, "caption": {"raw": "Figure 5: Total completion times broken down by navigation technique, gender, and display size. Error bars denote standard deviations (95% CI).", "tokens": ["Figure", "5", ":", "Total", "completion", "times", "broken", "down", "by", "navigation", "technique", ",", "gender", ",", "and", "display", "size", ".", "Error", "bars", "denote", "standard", "deviations", "(", "95", "%", "CI", ")", "."]}, "context": {"raw": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays Figure 5: Total completion times broken down by navigation technique, gender, and display size. Error bars denote standard deviations (95% CI).", "tokens": ["Pinch-drag-flick", "vs.", "spatial", "input", ":", "rethinking", "zoom", "&", "pan", "on", "mobile", "displays", "Figure", "5", ":", "Total", "completion", "times", "broken", "down", "by", "navigation", "technique", ",", "gender", ",", "and", "display", "size", ".", "Error", "bars", "denote", "standard", "deviations", "(", "95", "%", "CI", ")", "."]}, "filename": "f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_007.jpg", "orig_filename": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "split": "val"}, {"article_id": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers", "description": {"raw": "A box plot displaying the Median Intensity across each condition, with units in decibels (dB). First results are presented for the 9 participants who spoke in all three conditions, with median value of 48.882 for Markup, 41.828 for No ASR, and 50.541 for ASR.  There were significant differences between Markup and No ASR, as well as ASR and No ASR.  Next, results are shown for all 12 participants, with median value of 49.253 for Markup and 49.578 for ASR.  There was no significant difference between these two conditions.", "tokens": ["A", "box", "plot", "displaying", "the", "Median", "Intensity", "across", "each", "condition", ",", "with", "units", "in", "decibels", "(", "dB", ")", ".", "First", "results", "are", "presented", "for", "the", "9", "participants", "who", "spoke", "in", "all", "three", "conditions", ",", "with", "median", "value", "of", "48.882", "for", "Markup", ",", "41.828", "for", "No", "ASR", ",", "and", "50.541", "for", "ASR", ".", "There", "were", "significant", "differences", "between", "Markup", "and", "No", "ASR", ",", "as", "well", "as", "ASR", "and", "No", "ASR", ".", "Next", ",", "results", "are", "shown", "for", "all", "12", "participants", ",", "with", "median", "value", "of", "49.253", "for", "Markup", "and", "49.578", "for", "ASR", ".", "There", "was", "no", "significant", "difference", "between", "these", "two", "conditions", "."]}, "caption": {"raw": "Figure 4: Box plots displaying the median intensity for each hearing participant’s voice, across all three conditions", "tokens": ["Figure", "4", ":", "Box", "plots", "displaying", "the", "median", "intensity", "for", "each", "hearing", "participant", "’", "s", "voice", ",", "across", "all", "three", "conditions"]}, "context": {"raw": "Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers Figure 4: Box plots displaying the median intensity for each hearing participant’s voice, across all three conditions", "tokens": ["Behavioral", "Changes", "in", "Speakers", "who", "are", "Automatically", "Captioned", "in", "Meetings", "with", "Deaf", "or", "Hard-of-Hearing", "Peers", "Figure", "4", ":", "Box", "plots", "displaying", "the", "median", "intensity", "for", "each", "hearing", "participant", "’", "s", "voice", ",", "across", "all", "three", "conditions"]}, "filename": "4cecd70a9e46a761774a54ed11d613b33721b95d_Image_007.gif", "orig_filename": "4cecd70a9e46a761774a54ed11d613b33721b95d", "split": "val"}, {"article_id": "Pupil responses during discrete goal-directed movements", "description": {"raw": "Figure 10. Mean pupil diameter changes against different IDs; data are aligned over a 7-second window around tooltip-start.  The vertical dash black line is the tooltip-start and other three solid color vertical lines represent the tooltip-reach moments of three IDs respectively. The three colors of bars at the bottom indicate significant differences in pupil dilation between Easy, Middle and Hard ID with black representing Easy vs. Hard, pink representing Easy vs. Middle, and green representing Middle vs. Hard. The error bars for 1 std. dev. are drawn every 400ms.", "tokens": ["Figure", "10", ".", "Mean", "pupil", "diameter", "changes", "against", "different", "IDs", ";", "data", "are", "aligned", "over", "a", "7-second", "window", "around", "tooltip-start", ".", "The", "vertical", "dash", "black", "line", "is", "the", "tooltip-start", "and", "other", "three", "solid", "color", "vertical", "lines", "represent", "the", "tooltip-reach", "moments", "of", "three", "IDs", "respectively", ".", "The", "three", "colors", "of", "bars", "at", "the", "bottom", "indicate", "significant", "differences", "in", "pupil", "dilation", "between", "Easy", ",", "Middle", "and", "Hard", "ID", "with", "black", "representing", "Easy", "vs.", "Hard", ",", "pink", "representing", "Easy", "vs.", "Middle", ",", "and", "green", "representing", "Middle", "vs.", "Hard", ".", "The", "error", "bars", "for", "1", "std", ".", "dev", ".", "are", "drawn", "every", "400ms", "."]}, "caption": {"raw": "Figure 10. Mean pupil diameter changes against different IDs; data are aligned over a 7-second window around tooltip-start. The vertical dash black line is the tooltip-start and other three solid color vertical lines represent the tooltip-reach moments of three IDs respectively. The three colors of bars at the bottom indicate significant differences in pupil dilation between Easy, Middle and Hard ID with black representing Easy vs. Hard, pink representing Easy vs. Middle, and green representing Middle vs. Hard. The error bars for 1 std. dev. are drawn every 400ms.", "tokens": ["Figure", "10", ".", "Mean", "pupil", "diameter", "changes", "against", "different", "IDs", ";", "data", "are", "aligned", "over", "a", "7-second", "window", "around", "tooltip-start", ".", "The", "vertical", "dash", "black", "line", "is", "the", "tooltip-start", "and", "other", "three", "solid", "color", "vertical", "lines", "represent", "the", "tooltip-reach", "moments", "of", "three", "IDs", "respectively", ".", "The", "three", "colors", "of", "bars", "at", "the", "bottom", "indicate", "significant", "differences", "in", "pupil", "dilation", "between", "Easy", ",", "Middle", "and", "Hard", "ID", "with", "black", "representing", "Easy", "vs.", "Hard", ",", "pink", "representing", "Easy", "vs.", "Middle", ",", "and", "green", "representing", "Middle", "vs.", "Hard", ".", "The", "error", "bars", "for", "1", "std", ".", "dev", ".", "are", "drawn", "every", "400ms", "."]}, "context": {"raw": "Pupil responses during discrete goal-directed movements Figure 10. Mean pupil diameter changes against different IDs; data are aligned over a 7-second window around tooltip-start. The vertical dash black line is the tooltip-start and other three solid color vertical lines represent the tooltip-reach moments of three IDs respectively. The three colors of bars at the bottom indicate significant differences in pupil dilation between Easy, Middle and Hard ID with black representing Easy vs. Hard, pink representing Easy vs. Middle, and green representing Middle vs. Hard. The error bars for 1 std. dev. are drawn every 400ms.", "tokens": ["Pupil", "responses", "during", "discrete", "goal-directed", "movements", "Figure", "10", ".", "Mean", "pupil", "diameter", "changes", "against", "different", "IDs", ";", "data", "are", "aligned", "over", "a", "7-second", "window", "around", "tooltip-start", ".", "The", "vertical", "dash", "black", "line", "is", "the", "tooltip-start", "and", "other", "three", "solid", "color", "vertical", "lines", "represent", "the", "tooltip-reach", "moments", "of", "three", "IDs", "respectively", ".", "The", "three", "colors", "of", "bars", "at", "the", "bottom", "indicate", "significant", "differences", "in", "pupil", "dilation", "between", "Easy", ",", "Middle", "and", "Hard", "ID", "with", "black", "representing", "Easy", "vs.", "Hard", ",", "pink", "representing", "Easy", "vs.", "Middle", ",", "and", "green", "representing", "Middle", "vs.", "Hard", ".", "The", "error", "bars", "for", "1", "std", ".", "dev", ".", "are", "drawn", "every", "400ms", "."]}, "filename": "1a91496ad6d8adf41f12b157343321732872c0c1_Image_013.gif", "orig_filename": "1a91496ad6d8adf41f12b157343321732872c0c1", "split": "val"}, {"article_id": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning", "description": {"raw": "An empty radar chart with 10 axis: Outcome, Collaboration, Organization process, Classroom dynamics, Confidence, Behaviour, Motivation, Language, Skillfullness, and Thinking skills.", "tokens": ["An", "empty", "radar", "chart", "with", "10", "axis", ":", "Outcome", ",", "Collaboration", ",", "Organization", "process", ",", "Classroom", "dynamics", ",", "Confidence", ",", "Behaviour", ",", "Motivation", ",", "Language", ",", "Skillfullness", ",", "and", "Thinking", "skills", "."]}, "caption": {"raw": "Figure 3. First version radar chart; superset of all possible axes as inspired by Activity Theory’s activity triangle.", "tokens": ["Figure", "3", ".", "First", "version", "radar", "chart", ";", "superset", "of", "all", "possible", "axes", "as", "inspired", "by", "Activity", "Theory", "’", "s", "activity", "triangle", "."]}, "context": {"raw": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning Figure 3. First version radar chart; superset of all possible axes as inspired by Activity Theory’s activity triangle.", "tokens": ["Group", "Spinner", ":", "Recognizing", "and", "Visualizing", "Learning", "in", "the", "Classroom", "for", "Reflection", ",", "Communication", ",", "and", "Planning", "Figure", "3", ".", "First", "version", "radar", "chart", ";", "superset", "of", "all", "possible", "axes", "as", "inspired", "by", "Activity", "Theory", "’", "s", "activity", "triangle", "."]}, "filename": "35941a4414b58e76d1f92b495c3c8d90a2593315_Image_004.gif", "orig_filename": "35941a4414b58e76d1f92b495c3c8d90a2593315", "split": "val"}, {"article_id": "Multiple view perspectives: improving inclusiveness and video compression in mainstream classroom recordings", "description": {"raw": "Figure 7 shows tabular chart of statistics for participant responses in study. It describes questions and responses. The questions are \"What is your rating for single/multiple view\", \"Did it help\", \"is it easy to use\", etc. It shows significant difference between SVP/MVP and C-MVP.", "tokens": ["Figure", "7", "shows", "tabular", "chart", "of", "statistics", "for", "participant", "responses", "in", "study", ".", "It", "describes", "questions", "and", "responses", ".", "The", "questions", "are", "``", "What", "is", "your", "rating", "for", "single/multiple", "view", "''", ",", "``", "Did", "it", "help", "''", ",", "``", "is", "it", "easy", "to", "use", "''", ",", "etc", ".", "It", "shows", "significant", "difference", "between", "SVP/MVP", "and", "C-MVP", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Multiple view perspectives: improving inclusiveness and video compression in mainstream classroom recordings ", "tokens": ["Multiple", "view", "perspectives", ":", "improving", "inclusiveness", "and", "video", "compression", "in", "mainstream", "classroom", "recordings"]}, "filename": "becd79d567f86115aa509b992dd1a09a765043cb_Image_009.jpg", "orig_filename": "becd79d567f86115aa509b992dd1a09a765043cb", "split": "val"}, {"article_id": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays", "description": {"raw": "Bar chart showing the diffrent compeltion time deppening on the visibility of the target. All numbers are in the text. In all cases the bars for visible tasks are much smaller than those for invisble.", "tokens": ["Bar", "chart", "showing", "the", "diffrent", "compeltion", "time", "deppening", "on", "the", "visibility", "of", "the", "target", ".", "All", "numbers", "are", "in", "the", "text", ".", "In", "all", "cases", "the", "bars", "for", "visible", "tasks", "are", "much", "smaller", "than", "those", "for", "invisble", "."]}, "caption": {"raw": "Figure 6: Average task completion times (gender-neutral).", "tokens": ["Figure", "6", ":", "Average", "task", "completion", "times", "(", "gender-neutral", ")", "."]}, "context": {"raw": "Pinch-drag-flick vs. spatial input: rethinking zoom & pan on mobile displays Figure 6: Average task completion times (gender-neutral).", "tokens": ["Pinch-drag-flick", "vs.", "spatial", "input", ":", "rethinking", "zoom", "&", "pan", "on", "mobile", "displays", "Figure", "6", ":", "Average", "task", "completion", "times", "(", "gender-neutral", ")", "."]}, "filename": "f11a212f60cef1b9319ab3a3502beb104415a6ef_Image_008.jpg", "orig_filename": "f11a212f60cef1b9319ab3a3502beb104415a6ef", "split": "val"}, {"article_id": "Demonstration of JetController: High-speed Ungrounded Force Feedback Controllers Using Air Propulsion Jets", "description": {"raw": "The bar charts show the maximum frequencies under different tubing configurations and different force magnitude.", "tokens": ["The", "bar", "charts", "show", "the", "maximum", "frequencies", "under", "different", "tubing", "configurations", "and", "different", "force", "magnitude", "."]}, "caption": {"raw": "Figure 3: Maximum impulse frequency at diferent force magnitudes for diferent tubing sizes and tubing lengths, cal- culated based on force rise time and fall time. (The 4.0N fre- quency is unavailable for 6mm x 250cm tubing because it could only achieve a maximum force of 3.3N.)", "tokens": ["Figure", "3", ":", "Maximum", "impulse", "frequency", "at", "diferent", "force", "magnitudes", "for", "diferent", "tubing", "sizes", "and", "tubing", "lengths", ",", "cal-", "culated", "based", "on", "force", "rise", "time", "and", "fall", "time", ".", "(", "The", "4.0N", "fre-", "quency", "is", "unavailable", "for", "6mm", "x", "250cm", "tubing", "because", "it", "could", "only", "achieve", "a", "maximum", "force", "of", "3.3N", ".", ")"]}, "context": {"raw": "Demonstration of JetController: High-speed Ungrounded Force Feedback Controllers Using Air Propulsion Jets Figure 3: Maximum impulse frequency at diferent force magnitudes for diferent tubing sizes and tubing lengths, cal- culated based on force rise time and fall time. (The 4.0N fre- quency is unavailable for 6mm x 250cm tubing because it could only achieve a maximum force of 3.3N.)", "tokens": ["Demonstration", "of", "JetController", ":", "High-speed", "Ungrounded", "Force", "Feedback", "Controllers", "Using", "Air", "Propulsion", "Jets", "Figure", "3", ":", "Maximum", "impulse", "frequency", "at", "diferent", "force", "magnitudes", "for", "diferent", "tubing", "sizes", "and", "tubing", "lengths", ",", "cal-", "culated", "based", "on", "force", "rise", "time", "and", "fall", "time", ".", "(", "The", "4.0N", "fre-", "quency", "is", "unavailable", "for", "6mm", "x", "250cm", "tubing", "because", "it", "could", "only", "achieve", "a", "maximum", "force", "of", "3.3N", ".", ")"]}, "filename": "89e2fdadc30740d5621ee09d5e2a05762e7df2d4_Image_004.png", "orig_filename": "89e2fdadc30740d5621ee09d5e2a05762e7df2d4", "split": "val"}, {"article_id": "Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills", "description": {"raw": "This figure shows a stacked column graph. The columns are grouped by activity (left to right): Lowercase Letters, Uppercase Letters, and Money Addition. Within each activity there is a column for each participant (who demonstrated any reaction while using this activity) which depicts the quantity of negative (black) and positive (gray) reactions he/she demonstrated while trying that activity.\n\nParticipants’ reactions during the Lowercase Letters activity: For P1, 12 out of 12 were positive. For P2, 75 out of 76 were positive. For P3, 1 out of 1 were positive. For P4, 18 out of 18 were positive. For P5, 20 out of 20 were positive. For P6, 4 out of 4 were positive. For P7, 1 out of 1 were negative. For P8, 18 out of 18 were positive. For P9, 30 out of 31 were positive. For P10, 7 out of 7 were positive.\n\nParticipants’ reactions during the Uppercase Letters activity: For P1, 16 out of 16 were positive. For P2, 96 out of 99 were positive. For P3, 1 out of 1 were positive. For P4, 8 out of 9 were positive. For P5, 35 out of 36 were positive. For P6, 24 out of 24 were positive. P7 did not demonstrate any reactions. For P8, 13 out of 13 were positive. For P9, 37 out of 37 were positive.\n\nParticipants’ reactions during the Money Addition activity: For P1, 12 out of 12 were positive. For P2, 67 out of 82 were positive. For P3, 39 out of 55 were positive. For P4, 14 out of 14 were negative. For P5, 9 out of 9 were positive. For P6, 24 out of 24 were positive. P7 did not demonstrate any reactions. For P8, 16 out of 16 were positive. For P9, 39 out of 43 were positive.", "tokens": ["This", "figure", "shows", "a", "stacked", "column", "graph", ".", "The", "columns", "are", "grouped", "by", "activity", "(", "left", "to", "right", ")", ":", "Lowercase", "Letters", ",", "Uppercase", "Letters", ",", "and", "Money", "Addition", ".", "Within", "each", "activity", "there", "is", "a", "column", "for", "each", "participant", "(", "who", "demonstrated", "any", "reaction", "while", "using", "this", "activity", ")", "which", "depicts", "the", "quantity", "of", "negative", "(", "black", ")", "and", "positive", "(", "gray", ")", "reactions", "he/she", "demonstrated", "while", "trying", "that", "activity", ".", "Participants", "’", "reactions", "during", "the", "Lowercase", "Letters", "activity", ":", "For", "P1", ",", "12", "out", "of", "12", "were", "positive", ".", "For", "P2", ",", "75", "out", "of", "76", "were", "positive", ".", "For", "P3", ",", "1", "out", "of", "1", "were", "positive", ".", "For", "P4", ",", "18", "out", "of", "18", "were", "positive", ".", "For", "P5", ",", "20", "out", "of", "20", "were", "positive", ".", "For", "P6", ",", "4", "out", "of", "4", "were", "positive", ".", "For", "P7", ",", "1", "out", "of", "1", "were", "negative", ".", "For", "P8", ",", "18", "out", "of", "18", "were", "positive", ".", "For", "P9", ",", "30", "out", "of", "31", "were", "positive", ".", "For", "P10", ",", "7", "out", "of", "7", "were", "positive", ".", "Participants", "’", "reactions", "during", "the", "Uppercase", "Letters", "activity", ":", "For", "P1", ",", "16", "out", "of", "16", "were", "positive", ".", "For", "P2", ",", "96", "out", "of", "99", "were", "positive", ".", "For", "P3", ",", "1", "out", "of", "1", "were", "positive", ".", "For", "P4", ",", "8", "out", "of", "9", "were", "positive", ".", "For", "P5", ",", "35", "out", "of", "36", "were", "positive", ".", "For", "P6", ",", "24", "out", "of", "24", "were", "positive", ".", "P7", "did", "not", "demonstrate", "any", "reactions", ".", "For", "P8", ",", "13", "out", "of", "13", "were", "positive", ".", "For", "P9", ",", "37", "out", "of", "37", "were", "positive", ".", "Participants", "’", "reactions", "during", "the", "Money", "Addition", "activity", ":", "For", "P1", ",", "12", "out", "of", "12", "were", "positive", ".", "For", "P2", ",", "67", "out", "of", "82", "were", "positive", ".", "For", "P3", ",", "39", "out", "of", "55", "were", "positive", ".", "For", "P4", ",", "14", "out", "of", "14", "were", "negative", ".", "For", "P5", ",", "9", "out", "of", "9", "were", "positive", ".", "For", "P6", ",", "24", "out", "of", "24", "were", "positive", ".", "P7", "did", "not", "demonstrate", "any", "reactions", ".", "For", "P8", ",", "16", "out", "of", "16", "were", "positive", ".", "For", "P9", ",", "39", "out", "of", "43", "were", "positive", "."]}, "caption": {"raw": "Figure 6. Positive and Negative Reactions with the LL, UL and MA activities.", "tokens": ["Figure", "6", ".", "Positive", "and", "Negative", "Reactions", "with", "the", "LL", ",", "UL", "and", "MA", "activities", "."]}, "context": {"raw": "Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills Figure 6. Positive and Negative Reactions with the LL, UL and MA activities.", "tokens": ["Online", "Learning", "System", "to", "Help", "People", "with", "Developmental", "Disabilities", "Reinforce", "Basic", "Skills", "Figure", "6", ".", "Positive", "and", "Negative", "Reactions", "with", "the", "LL", ",", "UL", "and", "MA", "activities", "."]}, "filename": "dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8_Image_018.png", "orig_filename": "dd652c6a297c57c8c0c9cd5d91f2cba213d2eba8", "split": "val"}, {"article_id": "Evaluating Smartwatch-based Sound Feedback for Deaf and Hard-of-hearing Users Across Contexts", "description": {"raw": "Three column charts are shown horizontally. The first chart is labelled \"Complexity of Haptic w/ Visual\" and has shows decreasing utility moving right from Simple, to Tacton, to none.    The second chart is labelled Sound characteristics and shows decreasing utility moving right, from Identity, to Direction, to Loudness.     The last chart, labelled filtering options, shows decreasing utility moving from Identity to Direction to Loudness.", "tokens": ["Three", "column", "charts", "are", "shown", "horizontally", ".", "The", "first", "chart", "is", "labelled", "``", "Complexity", "of", "Haptic", "w/", "Visual", "''", "and", "has", "shows", "decreasing", "utility", "moving", "right", "from", "Simple", ",", "to", "Tacton", ",", "to", "none", ".", "The", "second", "chart", "is", "labelled", "Sound", "characteristics", "and", "shows", "decreasing", "utility", "moving", "right", ",", "from", "Identity", ",", "to", "Direction", ",", "to", "Loudness", ".", "The", "last", "chart", ",", "labelled", "filtering", "options", ",", "shows", "decreasing", "utility", "moving", "from", "Identity", "to", "Direction", "to", "Loudness", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Evaluating Smartwatch-based Sound Feedback for Deaf and Hard-of-hearing Users Across Contexts ", "tokens": ["Evaluating", "Smartwatch-based", "Sound", "Feedback", "for", "Deaf", "and", "Hard-of-hearing", "Users", "Across", "Contexts"]}, "filename": "fdc69738f68745f1b11b269d30f9482fb428480b_Image_009.jpg", "orig_filename": "fdc69738f68745f1b11b269d30f9482fb428480b", "split": "val"}, {"article_id": "ASSETS: G: BrailleBlocks: Braille Toys for Cross-Ability Collaboration", "description": {"raw": "Two screens from the BrailleBlocks companion application. On the top is the selection chart from the Animal Name Game. The chart shows a cartoon dog, elephant, duck, and sheep. On the bottom is a level 1 of the Word Scramble Game. The screen shows instructions on how to play the game. It also shows a visual Braille representation of the letters \"t\", \"c\", and \"a\". Below the Braille are a list of words that the letters can unscramble to make.", "tokens": ["Two", "screens", "from", "the", "BrailleBlocks", "companion", "application", ".", "On", "the", "top", "is", "the", "selection", "chart", "from", "the", "Animal", "Name", "Game", ".", "The", "chart", "shows", "a", "cartoon", "dog", ",", "elephant", ",", "duck", ",", "and", "sheep", ".", "On", "the", "bottom", "is", "a", "level", "1", "of", "the", "Word", "Scramble", "Game", ".", "The", "screen", "shows", "instructions", "on", "how", "to", "play", "the", "game", ".", "It", "also", "shows", "a", "visual", "Braille", "representation", "of", "the", "letters", "``", "t", "''", ",", "``", "c", "''", ",", "and", "``", "a", "''", ".", "Below", "the", "Braille", "are", "a", "list", "of", "words", "that", "the", "letters", "can", "unscramble", "to", "make", "."]}, "caption": {"raw": "Figure 3 (top). The Animal Name Game selection page. Figure 4 (bottom). Level 1 of the Word Scramble game.", "tokens": ["Figure", "3", "(", "top", ")", ".", "The", "Animal", "Name", "Game", "selection", "page", ".", "Figure", "4", "(", "bottom", ")", ".", "Level", "1", "of", "the", "Word", "Scramble", "game", "."]}, "context": {"raw": "ASSETS: G: BrailleBlocks: Braille Toys for Cross-Ability Collaboration Figure 3 (top). The Animal Name Game selection page. Figure 4 (bottom). Level 1 of the Word Scramble game.", "tokens": ["ASSETS", ":", "G", ":", "BrailleBlocks", ":", "Braille", "Toys", "for", "Cross-Ability", "Collaboration", "Figure", "3", "(", "top", ")", ".", "The", "Animal", "Name", "Game", "selection", "page", ".", "Figure", "4", "(", "bottom", ")", ".", "Level", "1", "of", "the", "Word", "Scramble", "game", "."]}, "filename": "ac34c667f089ba776f496042d5f5a3aab8effa8c_Image_004.jpg", "orig_filename": "ac34c667f089ba776f496042d5f5a3aab8effa8c", "split": "val"}, {"article_id": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone", "description": {"raw": "Bar plot of participants agreement on the the 11 statements. The first four statements regarding the overall idea of learning handwriting on smartphone are: 1, \"It is meaningful to me to learn handwritten letters and digits on a smartphone.\"; 2, \"I am willing to learn handwritten letters and digits on a smartphone.\"; 3, \"Learning to write with my finger helps me learn to write with pens.\"; 4, \"I feel a sense of accomplishment by learning to write letters and digits.\". Statements 5 to 11 are about the experience with LighWrite: 5, \"LightWrite can successfully teach me how to write letters and digits.\"; 6, \"It is easy to learn the usage of LightWrite.\"; 7, \"It is easy to memorize the usage of LightWrite.\"; 8, \"I will be happy to download LightWrite and use it to learn to handwrite if it is launched in the app store.\"; 9, \"The touch-vibration feedback in basic stroke learning mode can help me better understand and write the basic strokes.\"; 10, \"Voice instructions in character learning mode can help me understand the shape of the characters.\"; 11, \"Voice feedback provided in the character learning module can let me know how to write better.\". All statements received an average score above 6.", "tokens": ["Bar", "plot", "of", "participants", "agreement", "on", "the", "the", "11", "statements", ".", "The", "first", "four", "statements", "regarding", "the", "overall", "idea", "of", "learning", "handwriting", "on", "smartphone", "are", ":", "1", ",", "``", "It", "is", "meaningful", "to", "me", "to", "learn", "handwritten", "letters", "and", "digits", "on", "a", "smartphone", ".", "``", ";", "2", ",", "``", "I", "am", "willing", "to", "learn", "handwritten", "letters", "and", "digits", "on", "a", "smartphone", ".", "``", ";", "3", ",", "``", "Learning", "to", "write", "with", "my", "finger", "helps", "me", "learn", "to", "write", "with", "pens", ".", "``", ";", "4", ",", "``", "I", "feel", "a", "sense", "of", "accomplishment", "by", "learning", "to", "write", "letters", "and", "digits.", "''", ".", "Statements", "5", "to", "11", "are", "about", "the", "experience", "with", "LighWrite", ":", "5", ",", "``", "LightWrite", "can", "successfully", "teach", "me", "how", "to", "write", "letters", "and", "digits", ".", "``", ";", "6", ",", "``", "It", "is", "easy", "to", "learn", "the", "usage", "of", "LightWrite", ".", "``", ";", "7", ",", "``", "It", "is", "easy", "to", "memorize", "the", "usage", "of", "LightWrite", ".", "``", ";", "8", ",", "``", "I", "will", "be", "happy", "to", "download", "LightWrite", "and", "use", "it", "to", "learn", "to", "handwrite", "if", "it", "is", "launched", "in", "the", "app", "store", ".", "``", ";", "9", ",", "``", "The", "touch-vibration", "feedback", "in", "basic", "stroke", "learning", "mode", "can", "help", "me", "better", "understand", "and", "write", "the", "basic", "strokes", ".", "``", ";", "10", ",", "``", "Voice", "instructions", "in", "character", "learning", "mode", "can", "help", "me", "understand", "the", "shape", "of", "the", "characters", ".", "``", ";", "11", ",", "``", "Voice", "feedback", "provided", "in", "the", "character", "learning", "module", "can", "let", "me", "know", "how", "to", "write", "better.", "''", ".", "All", "statements", "received", "an", "average", "score", "above", "6", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "LightWrite: Teach Handwriting to The Visually Impaired with A Smartphone ", "tokens": ["LightWrite", ":", "Teach", "Handwriting", "to", "The", "Visually", "Impaired", "with", "A", "Smartphone"]}, "filename": "1bd67672e9376feaac80770695717b13ece2c47b_Image_009.png", "orig_filename": "1bd67672e9376feaac80770695717b13ece2c47b", "split": "val"}, {"article_id": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation", "description": {"raw": "A bar chart with 5 columns: ``no aid'', ``perm. arrow'', ``perm. map'',     ``adaptive arrow'' and adaptive map. The values range from 1 to 5 and are     labeled with Rating.", "tokens": ["A", "bar", "chart", "with", "5", "columns", ":", "``", "no", "aid", "''", ",", "``", "perm", ".", "arrow", "''", ",", "``", "perm", ".", "map", "''", ",", "``", "adaptive", "arrow", "''", "and", "adaptive", "map", ".", "The", "values", "range", "from", "1", "to", "5", "and", "are", "labeled", "with", "Rating", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Lost in Style: Gaze-driven Adaptive Aid for VR Navigation ", "tokens": ["Lost", "in", "Style", ":", "Gaze-driven", "Adaptive", "Aid", "for", "VR", "Navigation"]}, "filename": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab_Image_015.jpg", "orig_filename": "5fbe768879ccd88f675d4783a05a9f26a1ef49ab", "split": "val"}, {"article_id": "Teacher Views of Math E-learning Tools for Students with Specific Learning Disabilities", "description": {"raw": "Figure 3 is a bar graph that shows the number of participants who used a specific math e-learning tool. The data is sorted in descending order. 4 people used IXL Learning; 4 people used Khan Academy; 3 people used CMP3; 2 people used Flocabulary; 2 people used Kahoot!; 2 people used ST Math; 1 person used BrainPOP; 1 person used Prodigy; 1 person used DareDash; 1 person used Jungle Math; and 1 person used Slice Fractions.", "tokens": ["Figure", "3", "is", "a", "bar", "graph", "that", "shows", "the", "number", "of", "participants", "who", "used", "a", "specific", "math", "e-learning", "tool", ".", "The", "data", "is", "sorted", "in", "descending", "order", ".", "4", "people", "used", "IXL", "Learning", ";", "4", "people", "used", "Khan", "Academy", ";", "3", "people", "used", "CMP3", ";", "2", "people", "used", "Flocabulary", ";", "2", "people", "used", "Kahoot", "!", ";", "2", "people", "used", "ST", "Math", ";", "1", "person", "used", "BrainPOP", ";", "1", "person", "used", "Prodigy", ";", "1", "person", "used", "DareDash", ";", "1", "person", "used", "Jungle", "Math", ";", "and", "1", "person", "used", "Slice", "Fractions", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Teacher Views of Math E-learning Tools for Students with Specific Learning Disabilities ", "tokens": ["Teacher", "Views", "of", "Math", "E-learning", "Tools", "for", "Students", "with", "Specific", "Learning", "Disabilities"]}, "filename": "e79cd418e238686f30e4b8f2660ab7e729fbec17_Image_009.png", "orig_filename": "e79cd418e238686f30e4b8f2660ab7e729fbec17", "split": "val"}, {"article_id": "Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints", "description": {"raw": "Figure 2 contains a snapshot of the CrowdFit plannign page. The components shown on the page are (A) profile of client: Ray, Female, 29 years, 135 lb, Unemployed. Goals: Next week's goal: achieve more active days per week than sedentary, Long term goal: Become comfortable working out on a daily basis, Constraints: Limited access to workout facilities, Access to: Yoga mats, small weights, fitness ball, Activities: Likes: bodyweight exercises, can be done anywhere with limited resources. Interested in, but have not tried, (B) distribution of cardio and strength over the week: 32% cardio, 68% strength, (C) calories burnt if following the plan on the calendar: 649 calories of 536 to 1072 calories, (D) distribution of calories and strength-cardio per day: 7 day graph, showing around 150 calories of cardio on Monday, around 150 calories of mostly strength on Wednesday, about 400 calories of mostly strength on Friday, (E) client calendar with scheduled physical activities in green: e.g. swimming 9-10am on Sunday, (F) overview of last week’s plan: Ray's plan was on Monday: Bodyweight workout for 45 min, on Tuesday Yoga for 45 min,  and (G) plysical activity the client performed: What Ray actually did: Monday bodyweight workout for 30 minutes - slightly shorter than taget, based on video length, on Tuesday walking 30 minutes I did not have time to go to yoga.", "tokens": ["Figure", "2", "contains", "a", "snapshot", "of", "the", "CrowdFit", "plannign", "page", ".", "The", "components", "shown", "on", "the", "page", "are", "(", "A", ")", "profile", "of", "client", ":", "Ray", ",", "Female", ",", "29", "years", ",", "135", "lb", ",", "Unemployed", ".", "Goals", ":", "Next", "week", "'s", "goal", ":", "achieve", "more", "active", "days", "per", "week", "than", "sedentary", ",", "Long", "term", "goal", ":", "Become", "comfortable", "working", "out", "on", "a", "daily", "basis", ",", "Constraints", ":", "Limited", "access", "to", "workout", "facilities", ",", "Access", "to", ":", "Yoga", "mats", ",", "small", "weights", ",", "fitness", "ball", ",", "Activities", ":", "Likes", ":", "bodyweight", "exercises", ",", "can", "be", "done", "anywhere", "with", "limited", "resources", ".", "Interested", "in", ",", "but", "have", "not", "tried", ",", "(", "B", ")", "distribution", "of", "cardio", "and", "strength", "over", "the", "week", ":", "32", "%", "cardio", ",", "68", "%", "strength", ",", "(", "C", ")", "calories", "burnt", "if", "following", "the", "plan", "on", "the", "calendar", ":", "649", "calories", "of", "536", "to", "1072", "calories", ",", "(", "D", ")", "distribution", "of", "calories", "and", "strength-cardio", "per", "day", ":", "7", "day", "graph", ",", "showing", "around", "150", "calories", "of", "cardio", "on", "Monday", ",", "around", "150", "calories", "of", "mostly", "strength", "on", "Wednesday", ",", "about", "400", "calories", "of", "mostly", "strength", "on", "Friday", ",", "(", "E", ")", "client", "calendar", "with", "scheduled", "physical", "activities", "in", "green", ":", "e.g", ".", "swimming", "9-10am", "on", "Sunday", ",", "(", "F", ")", "overview", "of", "last", "week", "’", "s", "plan", ":", "Ray", "'s", "plan", "was", "on", "Monday", ":", "Bodyweight", "workout", "for", "45", "min", ",", "on", "Tuesday", "Yoga", "for", "45", "min", ",", "and", "(", "G", ")", "plysical", "activity", "the", "client", "performed", ":", "What", "Ray", "actually", "did", ":", "Monday", "bodyweight", "workout", "for", "30", "minutes", "-", "slightly", "shorter", "than", "taget", ",", "based", "on", "video", "length", ",", "on", "Tuesday", "walking", "30", "minutes", "I", "did", "not", "have", "time", "to", "go", "to", "yoga", "."]}, "caption": {"raw": "Figure 2. (A) profile of client, (B) distribution of cardio and strength over the week, (C) calories burnt if following the plan on the calendar, (D) distribution of calories and strength-cardio per day, (E) client calendar with scheduled physical activities in green, (F) overview of last week’s plan and (G) plysical activity the client performed.", "tokens": ["Figure", "2", ".", "(", "A", ")", "profile", "of", "client", ",", "(", "B", ")", "distribution", "of", "cardio", "and", "strength", "over", "the", "week", ",", "(", "C", ")", "calories", "burnt", "if", "following", "the", "plan", "on", "the", "calendar", ",", "(", "D", ")", "distribution", "of", "calories", "and", "strength-cardio", "per", "day", ",", "(", "E", ")", "client", "calendar", "with", "scheduled", "physical", "activities", "in", "green", ",", "(", "F", ")", "overview", "of", "last", "week", "’", "s", "plan", "and", "(", "G", ")", "plysical", "activity", "the", "client", "performed", "."]}, "context": {"raw": "Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints Figure 2. (A) profile of client, (B) distribution of cardio and strength over the week, (C) calories burnt if following the plan on the calendar, (D) distribution of calories and strength-cardio per day, (E) client calendar with scheduled physical activities in green, (F) overview of last week’s plan and (G) plysical activity the client performed.", "tokens": ["Crowdsourcing", "Exercise", "Plans", "Aligned", "with", "Expert", "Guidelines", "and", "Everyday", "Constraints", "Figure", "2", ".", "(", "A", ")", "profile", "of", "client", ",", "(", "B", ")", "distribution", "of", "cardio", "and", "strength", "over", "the", "week", ",", "(", "C", ")", "calories", "burnt", "if", "following", "the", "plan", "on", "the", "calendar", ",", "(", "D", ")", "distribution", "of", "calories", "and", "strength-cardio", "per", "day", ",", "(", "E", ")", "client", "calendar", "with", "scheduled", "physical", "activities", "in", "green", ",", "(", "F", ")", "overview", "of", "last", "week", "’", "s", "plan", "and", "(", "G", ")", "plysical", "activity", "the", "client", "performed", "."]}, "filename": "87f5eb86ba8eb8ccd444326ff265e883f7c99948_Image_002.jpg", "orig_filename": "87f5eb86ba8eb8ccd444326ff265e883f7c99948", "split": "val"}, {"article_id": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations", "description": {"raw": "A bubble chart with \"IMDB Rating\" binned on x, \"Rotten Tomatoes Rating\" on y, and \"US Gross\" on size.", "tokens": ["A", "bubble", "chart", "with", "``", "IMDB", "Rating", "''", "binned", "on", "x", ",", "``", "Rotten", "Tomatoes", "Rating", "''", "on", "y", ",", "and", "``", "US", "Gross", "''", "on", "size", "."]}, "caption": {"raw": "Notice that this visualization, while effective if the analyst’s intent is to gain an understanding of US_Gross as a function of the other variables, does not adhere to the encodings pro- duced by the imdb_by_us_gross query. If users intend to reﬁne their chart (rather than ask a new question), they can anchor on a previous chart to signal their intent and guide Dziban towards a similar design. This can be done by either invoking the anchor function on a previous chart or invoking the anchor_on(...) function with an arbitrary chart.", "tokens": ["Notice", "that", "this", "visualization", ",", "while", "effective", "if", "the", "analyst", "’", "s", "intent", "is", "to", "gain", "an", "understanding", "of", "US_Gross", "as", "a", "function", "of", "the", "other", "variables", ",", "does", "not", "adhere", "to", "the", "encodings", "pro-", "duced", "by", "the", "imdb_by_us_gross", "query", ".", "If", "users", "intend", "to", "reﬁne", "their", "chart", "(", "rather", "than", "ask", "a", "new", "question", ")", ",", "they", "can", "anchor", "on", "a", "previous", "chart", "to", "signal", "their", "intent", "and", "guide", "Dziban", "towards", "a", "similar", "design", ".", "This", "can", "be", "done", "by", "either", "invoking", "the", "anchor", "function", "on", "a", "previous", "chart", "or", "invoking", "the", "anchor_on", "(", "...", ")", "function", "with", "an", "arbitrary", "chart", "."]}, "context": {"raw": "Dziban: Balancing Agency & Automation in Visualization Design via Anchored Recommendations Notice that this visualization, while effective if the analyst’s intent is to gain an understanding of US_Gross as a function of the other variables, does not adhere to the encodings pro- duced by the imdb_by_us_gross query. If users intend to reﬁne their chart (rather than ask a new question), they can anchor on a previous chart to signal their intent and guide Dziban towards a similar design. This can be done by either invoking the anchor function on a previous chart or invoking the anchor_on(...) function with an arbitrary chart.", "tokens": ["Dziban", ":", "Balancing", "Agency", "&", "Automation", "in", "Visualization", "Design", "via", "Anchored", "Recommendations", "Notice", "that", "this", "visualization", ",", "while", "effective", "if", "the", "analyst", "’", "s", "intent", "is", "to", "gain", "an", "understanding", "of", "US_Gross", "as", "a", "function", "of", "the", "other", "variables", ",", "does", "not", "adhere", "to", "the", "encodings", "pro-", "duced", "by", "the", "imdb_by_us_gross", "query", ".", "If", "users", "intend", "to", "reﬁne", "their", "chart", "(", "rather", "than", "ask", "a", "new", "question", ")", ",", "they", "can", "anchor", "on", "a", "previous", "chart", "to", "signal", "their", "intent", "and", "guide", "Dziban", "towards", "a", "similar", "design", ".", "This", "can", "be", "done", "by", "either", "invoking", "the", "anchor", "function", "on", "a", "previous", "chart", "or", "invoking", "the", "anchor_on", "(", "...", ")", "function", "with", "an", "arbitrary", "chart", "."]}, "filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239_Image_005.png", "orig_filename": "2247b3cb46f3b434c8018fdf71ec8c9f780d7239", "split": "val"}, {"article_id": "PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization", "description": {"raw": "The figure consists of two columns of 3 pictures each.  Picture a (top left) - The two electrodes of a multimeter are connected to each side of a stretch-sensitive yarn. A ruler is situated next to the yarn to show its length which seems to be approximately 10 centimeters. The user does not seem to stretch the yarn and the multimeter shows a value of 0.12MOhm.  Picture b (center left) - It is the same setup as Picture a, but this time the yarn is stretched to reach a length of approximately 20 centimeters. The multimeter shows a value of 1.18MOhm.  Picture c (bottom left) - A plot with two axes shows the evolution of the yarn resistance depending on its extension. It shows linear progression from approximately 0 to 5MOhm (y-axis) going from an extension of 0% to 100% by steps of 20% (x-axis).  Picture d (top right) - This is a close-up on a pressure sensing yarn held within fingertips. The yarn consists of a copper conductive core and a polymerized outer layer.  Picture e (center right) - The two electrodes of a multimeter are connected to two independent pressure-sensitive yarns that stack up on each other. The user is pressing the yarns at their intersection. The value on the multimeter shows a value of 1.93MOhm.  Picture f (bottom right) - A plot shows how the resistances of the yarns evolve depending on the weight applied on top of them. It shows a linear progression from 6MOhm to 2MOhm (y-axis) for weights going from 0.1g to 1000g increasing the weight ten times at each step (x-axis, logarithmic scale).", "tokens": ["The", "figure", "consists", "of", "two", "columns", "of", "3", "pictures", "each", ".", "Picture", "a", "(", "top", "left", ")", "-", "The", "two", "electrodes", "of", "a", "multimeter", "are", "connected", "to", "each", "side", "of", "a", "stretch-sensitive", "yarn", ".", "A", "ruler", "is", "situated", "next", "to", "the", "yarn", "to", "show", "its", "length", "which", "seems", "to", "be", "approximately", "10", "centimeters", ".", "The", "user", "does", "not", "seem", "to", "stretch", "the", "yarn", "and", "the", "multimeter", "shows", "a", "value", "of", "0.12MOhm", ".", "Picture", "b", "(", "center", "left", ")", "-", "It", "is", "the", "same", "setup", "as", "Picture", "a", ",", "but", "this", "time", "the", "yarn", "is", "stretched", "to", "reach", "a", "length", "of", "approximately", "20", "centimeters", ".", "The", "multimeter", "shows", "a", "value", "of", "1.18MOhm", ".", "Picture", "c", "(", "bottom", "left", ")", "-", "A", "plot", "with", "two", "axes", "shows", "the", "evolution", "of", "the", "yarn", "resistance", "depending", "on", "its", "extension", ".", "It", "shows", "linear", "progression", "from", "approximately", "0", "to", "5MOhm", "(", "y-axis", ")", "going", "from", "an", "extension", "of", "0", "%", "to", "100", "%", "by", "steps", "of", "20", "%", "(", "x-axis", ")", ".", "Picture", "d", "(", "top", "right", ")", "-", "This", "is", "a", "close-up", "on", "a", "pressure", "sensing", "yarn", "held", "within", "fingertips", ".", "The", "yarn", "consists", "of", "a", "copper", "conductive", "core", "and", "a", "polymerized", "outer", "layer", ".", "Picture", "e", "(", "center", "right", ")", "-", "The", "two", "electrodes", "of", "a", "multimeter", "are", "connected", "to", "two", "independent", "pressure-sensitive", "yarns", "that", "stack", "up", "on", "each", "other", ".", "The", "user", "is", "pressing", "the", "yarns", "at", "their", "intersection", ".", "The", "value", "on", "the", "multimeter", "shows", "a", "value", "of", "1.93MOhm", ".", "Picture", "f", "(", "bottom", "right", ")", "-", "A", "plot", "shows", "how", "the", "resistances", "of", "the", "yarns", "evolve", "depending", "on", "the", "weight", "applied", "on", "top", "of", "them", ".", "It", "shows", "a", "linear", "progression", "from", "6MOhm", "to", "2MOhm", "(", "y-axis", ")", "for", "weights", "going", "from", "0.1g", "to", "1000g", "increasing", "the", "weight", "ten", "times", "at", "each", "step", "(", "x-axis", ",", "logarithmic", "scale", ")", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization ", "tokens": ["PolySense", ":", "Augmenting", "Textiles", "with", "Electrical", "Functionality", "using", "In-Situ", "Polymerization"]}, "filename": "b58b35da221a853d5b99bbfaa3ea40252d70016b_Image_006.jpg", "orig_filename": "b58b35da221a853d5b99bbfaa3ea40252d70016b", "split": "val"}, {"article_id": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users", "description": {"raw": "Figure 5 - Three line graphs showing word error rate of worker transcriptions in a 10-step iterative workflow, with step number on the X axis and word error rate on the Y axis. The first graph (intel 30) shows iteration fails to improve word error rate The second graph (intel 40) shows iteration produces transcriptions with significantly lower word error rate after 10 steps. The third graph (intel 50) shows iteration is able to produce transcriptions with average word error rate below 0.1, significantly outperforming automated and individual human approaches.", "tokens": ["Figure", "5", "-", "Three", "line", "graphs", "showing", "word", "error", "rate", "of", "worker", "transcriptions", "in", "a", "10-step", "iterative", "workflow", ",", "with", "step", "number", "on", "the", "X", "axis", "and", "word", "error", "rate", "on", "the", "Y", "axis", ".", "The", "first", "graph", "(", "intel", "30", ")", "shows", "iteration", "fails", "to", "improve", "word", "error", "rate", "The", "second", "graph", "(", "intel", "40", ")", "shows", "iteration", "produces", "transcriptions", "with", "significantly", "lower", "word", "error", "rate", "after", "10", "steps", ".", "The", "third", "graph", "(", "intel", "50", ")", "shows", "iteration", "is", "able", "to", "produce", "transcriptions", "with", "average", "word", "error", "rate", "below", "0.1", ",", "significantly", "outperforming", "automated", "and", "individual", "human", "approaches", "."]}, "caption": {"raw": "Figure 5. WER of transcriptions from iterative versus baseline approaches: intel 30 (left), intel 40 (center), intel 50 (right). The quality of iteratively- generated transcriptions quickly surpassed those from automated and individual crowd worker approaches for intelligibility levels 40 and 50. However the iterative approach failed to produce better transcription at intelligibility level 30.", "tokens": ["Figure", "5", ".", "WER", "of", "transcriptions", "from", "iterative", "versus", "baseline", "approaches", ":", "intel", "30", "(", "left", ")", ",", "intel", "40", "(", "center", ")", ",", "intel", "50", "(", "right", ")", ".", "The", "quality", "of", "iteratively-", "generated", "transcriptions", "quickly", "surpassed", "those", "from", "automated", "and", "individual", "crowd", "worker", "approaches", "for", "intelligibility", "levels", "40", "and", "50", ".", "However", "the", "iterative", "approach", "failed", "to", "produce", "better", "transcription", "at", "intelligibility", "level", "30", "."]}, "context": {"raw": "Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users Figure 5. WER of transcriptions from iterative versus baseline approaches: intel 30 (left), intel 40 (center), intel 50 (right). The quality of iteratively- generated transcriptions quickly surpassed those from automated and individual crowd worker approaches for intelligibility levels 40 and 50. However the iterative approach failed to produce better transcription at intelligibility level 30.", "tokens": ["Towards", "More", "Robust", "Speech", "Interactions", "for", "Deaf", "and", "Hard", "of", "Hearing", "Users", "Figure", "5", ".", "WER", "of", "transcriptions", "from", "iterative", "versus", "baseline", "approaches", ":", "intel", "30", "(", "left", ")", ",", "intel", "40", "(", "center", ")", ",", "intel", "50", "(", "right", ")", ".", "The", "quality", "of", "iteratively-", "generated", "transcriptions", "quickly", "surpassed", "those", "from", "automated", "and", "individual", "crowd", "worker", "approaches", "for", "intelligibility", "levels", "40", "and", "50", ".", "However", "the", "iterative", "approach", "failed", "to", "produce", "better", "transcription", "at", "intelligibility", "level", "30", "."]}, "filename": "8bba1845a85370618cd5c400ec8be42208554549_Image_008.jpg", "orig_filename": "8bba1845a85370618cd5c400ec8be42208554549", "split": "val"}, {"article_id": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "description": {"raw": "This figure presents a collum chart with the MSD Error Rate for each method. MultiTap has the largest MSD Error Rate, followed by QWERTY, NavTouch and BrailleTouch. the method with thefastest followed by MultiTap, NavTouch and BrailleTouch.", "tokens": ["This", "figure", "presents", "a", "collum", "chart", "with", "the", "MSD", "Error", "Rate", "for", "each", "method", ".", "MultiTap", "has", "the", "largest", "MSD", "Error", "Rate", ",", "followed", "by", "QWERTY", ",", "NavTouch", "and", "BrailleTouch", ".", "the", "method", "with", "thefastest", "followed", "by", "MultiTap", ",", "NavTouch", "and", "BrailleTouch", "."]}, "caption": {"raw": "Figure 5. MSD Error Rates. Error bars denote 95% CI.", "tokens": ["Figure", "5", ".", "MSD", "Error", "Rates", ".", "Error", "bars", "denote", "95", "%", "CI", "."]}, "context": {"raw": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors Figure 5. MSD Error Rates. Error bars denote 95% CI.", "tokens": ["Blind", "people", "and", "mobile", "touch-based", "text-entry", ":", "acknowledging", "the", "need", "for", "different", "flavors", "Figure", "5", ".", "MSD", "Error", "Rates", ".", "Error", "bars", "denote", "95", "%", "CI", "."]}, "filename": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_006.jpg", "orig_filename": "3b78980f06c57940475cfc741de29099847b6aa66c8bc0_Image_021e31176cfe6b7e2b6309e2f18c63103baa9", "split": "val"}, {"article_id": "User Experiences with Online Status Indicators", "description": {"raw": "A stacked bar chart showing how many participants guessed that an app does or does not have OSIs and how many participants were not sure. Each bar is the same height, so that the bars show the percent of people answering about this app that gave each answer choice, and the total number of participants giving each answer is overlaid on top of the bars. The top 15 most popular apps, used by at least 10% of participants, are shown, with responses ranging from mostly correctly stating that the app had OSIs to mostly expressing uncertainty about whether the app had OSIs.\" Table 4: \"A table conveying the mean difference, SE, DF, t value, p value, and 95% CI between the green dot experimental condition and each other color (blue, gray, orange).", "tokens": ["A", "stacked", "bar", "chart", "showing", "how", "many", "participants", "guessed", "that", "an", "app", "does", "or", "does", "not", "have", "OSIs", "and", "how", "many", "participants", "were", "not", "sure", ".", "Each", "bar", "is", "the", "same", "height", ",", "so", "that", "the", "bars", "show", "the", "percent", "of", "people", "answering", "about", "this", "app", "that", "gave", "each", "answer", "choice", ",", "and", "the", "total", "number", "of", "participants", "giving", "each", "answer", "is", "overlaid", "on", "top", "of", "the", "bars", ".", "The", "top", "15", "most", "popular", "apps", ",", "used", "by", "at", "least", "10", "%", "of", "participants", ",", "are", "shown", ",", "with", "responses", "ranging", "from", "mostly", "correctly", "stating", "that", "the", "app", "had", "OSIs", "to", "mostly", "expressing", "uncertainty", "about", "whether", "the", "app", "had", "OSIs", ".", "''", "Table", "4", ":", "``", "A", "table", "conveying", "the", "mean", "difference", ",", "SE", ",", "DF", ",", "t", "value", ",", "p", "value", ",", "and", "95", "%", "CI", "between", "the", "green", "dot", "experimental", "condition", "and", "each", "other", "color", "(", "blue", ",", "gray", ",", "orange", ")", "."]}, "caption": {"raw": "Discord had OSIs, but only a few knew that MyFitnessPal had them. Some differences may be related to how OSIs are designed in each app. For instance, OSIs on Instagram are only visible between users of the messaging feature, so it is plausible that responses for Instagram correlate with whether each participant uses that feature.", "tokens": ["Discord", "had", "OSIs", ",", "but", "only", "a", "few", "knew", "that", "MyFitnessPal", "had", "them", ".", "Some", "differences", "may", "be", "related", "to", "how", "OSIs", "are", "designed", "in", "each", "app", ".", "For", "instance", ",", "OSIs", "on", "Instagram", "are", "only", "visible", "between", "users", "of", "the", "messaging", "feature", ",", "so", "it", "is", "plausible", "that", "responses", "for", "Instagram", "correlate", "with", "whether", "each", "participant", "uses", "that", "feature", "."]}, "context": {"raw": "User Experiences with Online Status Indicators Discord had OSIs, but only a few knew that MyFitnessPal had them. Some differences may be related to how OSIs are designed in each app. For instance, OSIs on Instagram are only visible between users of the messaging feature, so it is plausible that responses for Instagram correlate with whether each participant uses that feature.", "tokens": ["User", "Experiences", "with", "Online", "Status", "Indicators", "Discord", "had", "OSIs", ",", "but", "only", "a", "few", "knew", "that", "MyFitnessPal", "had", "them", ".", "Some", "differences", "may", "be", "related", "to", "how", "OSIs", "are", "designed", "in", "each", "app", ".", "For", "instance", ",", "OSIs", "on", "Instagram", "are", "only", "visible", "between", "users", "of", "the", "messaging", "feature", ",", "so", "it", "is", "plausible", "that", "responses", "for", "Instagram", "correlate", "with", "whether", "each", "participant", "uses", "that", "feature", "."]}, "filename": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b_Image_006.jpg", "orig_filename": "6c91ce2625e85d690e8ad2131cc9e759d18ed51b", "split": "val"}, {"article_id": "HapticSeer: A Multi-channel, Black-box, Platform-agnostic Approach to Detecting Video Game Events for Real-time Haptic Feedback", "description": {"raw": "The first line graph showing that the discrete output from a Speed Detector almost fit the ground truth.  The second line graph showing that the longitudinal acceleration output from an Inertia Detector has a similar trend with ground truth, but with a smaller magnitude.  The third line graph showing that the lateral acceleration output from an Inertia Detector has a similar trend with the ground truth, but cramped to -20 to 20.", "tokens": ["The", "first", "line", "graph", "showing", "that", "the", "discrete", "output", "from", "a", "Speed", "Detector", "almost", "fit", "the", "ground", "truth", ".", "The", "second", "line", "graph", "showing", "that", "the", "longitudinal", "acceleration", "output", "from", "an", "Inertia", "Detector", "has", "a", "similar", "trend", "with", "ground", "truth", ",", "but", "with", "a", "smaller", "magnitude", ".", "The", "third", "line", "graph", "showing", "that", "the", "lateral", "acceleration", "output", "from", "an", "Inertia", "Detector", "has", "a", "similar", "trend", "with", "the", "ground", "truth", ",", "but", "cramped", "to", "-20", "to", "20", "."]}, "caption": {"raw": "We concluded that", "tokens": ["We", "concluded", "that"]}, "context": {"raw": "HapticSeer: A Multi-channel, Black-box, Platform-agnostic Approach to Detecting Video Game Events for Real-time Haptic Feedback We concluded that", "tokens": ["HapticSeer", ":", "A", "Multi-channel", ",", "Black-box", ",", "Platform-agnostic", "Approach", "to", "Detecting", "Video", "Game", "Events", "for", "Real-time", "Haptic", "Feedback", "We", "concluded", "that"]}, "filename": "341ee80cdaeb3403beae340824f7127cb9d77622_Image_014.jpg", "orig_filename": "341ee80cdaeb3403beae340824f7127cb9d77622", "split": "val"}, {"article_id": "DDF Seeks Same : Sexual Health-Related Language in Online Personal Ads For Men Who Have Sex With Men", "description": {"raw": "This figure is a scatterplot showing the relationship between HIV Estimated Prevalence Rate Per 100,000 Population (CDC, 2011) on the x-axis and Percentage of Ads Containing Sexual Health-Related Language on the y-axis in 95 locations. The relationship is estimated linearly by the equation SHR language = 46.360 + 0.009 * prevalence rate, p = 0.009. Each location is represented by a circle, with the size of the circle representing its population relative to the other locations. Outliers with a low HIV prevalence rate and a high % of SHR language in ads include SF Bay Area and Boise, ID. Outliers with a high HIV prevalence rate and a low % of SHR language in ads include Wichita, KS and Jackson, MS. New York City is close to the linear trend, with a high HIV prevalence rate and a high % of SHR language in ads. In general, locations with a higher population tend to have a greater percentage of ads containing SHR language.", "tokens": ["This", "figure", "is", "a", "scatterplot", "showing", "the", "relationship", "between", "HIV", "Estimated", "Prevalence", "Rate", "Per", "100,000", "Population", "(", "CDC", ",", "2011", ")", "on", "the", "x-axis", "and", "Percentage", "of", "Ads", "Containing", "Sexual", "Health-Related", "Language", "on", "the", "y-axis", "in", "95", "locations", ".", "The", "relationship", "is", "estimated", "linearly", "by", "the", "equation", "SHR", "language", "=", "46.360", "+", "0.009", "*", "prevalence", "rate", ",", "p", "=", "0.009", ".", "Each", "location", "is", "represented", "by", "a", "circle", ",", "with", "the", "size", "of", "the", "circle", "representing", "its", "population", "relative", "to", "the", "other", "locations", ".", "Outliers", "with", "a", "low", "HIV", "prevalence", "rate", "and", "a", "high", "%", "of", "SHR", "language", "in", "ads", "include", "SF", "Bay", "Area", "and", "Boise", ",", "ID", ".", "Outliers", "with", "a", "high", "HIV", "prevalence", "rate", "and", "a", "low", "%", "of", "SHR", "language", "in", "ads", "include", "Wichita", ",", "KS", "and", "Jackson", ",", "MS.", "New", "York", "City", "is", "close", "to", "the", "linear", "trend", ",", "with", "a", "high", "HIV", "prevalence", "rate", "and", "a", "high", "%", "of", "SHR", "language", "in", "ads", ".", "In", "general", ",", "locations", "with", "a", "higher", "population", "tend", "to", "have", "a", "greater", "percentage", "of", "ads", "containing", "SHR", "language", "."]}, "caption": {"raw": "SHR language = 46.360 + 0.009 * prevalence rate, p=0.009", "tokens": ["SHR", "language", "=", "46.360", "+", "0.009", "*", "prevalence", "rate", ",", "p=0.009"]}, "context": {"raw": "DDF Seeks Same : Sexual Health-Related Language in Online Personal Ads For Men Who Have Sex With Men SHR language = 46.360 + 0.009 * prevalence rate, p=0.009", "tokens": ["DDF", "Seeks", "Same", ":", "Sexual", "Health-Related", "Language", "in", "Online", "Personal", "Ads", "For", "Men", "Who", "Have", "Sex", "With", "Men", "SHR", "language", "=", "46.360", "+", "0.009", "*", "prevalence", "rate", ",", "p=0.009"]}, "filename": "be00f1bcb7d9f911a3ca855e2e7b1bc926be47e8_Image_002.jpg", "orig_filename": "be00f1bcb7d9f911a3ca855e2e7b1bc926be47e8", "split": "val"}, {"article_id": "Changing Perspective: A Co-Design Approach to Explore Future Possibilities of Divergent Hearing", "description": {"raw": "Titel: Infographics \"Intra-active\" Dynamics - Beschreibung: The term \"activity\" is positioned in the middle of a triangular graph. The corners of the triangle are made out of the acronyms \"PPP\", \"OP\" and \"P\". The term \"activitiy\" and the acronyms are connected with double-ended arrows.", "tokens": ["Titel", ":", "Infographics", "``", "Intra-active", "''", "Dynamics", "-", "Beschreibung", ":", "The", "term", "``", "activity", "''", "is", "positioned", "in", "the", "middle", "of", "a", "triangular", "graph", ".", "The", "corners", "of", "the", "triangle", "are", "made", "out", "of", "the", "acronyms", "``", "PPP", "''", ",", "``", "OP", "''", "and", "``", "P", "''", ".", "The", "term", "``", "activitiy", "''", "and", "the", "acronyms", "are", "connected", "with", "double-ended", "arrows", "."]}, "caption": {"raw": "", "tokens": []}, "context": {"raw": "Changing Perspective: A Co-Design Approach to Explore Future Possibilities of Divergent Hearing ", "tokens": ["Changing", "Perspective", ":", "A", "Co-Design", "Approach", "to", "Explore", "Future", "Possibilities", "of", "Divergent", "Hearing"]}, "filename": "6443e3632848430cfac5aeb3ab94a46acf0bbf0c_Image_006.jpg", "orig_filename": "6443e3632848430cfac5aeb3ab94a46acf0bbf0c", "split": "val"}, {"article_id": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning", "description": {"raw": "The radar chart from T1 shows plots from 6 previous sessions as well as the yet unmodified plot for the 7th session. It shows how the teachers' focus may change from session to session.", "tokens": ["The", "radar", "chart", "from", "T1", "shows", "plots", "from", "6", "previous", "sessions", "as", "well", "as", "the", "yet", "unmodified", "plot", "for", "the", "7th", "session", ".", "It", "shows", "how", "the", "teachers", "'", "focus", "may", "change", "from", "session", "to", "session", "."]}, "caption": {"raw": "Figure 6. Graphs of 6 sessions by T1 with the yet unmodified graph of session 7.", "tokens": ["Figure", "6", ".", "Graphs", "of", "6", "sessions", "by", "T1", "with", "the", "yet", "unmodified", "graph", "of", "session", "7", "."]}, "context": {"raw": "Group Spinner: Recognizing and Visualizing Learning in the Classroom for Reflection, Communication, and Planning Figure 6. Graphs of 6 sessions by T1 with the yet unmodified graph of session 7.", "tokens": ["Group", "Spinner", ":", "Recognizing", "and", "Visualizing", "Learning", "in", "the", "Classroom", "for", "Reflection", ",", "Communication", ",", "and", "Planning", "Figure", "6", ".", "Graphs", "of", "6", "sessions", "by", "T1", "with", "the", "yet", "unmodified", "graph", "of", "session", "7", "."]}, "filename": "35941a4414b58e76d1f92b495c3c8d90a2593315_Image_007.jpg", "orig_filename": "35941a4414b58e76d1f92b495c3c8d90a2593315", "split": "val"}, {"article_id": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users", "description": {"raw": "Figure 2: Three bar charts of missed sounds (a) at home, (b) at work, and (c) while mobile. Each chart displays the percentage of participants who reported missing sounds never, once/month, once/week, once/day, and more than once/day. There are one bar for deaf participants, and one for hard-of-hearing participants in each condition. (a) At home, about 30% of deaf participants never missed sounds and 50% missed sounds more than once per day. About 25% of hard-of-hearing participants missed sounds once/week, and about 50% more than once per day. Other responses were all under about 12%. (b) At work, the bar chart takes a U-shape for both deaf and hard-of-hearing participants. About 35% of deaf participants never missed sounds, and about 40% missed sounds more than once per day, with all other answers under 15%. Hard-of-hearing participants followed a similar curve. (c) While mobile, the bar chart takes a U-shape for both deaf and hard-of-hearing participants. About 35% of deaf participants never missed sounds, and about 50% missed more than once per day. The other responses were under about 10%. Hard-of-hearing participants followed a similar trend, but less extreme.", "tokens": ["Figure", "2", ":", "Three", "bar", "charts", "of", "missed", "sounds", "(", "a", ")", "at", "home", ",", "(", "b", ")", "at", "work", ",", "and", "(", "c", ")", "while", "mobile", ".", "Each", "chart", "displays", "the", "percentage", "of", "participants", "who", "reported", "missing", "sounds", "never", ",", "once/month", ",", "once/week", ",", "once/day", ",", "and", "more", "than", "once/day", ".", "There", "are", "one", "bar", "for", "deaf", "participants", ",", "and", "one", "for", "hard-of-hearing", "participants", "in", "each", "condition", ".", "(", "a", ")", "At", "home", ",", "about", "30", "%", "of", "deaf", "participants", "never", "missed", "sounds", "and", "50", "%", "missed", "sounds", "more", "than", "once", "per", "day", ".", "About", "25", "%", "of", "hard-of-hearing", "participants", "missed", "sounds", "once/week", ",", "and", "about", "50", "%", "more", "than", "once", "per", "day", ".", "Other", "responses", "were", "all", "under", "about", "12", "%", ".", "(", "b", ")", "At", "work", ",", "the", "bar", "chart", "takes", "a", "U-shape", "for", "both", "deaf", "and", "hard-of-hearing", "participants", ".", "About", "35", "%", "of", "deaf", "participants", "never", "missed", "sounds", ",", "and", "about", "40", "%", "missed", "sounds", "more", "than", "once", "per", "day", ",", "with", "all", "other", "answers", "under", "15", "%", ".", "Hard-of-hearing", "participants", "followed", "a", "similar", "curve", ".", "(", "c", ")", "While", "mobile", ",", "the", "bar", "chart", "takes", "a", "U-shape", "for", "both", "deaf", "and", "hard-of-hearing", "participants", ".", "About", "35", "%", "of", "deaf", "participants", "never", "missed", "sounds", ",", "and", "about", "50", "%", "missed", "more", "than", "once", "per", "day", ".", "The", "other", "responses", "were", "under", "about", "10", "%", ".", "Hard-of-hearing", "participants", "followed", "a", "similar", "trend", ",", "but", "less", "extreme", "."]}, "caption": {"raw": "(a) How often sounds missed at home           (b) How often sounds missed at work         (c) How often sounds missed while mobile", "tokens": ["(", "a", ")", "How", "often", "sounds", "missed", "at", "home", "(", "b", ")", "How", "often", "sounds", "missed", "at", "work", "(", "c", ")", "How", "often", "sounds", "missed", "while", "mobile"]}, "context": {"raw": "A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users (a) How often sounds missed at home           (b) How often sounds missed at work         (c) How often sounds missed while mobile", "tokens": ["A", "Personalizable", "Mobile", "Sound", "Detector", "App", "Design", "for", "Deaf", "and", "Hard-of-Hearing", "Users", "(", "a", ")", "How", "often", "sounds", "missed", "at", "home", "(", "b", ")", "How", "often", "sounds", "missed", "at", "work", "(", "c", ")", "How", "often", "sounds", "missed", "while", "mobile"]}, "filename": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77_Image_004.jpg", "orig_filename": "28e2c1a3d6158c3fcb765cec4094e46ef5847a77", "split": "val"}, {"article_id": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors", "description": {"raw": "This figure presents a line chart with the average WPM for each method, for two pressure sensitivity groups (<=3.61 and 3.62-4.31). The difference in performance of the two groups is considerable for QWERTY and MultiTap, especially for MultiTap, with the users with better sensitivity being much faster.", "tokens": ["This", "figure", "presents", "a", "line", "chart", "with", "the", "average", "WPM", "for", "each", "method", ",", "for", "two", "pressure", "sensitivity", "groups", "(", "<", "=3.61", "and", "3.62-4.31", ")", ".", "The", "difference", "in", "performance", "of", "the", "two", "groups", "is", "considerable", "for", "QWERTY", "and", "MultiTap", ",", "especially", "for", "MultiTap", ",", "with", "the", "users", "with", "better", "sensitivity", "being", "much", "faster", "."]}, "caption": {"raw": "Figure 7. Pressure sensitivity impact on WPM.", "tokens": ["Figure", "7", ".", "Pressure", "sensitivity", "impact", "on", "WPM", "."]}, "context": {"raw": "Blind people and mobile touch-based text-entry: acknowledging the need for different flavors Figure 7. Pressure sensitivity impact on WPM.", "tokens": ["Blind", "people", "and", "mobile", "touch-based", "text-entry", ":", "acknowledging", "the", "need", "for", "different", "flavors", "Figure", "7", ".", "Pressure", "sensitivity", "impact", "on", "WPM", "."]}, "filename": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9_Image_008.jpg", "orig_filename": "3b7898e31176cfe6b7e2b6309e2f18c63103baa9", "split": "val"}, {"article_id": "Voice Games: Investigation Into the Use of Non-speech Voice Input for Making Computer Games More Accessible", "description": {"raw": "A bar graph showing the timing results for the NMI group from the categorical input experiment.", "tokens": ["A", "bar", "graph", "showing", "the", "timing", "results", "for", "the", "NMI", "group", "from", "the", "categorical", "input", "experiment", "."]}, "caption": {"raw": "Figure 4: Summary of the aggregate results from the categorical input experiment for the NMI group. The results are grouped by task type. Each bar shows the contribution from reaction time and system time towards the total input time, as defined in Figure 2. Error bars show 95% confidence intervals for total input times.", "tokens": ["Figure", "4", ":", "Summary", "of", "the", "aggregate", "results", "from", "the", "categorical", "input", "experiment", "for", "the", "NMI", "group", ".", "The", "results", "are", "grouped", "by", "task", "type", ".", "Each", "bar", "shows", "the", "contribution", "from", "reaction", "time", "and", "system", "time", "towards", "the", "total", "input", "time", ",", "as", "defined", "in", "Figure", "2", ".", "Error", "bars", "show", "95", "%", "confidence", "intervals", "for", "total", "input", "times", "."]}, "context": {"raw": "Voice Games: Investigation Into the Use of Non-speech Voice Input for Making Computer Games More Accessible Figure 4: Summary of the aggregate results from the categorical input experiment for the NMI group. The results are grouped by task type. Each bar shows the contribution from reaction time and system time towards the total input time, as defined in Figure 2. Error bars show 95% confidence intervals for total input times.", "tokens": ["Voice", "Games", ":", "Investigation", "Into", "the", "Use", "of", "Non-speech", "Voice", "Input", "for", "Making", "Computer", "Games", "More", "Accessible", "Figure", "4", ":", "Summary", "of", "the", "aggregate", "results", "from", "the", "categorical", "input", "experiment", "for", "the", "NMI", "group", ".", "The", "results", "are", "grouped", "by", "task", "type", ".", "Each", "bar", "shows", "the", "contribution", "from", "reaction", "time", "and", "system", "time", "towards", "the", "total", "input", "time", ",", "as", "defined", "in", "Figure", "2", ".", "Error", "bars", "show", "95", "%", "confidence", "intervals", "for", "total", "input", "times", "."]}, "filename": "28636cf804c2520de41a7a7ff4392d173ba98a7b_Image_006.png", "orig_filename": "28636cf804c2520de41a7a7ff4392d173ba98a7b", "split": "val"}]}
